{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkAX88H3RNRW"
      },
      "source": [
        "# **Package Installation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bSuJvX5_qNhr",
        "outputId": "c28be0f5-3fc3-4d3f-ce24-aa4d960c5654"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.6.0\n",
            "  Using cached torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision==0.21.0\n",
            "  Using cached torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting filelock (from torch==2.6.0)\n",
            "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting typing-extensions>=4.10.0 (from torch==2.6.0)\n",
            "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting networkx (from torch==2.6.0)\n",
            "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch==2.6.0)\n",
            "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec (from torch==2.6.0)\n",
            "  Using cached fsspec-2025.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0)\n",
            "  Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.6.0)\n",
            "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.2.0 (from torch==2.6.0)\n",
            "  Using cached triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting sympy==1.13.1 (from torch==2.6.0)\n",
            "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting numpy (from torchvision==0.21.0)\n",
            "  Using cached numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.21.0)\n",
            "  Using cached pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch==2.6.0)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.6.0)\n",
            "  Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Using cached torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "Using cached torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
            "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "Using cached triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
            "Using cached pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl (4.6 MB)\n",
            "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Using cached fsspec-2025.5.0-py3-none-any.whl (196 kB)\n",
            "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "Using cached numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Installing collected packages: triton, nvidia-cusparselt-cu12, mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.3.0\n",
            "    Uninstalling mpmath-1.3.0:\n",
            "      Successfully uninstalled mpmath-1.3.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.13.2\n",
            "    Uninstalling typing_extensions-4.13.2:\n",
            "      Successfully uninstalled typing_extensions-4.13.2\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.2.1\n",
            "    Uninstalling pillow-11.2.1:\n",
            "      Successfully uninstalled pillow-11.2.1\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
            "    Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
            "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.6\n",
            "    Uninstalling numpy-2.2.6:\n",
            "      Successfully uninstalled numpy-2.2.6\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.4.2\n",
            "    Uninstalling networkx-3.4.2:\n",
            "      Successfully uninstalled networkx-3.4.2\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.18.0\n",
            "    Uninstalling filelock-3.18.0:\n",
            "      Successfully uninstalled filelock-3.18.0\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
            "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0\n",
            "    Uninstalling torch-2.6.0:\n",
            "      Successfully uninstalled torch-2.6.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0\n",
            "    Uninstalling torchvision-0.21.0:\n",
            "      Successfully uninstalled torchvision-0.21.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 filelock-3.18.0 fsspec-2025.5.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.6 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pillow-11.2.1 sympy-1.13.1 torch-2.6.0 torchvision-0.21.0 triton-3.2.0 typing-extensions-4.13.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "functorch",
                  "markupsafe",
                  "mpmath",
                  "networkx",
                  "sympy",
                  "torch",
                  "torchgen",
                  "triton"
                ]
              },
              "id": "f65ce16693474c3faa1e9eb4eb3e91f1"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Reinstall a specific version of PyTorch (v2.6.0) and torchvision (v0.21.0)\n",
        "# The \"--force-reinstall\" flag ensures that the packages are reinstalled even if the correct version is already present.\n",
        "# This is useful to resolve environment issues or when dependencies need to be reset.\n",
        "!pip install torch==2.6.0 torchvision==0.21.0 --force-reinstall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItSvFeEAfLBF",
        "outputId": "4c0520f4-7801-40dc-a63d-7261193083fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.2.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.2.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Collecting torch-geometric==2.4.0\n",
            "  Using cached torch_geometric-2.4.0-py3-none-any.whl.metadata (63 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (2.2.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (1.15.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (3.1.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (2.32.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (3.2.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (1.6.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric==2.4.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.4.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.4.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.4.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.4.0) (2025.4.26)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->torch-geometric==2.4.0) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->torch-geometric==2.4.0) (3.6.0)\n",
            "Using cached torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "Installing collected packages: torch-geometric\n",
            "  Attempting uninstall: torch-geometric\n",
            "    Found existing installation: torch-geometric 2.7.0\n",
            "    Uninstalling torch-geometric-2.7.0:\n",
            "      Successfully uninstalled torch-geometric-2.7.0\n",
            "Successfully installed torch-geometric-2.4.0\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.11/dist-packages (2.1.2+pt20cpu)\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.11/dist-packages (0.6.18+pt20cpu)\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.11/dist-packages (1.6.3+pt20cpu)\n",
            "Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.11/dist-packages (1.2.2+pt20cpu)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (2.2.6)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: deeponto in /usr/local/lib/python3.11/dist-packages (0.9.3)\n",
            "Requirement already satisfied: JPype1 in /usr/local/lib/python3.11/dist-packages (from deeponto) (1.5.2)\n",
            "Requirement already satisfied: yacs in /usr/local/lib/python3.11/dist-packages (from deeponto) (0.1.8)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from deeponto) (2.6.0)\n",
            "Requirement already satisfied: anytree in /usr/local/lib/python3.11/dist-packages (from deeponto) (2.13.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from deeponto) (8.2.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from deeponto) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from deeponto) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deeponto) (2.2.6)\n",
            "Requirement already satisfied: scikit_learn in /usr/local/lib/python3.11/dist-packages (from deeponto) (1.6.1)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.11/dist-packages (from deeponto) (4.51.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from deeponto) (3.6.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (from deeponto) (3.8.5)\n",
            "Requirement already satisfied: pprintpp in /usr/local/lib/python3.11/dist-packages (from deeponto) (0.4.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from deeponto) (3.4.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from deeponto) (5.4.0)\n",
            "Requirement already satisfied: textdistance in /usr/local/lib/python3.11/dist-packages (from deeponto) (4.6.3)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (from deeponto) (7.7.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from deeponto) (6.17.1)\n",
            "Requirement already satisfied: enlighten in /usr/local/lib/python3.11/dist-packages (from deeponto) (1.14.1)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.11/dist-packages (from deeponto) (7.1.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from deeponto) (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (20.0.0)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (6.0.2)\n",
            "Requirement already satisfied: blessed>=1.17.7 in /usr/local/lib/python3.11/dist-packages (from enlighten->deeponto) (1.21.0)\n",
            "Requirement already satisfied: prefixed>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from enlighten->deeponto) (0.9.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (1.8.0)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (6.4.2)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (5.7.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->deeponto) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->deeponto) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->deeponto) (3.0.15)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->deeponto) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->deeponto) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->deeponto) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->deeponto) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->deeponto) (2025.2)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib->deeponto) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn->deeponto) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn->deeponto) (3.6.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (0.15.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (4.13.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->deeponto) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]->deeponto) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]->deeponto) (0.5.3)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]->deeponto) (1.6.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from blessed>=1.17.7->enlighten->deeponto) (0.2.13)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (3.11.15)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (5.2.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (4.9.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel->deeponto) (5.7.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->deeponto) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->deeponto) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->deeponto) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->deeponto) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->deeponto) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->deeponto) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->deeponto) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->deeponto) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->deeponto) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->deeponto) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->deeponto) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy->deeponto) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy->deeponto) (14.0.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->deeponto) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->deeponto) (7.1.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->deeponto) (6.5.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy->deeponto) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (1.20.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->deeponto) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel->deeponto) (4.3.8)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->deeponto) (1.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (23.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->deeponto) (0.7.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->deeponto) (3.0.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->deeponto) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->deeponto) (0.1.2)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (4.23.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.24.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (2.7)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (2.22)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "# === Base Libraries ===\n",
        "!pip install numpy --upgrade\n",
        "!pip install pandas\n",
        "!pip install optuna\n",
        "\n",
        "# === FAISS (for Approximate Nearest Neighbor Search) ===\n",
        "!pip install faiss-cpu        # CPU version (recommended unless using GPU)\n",
        "# !pip install faiss-gpu      # Uncomment if running on CUDA-enabled GPU\n",
        "\n",
        "# === PyTorch Geometric and dependencies ===\n",
        "!pip install torch-geometric==2.4.0\n",
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
        "# Optional: latest dev version from GitHub\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "\n",
        "# === DeepOnto (Ontology Matching Toolkit) ===\n",
        "!pip install deeponto\n",
        "# Optionally install custom version from a GitHub repository\n",
        "# !pip install git+https://github.com/<username>/deeponto.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nFonRjT5fMCv"
      },
      "outputs": [],
      "source": [
        "# Import pandas for working with tabular data (e.g., CSV, TSV files)\n",
        "import pandas as pd\n",
        "\n",
        "# Import numpy for numerical operations and efficient array handling\n",
        "import numpy as np\n",
        "\n",
        "# Import json for reading and writing JSON-formatted files (useful for config or ontology structures)\n",
        "import json\n",
        "\n",
        "# Import pickle for serializing and deserializing Python objects (e.g., saving models or processed data)\n",
        "import pickle\n",
        "\n",
        "# Import warnings to control or suppress warning messages during runtime\n",
        "import warnings\n",
        "\n",
        "# Import gc (garbage collector) for managing memory manually when dealing with large datasets\n",
        "import gc\n",
        "\n",
        "# Ignore all warning messages to keep the output clean\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uchfZJP2fZwe"
      },
      "outputs": [],
      "source": [
        "# Import PyTorch core library for tensor operations and model definition\n",
        "import torch\n",
        "\n",
        "# Import commonly used PyTorch components\n",
        "from torch import Tensor, optim  # Tensor type and optimization algorithms (e.g., SGD, Adam)\n",
        "\n",
        "# Import PyTorch's neural network module (base class for defining models)\n",
        "import torch.nn as nn\n",
        "\n",
        "# Import PyTorch's functional API for operations like activations and loss functions\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Import DataLoader utilities for batching and loading datasets during training\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# === PyTorch Geometric (PyG) modules for graph-based learning ===\n",
        "\n",
        "# Basic graph data structure from PyG\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# PyG-specific DataLoader for batching graphs\n",
        "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
        "\n",
        "# Import graph convolution layers and pooling functions from PyG\n",
        "from torch_geometric.nn import (\n",
        "    GCNConv,             # Graph Convolutional Network layer\n",
        "    GINConv,             # Graph Isomorphism Network convolution\n",
        "    global_mean_pool,    # Global mean pooling over node embeddings\n",
        "    global_add_pool,     # Global sum pooling over node embeddings\n",
        "    MessagePassing       # Base class for defining custom GNN layers\n",
        ")\n",
        "\n",
        "# Explicitly re-import MessagePassing (optional if already above)\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "\n",
        "# Graph utility functions from PyG\n",
        "from torch_geometric.utils import (\n",
        "    to_undirected,       # Converts a directed graph to undirected\n",
        "    softmax              # Softmax over edges (e.g., for attention)\n",
        ")\n",
        "\n",
        "# Initialization utilities for GNN layers\n",
        "from torch_geometric.nn.inits import (\n",
        "    reset,               # Reset parameters\n",
        "    glorot,              # Glorot (Xavier) weight initialization\n",
        "    zeros                # Zero initialization\n",
        ")\n",
        "\n",
        "# Typing utilities from PyG for adjacency and tensor specifications\n",
        "from torch_geometric.typing import (\n",
        "    Adj, OptTensor, PairTensor, SparseTensor\n",
        ")\n",
        "\n",
        "# Dense linear transformation layer from PyG (alternative to torch.nn.Linear)\n",
        "from torch_geometric.nn.dense.linear import Linear\n",
        "\n",
        "# Additional PyTorch neural network components\n",
        "from torch.nn import (\n",
        "    Linear,             # Fully connected (dense) layer\n",
        "    PReLU,              # Parametric ReLU activation\n",
        "    Sequential,         # Layer container for building sequential models\n",
        "    BatchNorm1d,        # Batch normalization for 1D inputs\n",
        "    Dropout             # Dropout regularization\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ziMBSWE8ff1N"
      },
      "outputs": [],
      "source": [
        "# Import matplotlib for creating visualizations (e.g., loss curves, evaluation metrics, embedding projections)\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JeAvp6PNfiLh"
      },
      "outputs": [],
      "source": [
        "# Import function to split data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import encoder to convert categorical labels into integer values (useful for classification tasks)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Import evaluation metrics for classification and regression tasks\n",
        "from sklearn.metrics import (\n",
        "    f1_score,            # Harmonic mean of precision and recall; useful for imbalanced classification\n",
        "    precision_score,     # Measures the proportion of true positives among all predicted positives\n",
        "    accuracy_score,      # Measures overall correctness of predictions (classification)\n",
        "    mean_squared_error,  # Measures average squared difference between predicted and actual values (regression)\n",
        "    mean_absolute_error  # Measures average absolute difference between predicted and actual values (regression)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jm1rMZvmfl2M",
        "outputId": "f07b1d37-6c61-4375-f3a0-7ee7d2a03314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the maximum memory located to JVM [8g]: 8g\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import the Ontology class for loading and manipulating OWL ontologies\n",
        "from deeponto.onto import Ontology\n",
        "\n",
        "# Import all components related to OAEI (Ontology Alignment Evaluation Initiative) benchmarking\n",
        "from deeponto.align.oaei import *\n",
        "\n",
        "# Import data structures for representing mappings between ontology entities\n",
        "from deeponto.align.mapping import EntityMapping, ReferenceMapping\n",
        "# - EntityMapping: represents a predicted alignment (one or more mappings)\n",
        "# - ReferenceMapping: represents the gold standard/reference alignments\n",
        "\n",
        "# Import the evaluator to compute Precision, Recall, and F1-score for alignments\n",
        "from deeponto.align.evaluation import AlignmentEvaluator\n",
        "\n",
        "# Utility function to read TSV/CSV tables as mapping or data frames\n",
        "from deeponto.utils import read_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JYhwr3Q_ft2N"
      },
      "outputs": [],
      "source": [
        "# Import Optuna, a hyperparameter optimization framework for automating model tuning using strategies like Bayesian optimization\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UmSCo5Olfzuz"
      },
      "outputs": [],
      "source": [
        "# Import the math module for mathematical functions (e.g., sqrt, log, exp)\n",
        "import math\n",
        "\n",
        "# Import the time module for measuring execution time of code blocks or functions\n",
        "import time\n",
        "\n",
        "# Import typing annotations for function signatures and code clarity\n",
        "from typing import Optional, Tuple, Union, Callable\n",
        "# - Optional[T]: denotes a value that could be of type T or None\n",
        "# - Tuple: fixed-size ordered collection of elements\n",
        "# - Union: allows multiple possible types (e.g., Union[int, str])\n",
        "# - Callable: represents a function or method type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9WNn0OMQW2CS"
      },
      "outputs": [],
      "source": [
        "# Import Python's built-in random module for generating pseudo-random numbers\n",
        "import random\n",
        "\n",
        "# Set the seed for PyTorch's random number generator to ensure reproducibility\n",
        "import torch\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Set the seed for NumPy's random number generator to ensure reproducibility\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "\n",
        "# Set the seed for Python's built-in random module to ensure reproducibility\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-abbBHOoRdWl"
      },
      "source": [
        "# **Paths Definition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVgl_Bb42naS",
        "outputId": "68ad3810-2d89-44d5-f6af-62958dd11a53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Importing the 'drive' module from Google Colab to interact with Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount the user's Google Drive to the Colab environment\n",
        "# After running this, a link will appear to authorize access, and Google Drive will be mounted at '/content/gdrive'\n",
        "drive.mount('/content/gdrive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "36ttssQ3W7cx"
      },
      "outputs": [],
      "source": [
        "# Define the source ontology name\n",
        "src_ent = \"ncit\"\n",
        "\n",
        "# Define the target ontology name\n",
        "tgt_ent = \"doid\"\n",
        "\n",
        "# Define the task name for this ontology matching process\n",
        "task = \"ncit2doid\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SJpvkdwVSQye"
      },
      "outputs": [],
      "source": [
        "dir = \"/content/gdrive/My Drive/BioGITOM-VLDB/\"\n",
        "\n",
        "# Define the directory for the dataset containing source and target ontologies\n",
        "dataset_dir = f\"{dir}/Datasets/{task}\"\n",
        "\n",
        "# Define the data directory for storing embeddings, adjacency matrices, and related files\n",
        "data_dir = f\"{dir}/{task}/Data\"\n",
        "\n",
        "# Define the directory for storing the results\n",
        "results_dir = f\"{dir}/{task}/Results\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eFDNSFef23er"
      },
      "outputs": [],
      "source": [
        "# Load the Source ontology using the Ontology class from DeepOnto\n",
        "# This initializes the source ontology by loading its .owl file.\n",
        "src_onto = Ontology(f\"{dataset_dir}/{src_ent}.owl\")\n",
        "\n",
        "# Load the Target ontology using the Ontology class from DeepOnto\n",
        "# This initializes the target ontology by loading its .owl file.\n",
        "tgt_onto = Ontology(f\"{dataset_dir}/{tgt_ent}.owl\")\n",
        "\n",
        "# Define the file path for the Source embeddings CSV file\n",
        "# Embeddings for the source ontology entities are stored in this file.\n",
        "src_Emb = f\"{data_dir}/{src_ent}_Sentence_SapBERT_emb.csv\"\n",
        "\n",
        "# Define the file path for the Target embeddings CSV file\n",
        "# Embeddings for the target ontology entities are stored in this file.\n",
        "tgt_Emb = f\"{data_dir}/{tgt_ent}_Sentence_SapBERT_emb.csv\"\n",
        "\n",
        "# Define the file path for the Source adjacency matrix\n",
        "# This file represents the relationships (edges) between entities in the source ontology.\n",
        "src_Adjacence = f\"{data_dir}/{src_ent}_adjacence.csv\"\n",
        "\n",
        "# Define the file path for the Target adjacency matrix\n",
        "# This file represents the relationships (edges) between entities in the target ontology.\n",
        "tgt_Adjacence = f\"{data_dir}/{tgt_ent}_adjacence.csv\"\n",
        "\n",
        "# Define the file path for the JSON file containing the Source ontology class labels\n",
        "# This file maps the source ontology entities to their labels or names.\n",
        "src_class = f\"{data_dir}/{src_ent}_classes.json\"\n",
        "\n",
        "# Define the file path for the JSON file containing the Target ontology class labels\n",
        "# This file maps the target ontology entities to their labels or names.\n",
        "tgt_class = f\"{data_dir}/{tgt_ent}_classes.json\"\n",
        "\n",
        "# Define the file path for the train data\n",
        "train_file = f\"{data_dir}/{task}_train_50.csv\"\n",
        "\n",
        "# Define the file path for the test data\n",
        "# The test file contains reference mappings (ground truth) between the source and target ontologies.\n",
        "test_file = f\"{dataset_dir}/refs_equiv/test.tsv\"\n",
        "\n",
        "# Define the file path for the candidate mappings used during testing\n",
        "# This file includes the candidate pairs (source and target entities) for ranking based metrics.\n",
        "test_cands = f\"{dataset_dir}/refs_equiv/test.cands.tsv\"\n",
        "cands_path = f\"{data_dir}/{task}_cands.csv\"\n",
        "\n",
        "# Define the path where the prediction results will be saved in TSV format\n",
        "# This file will store the final predictions (mappings) between source and target entities.\n",
        "prediction_path = f\"{results_dir}/{task}_matching_results.tsv\"\n",
        "\n",
        "# Define the path where all prediction results will be saved in TSV format\n",
        "# This file will store detailed prediction results, including all candidate scores.\n",
        "all_predictions_path = f\"{results_dir}/{task}_all_predictions.tsv\"\n",
        "\n",
        "# Define the path where formatted ranking predictions will be saved in TSV format\n",
        "# This file will contain predictions formatted for evaluation using ranking-based metrics.\n",
        "formatted_predictions_path = f\"{results_dir}/{task}_formatted_predictions.tsv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqEXsgPGMVhw"
      },
      "source": [
        "# **GIT Architecture**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "A_d6XCsUMVhx"
      },
      "outputs": [],
      "source": [
        "# RGIT class definition which inherits from PyTorch Geometric's MessagePassing class\n",
        "class RGIT(MessagePassing):\n",
        "\n",
        "    _alpha: OptTensor  # Define _alpha as an optional tensor for storing attention weights\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        nn: Callable,  # Neural network to be used in the final layer of the GNN\n",
        "        in_channels: Union[int, Tuple[int, int]],  # Input dimension, can be a single or pair of integers\n",
        "        out_channels: int,  # Output dimension of the GNN\n",
        "        eps: float = 0.,  # GIN parameter: epsilon for GIN aggregation\n",
        "        train_eps: bool = False,  # GIN parameter: whether epsilon should be learnable\n",
        "        heads: int = 1,  # Transformer parameter: number of attention heads\n",
        "        dropout: float = 0.,  # Dropout rate for attention weights\n",
        "        edge_dim: Optional[int] = None,  # Dimension for edge attributes (optional)\n",
        "        bias: bool = True,  # Whether to use bias in linear layers\n",
        "        root_weight: bool = True,  # GIN parameter: whether to apply root weight in aggregation\n",
        "        **kwargs,  # Additional arguments passed to the parent class\n",
        "    ):\n",
        "        # Set the aggregation type to 'add' and initialize the parent class with node_dim=0\n",
        "        kwargs.setdefault('aggr', 'add')\n",
        "        super().__init__(node_dim=0, **kwargs)\n",
        "\n",
        "        # Initialize input/output dimensions, neural network, and GIN/transformer parameters\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.nn = nn  # Neural network used by the GNN\n",
        "        self.initial_eps = eps  # Initial value of epsilon for GIN\n",
        "\n",
        "        # Set epsilon to be learnable or fixed\n",
        "        if train_eps:\n",
        "            self.eps = torch.nn.Parameter(torch.empty(1))  # Learnable epsilon\n",
        "        else:\n",
        "            self.register_buffer('eps', torch.empty(1))  # Non-learnable epsilon (fixed)\n",
        "\n",
        "        # Initialize transformer-related parameters\n",
        "        self.heads = heads\n",
        "        self.dropout = dropout\n",
        "        self.edge_dim = edge_dim\n",
        "        self._alpha = None  # Placeholder for attention weights\n",
        "\n",
        "        # Handle case where in_channels is a single integer or a tuple\n",
        "        if isinstance(in_channels, int):\n",
        "            in_channels = (in_channels, in_channels)\n",
        "\n",
        "        # Define the linear layers for key, query, and value for the transformer mechanism\n",
        "        self.lin_key = Linear(in_channels[0], heads * out_channels)\n",
        "        self.lin_query = Linear(in_channels[1], heads * out_channels)\n",
        "        self.lin_value = Linear(in_channels[0], heads * out_channels)\n",
        "\n",
        "        # Define linear transformation for edge embeddings if provided\n",
        "        if edge_dim is not None:\n",
        "            self.lin_edge = Linear(edge_dim, heads * out_channels, bias=False)\n",
        "        else:\n",
        "            self.lin_edge = self.register_parameter('lin_edge', None)\n",
        "\n",
        "        # Reset all parameters to their initial values\n",
        "        self.reset_parameters()\n",
        "\n",
        "    # Function to reset model parameters\n",
        "    def reset_parameters(self):\n",
        "        super().reset_parameters()  # Call parent class reset method\n",
        "        self.lin_key.reset_parameters()  # Reset key linear layer\n",
        "        self.lin_query.reset_parameters()  # Reset query linear layer\n",
        "        self.lin_value.reset_parameters()  # Reset value linear layer\n",
        "        if self.edge_dim:\n",
        "            self.lin_edge.reset_parameters()  # Reset edge linear layer if used\n",
        "        reset(self.nn)  # Reset the neural network provided\n",
        "        self.eps.data.fill_(self.initial_eps)  # Initialize epsilon with the starting value\n",
        "\n",
        "    # Forward function defining how the input data flows through the model\n",
        "    def forward(self, x: Union[Tensor, PairTensor], edge_index: Adj,\n",
        "                edge_attr: OptTensor = None, return_attention_weights=None):\n",
        "        # Unpack number of heads and output channels\n",
        "        H, C = self.heads, self.out_channels\n",
        "\n",
        "        # If x is a tensor, treat it as a pair of tensors (source and target embeddings)\n",
        "        if isinstance(x, Tensor):\n",
        "            x: PairTensor = (x, x)\n",
        "\n",
        "        # Extract source node embeddings\n",
        "        x_t = x[0]\n",
        "\n",
        "        # Apply linear transformations and reshape query, key, and value for multi-head attention\n",
        "        query = self.lin_query(x[1]).view(-1, H, C)\n",
        "        key = self.lin_key(x[0]).view(-1, H, C)\n",
        "        value = self.lin_value(x[0]).view(-1, H, C)\n",
        "\n",
        "        # Propagate messages through the graph using the propagate function\n",
        "        out = self.propagate(edge_index, query=query, key=key, value=value,\n",
        "                             edge_attr=edge_attr, size=None)\n",
        "\n",
        "        # Retrieve attention weights and reset them\n",
        "        alpha = self._alpha\n",
        "        self._alpha = None  # Reset _alpha after use\n",
        "        out = out.mean(dim=1)  # Take the mean over all attention heads\n",
        "\n",
        "        # Apply GIN aggregation by adding epsilon-scaled original node embeddings\n",
        "        out = out + (1 + self.eps) * x_t\n",
        "        return self.nn(out)  # Pass through the neural network\n",
        "\n",
        "    # Message passing function which calculates attention and combines messages\n",
        "    def message(self, query_i: Tensor, key_j: Tensor, value_j: Tensor,\n",
        "                edge_attr: OptTensor, index: Tensor, ptr: OptTensor,\n",
        "                size_i: Optional[int]) -> Tensor:\n",
        "        # If edge attributes are used, apply linear transformation and add them to the key\n",
        "        if self.lin_edge is not None:\n",
        "            assert edge_attr is not None\n",
        "            edge_attr = self.lin_edge(edge_attr).view(-1, self.heads, self.out_channels)\n",
        "            key_j = key_j + edge_attr\n",
        "\n",
        "        # Calculate attention (alpha) using the dot product between query and key\n",
        "        alpha = (query_i * key_j).sum(dim=-1) / math.sqrt(self.out_channels)\n",
        "        alpha = softmax(alpha, index, ptr, size_i)  # Apply softmax to normalize attention\n",
        "        self._alpha = alpha  # Store attention weights\n",
        "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)  # Apply dropout\n",
        "\n",
        "        # Calculate the output message by applying attention to the value\n",
        "        out = value_j\n",
        "        if edge_attr is not None:\n",
        "            out = out + edge_attr  # Add edge embeddings to the output if present\n",
        "        out = out * alpha.view(-1, self.heads, 1)  # Scale by attention weights\n",
        "        return out\n",
        "\n",
        "    # String representation function for debugging or printing\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
        "                f'{self.out_channels}, heads={self.heads})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qwFv6RgHmGCf"
      },
      "outputs": [],
      "source": [
        "# Define the RGIT_mod class, a multi-layer GNN that uses both RGIT and linear layers\n",
        "class RGIT_mod(torch.nn.Module):\n",
        "    \"\"\"Multi-layer RGIT with optional linear layers\"\"\"\n",
        "\n",
        "    # Initialize the model with hidden dimension, number of RGIT layers, and number of linear layers\n",
        "    def __init__(self, dim_h, num_layers, num_linear_layers=1):\n",
        "        super(RGIT_mod, self).__init__()\n",
        "        self.num_layers = num_layers  # Number of RGIT layers\n",
        "        self.num_linear_layers = num_linear_layers  # Number of linear layers\n",
        "        self.linears = torch.nn.ModuleList()  # List to store linear layers\n",
        "        self.rgit_layers = torch.nn.ModuleList()  # List to store RGIT layers\n",
        "\n",
        "        # Create a list of Linear and PReLU layers (for encoding entity names)\n",
        "        for _ in range(num_linear_layers):\n",
        "            self.linears.append(Linear(dim_h, dim_h))  # Linear transformation layer\n",
        "            self.linears.append(PReLU(num_parameters=dim_h))  # Parametric ReLU activation function\n",
        "\n",
        "        # Create a list of RGIT layers\n",
        "        for _ in range(num_layers):\n",
        "            self.rgit_layers.append(RGIT(  # Each RGIT layer contains a small MLP with Linear and PReLU\n",
        "                Sequential(Linear(dim_h, dim_h), PReLU(num_parameters=dim_h),\n",
        "                           Linear(dim_h, dim_h), PReLU(num_parameters=dim_h)), dim_h, dim_h))\n",
        "\n",
        "    # Forward pass through the model\n",
        "    def forward(self, x, edge_index):\n",
        "        # Apply the linear layers first to the input\n",
        "        for layer in self.linears:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Then apply the RGIT layers for message passing\n",
        "        for layer in self.rgit_layers:\n",
        "            x = layer(x, edge_index)\n",
        "\n",
        "        return x  # Return the final node embeddings after all layers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxCn5ztKVztw"
      },
      "source": [
        "# **Gated Network Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7MKQUv7o7zay"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "class GatedCombinationWithFaiss(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Linear layers to compute gating values for the source and target embeddings\n",
        "        self.gate_A_fc = nn.Linear(input_dim, input_dim)\n",
        "        self.gate_B_fc = nn.Linear(input_dim, input_dim)\n",
        "\n",
        "        # Final linear layer to map similarity score to prediction (sigmoid output)\n",
        "        self.fc = nn.Linear(1, 1)\n",
        "\n",
        "    def faiss_l2(self, a, b):\n",
        "        \"\"\"\n",
        "        Compute L2 distances using FAISS (non-differentiable).\n",
        "        This function converts tensors to NumPy, builds a FAISS index, and performs a search.\n",
        "        Only use this during inference or evaluation — not for training.\n",
        "\n",
        "        Args:\n",
        "            a (Tensor): Query vectors (batch_size x dim)\n",
        "            b (Tensor): Database vectors (batch_size x dim)\n",
        "\n",
        "        Returns:\n",
        "            Tensor: L2 distances between aligned rows (one-to-one)\n",
        "        \"\"\"\n",
        "        # Detach tensors from the computation graph and move to CPU\n",
        "        a_np = a.detach().cpu().numpy().astype(np.float32)\n",
        "        b_np = b.detach().cpu().numpy().astype(np.float32)\n",
        "\n",
        "        # Create a FAISS index for L2 distance\n",
        "        index = faiss.IndexFlatL2(a_np.shape[1])\n",
        "        index.add(b_np)\n",
        "\n",
        "        # Perform 1-NN search\n",
        "        distances, _ = index.search(a_np, 1)  # shape: (batch_size, 1)\n",
        "\n",
        "        # Convert back to PyTorch tensor on the original device\n",
        "        return torch.tensor(distances[:, 0], dtype=torch.float32, device=a.device)\n",
        "\n",
        "    def forward(self, x1, x2, x3, x4, return_embeddings=False):\n",
        "        \"\"\"\n",
        "        Forward pass through the gated combination model.\n",
        "        Combines original and transformed embeddings using learned gates.\n",
        "\n",
        "        Args:\n",
        "            x1, x2: original and GNN-transformed embeddings for source entities\n",
        "            x3, x4: original and GNN-transformed embeddings for target entities\n",
        "            return_embeddings (bool): if True, return gated embeddings instead of prediction\n",
        "\n",
        "        Returns:\n",
        "            Tensor: similarity score (if return_embeddings=False)\n",
        "            OR\n",
        "            Tuple[Tensor, Tensor]: gated source and target embeddings (if return_embeddings=True)\n",
        "        \"\"\"\n",
        "\n",
        "        # Compute gate for source embeddings\n",
        "        gate_values1 = torch.sigmoid(self.gate_A_fc(x1))\n",
        "        a = x1 * gate_values1 + x2 * (1 - gate_values1)\n",
        "\n",
        "        # Compute gate for target embeddings\n",
        "        gate_values2 = torch.sigmoid(self.gate_B_fc(x3))\n",
        "        b = x3 * gate_values2 + x4 * (1 - gate_values2)\n",
        "\n",
        "        if return_embeddings:\n",
        "            return a, b\n",
        "\n",
        "        # Compute non-differentiable distance with FAISS (1-to-1)\n",
        "        distance = self.faiss_l2(a, b)\n",
        "\n",
        "        # Pass through a sigmoid layer for binary classification output\n",
        "        out = torch.sigmoid(self.fc(distance.unsqueeze(1)))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLJ5j9FNMVhy"
      },
      "source": [
        "# **Utility functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "k0L86DgUQjMU"
      },
      "outputs": [],
      "source": [
        "def adjacency_matrix_to_undirected_edge_index(adjacency_matrix):\n",
        "    \"\"\"\n",
        "    Converts an adjacency matrix into an undirected edge index for use in graph-based neural networks.\n",
        "\n",
        "    Args:\n",
        "        adjacency_matrix: A 2D list or array representing the adjacency matrix of a graph.\n",
        "\n",
        "    Returns:\n",
        "        edge_index_undirected: A PyTorch tensor representing the undirected edges.\n",
        "    \"\"\"\n",
        "    # Convert each element in the adjacency matrix to an integer (from boolean or float)\n",
        "    adjacency_matrix = [[int(element) for element in sublist] for sublist in adjacency_matrix]\n",
        "\n",
        "    # Convert the adjacency matrix into a PyTorch LongTensor (used for indexing)\n",
        "    edge_index = torch.tensor(adjacency_matrix, dtype=torch.long)\n",
        "\n",
        "    # Transpose the edge_index tensor so that rows represent edges in the form [source, target]\n",
        "    edge_index = edge_index.t().contiguous()\n",
        "\n",
        "    # Convert the directed edge_index into an undirected edge_index, meaning both directions are added (i.e., (i, j) and (j, i))\n",
        "    edge_index_undirected = to_undirected(edge_index)\n",
        "\n",
        "    return edge_index_undirected  # Return the undirected edge index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YvmOxkLcpf9w"
      },
      "outputs": [],
      "source": [
        "def build_indexed_dict(file_path):\n",
        "    \"\"\"\n",
        "    Builds a dictionary with numeric indexes for each key from a JSON file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the JSON file.\n",
        "\n",
        "    Returns:\n",
        "        indexed_dict (dict): A new dictionary where each key from the JSON file is assigned a numeric index.\n",
        "    \"\"\"\n",
        "    # Load the JSON file into a Python dictionary\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Create a new dictionary with numeric indexes as keys and the original JSON keys as values\n",
        "    indexed_dict = {index: key for index, key in enumerate(data.keys())}\n",
        "\n",
        "    return indexed_dict  # Return the newly created dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "QgFINoPGl9Wg"
      },
      "outputs": [],
      "source": [
        "def select_rows_by_index(embedding_vector, index_vector):\n",
        "    \"\"\"\n",
        "    Select rows from an embedding vector using an index vector.\n",
        "\n",
        "    Args:\n",
        "        embedding_vector (torch.Tensor): 2D tensor representing the embedding vector with shape [num_rows, embedding_size].\n",
        "        index_vector (torch.Tensor): 1D tensor representing the index vector.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: New tensor with selected rows from the embedding vector.\n",
        "    \"\"\"\n",
        "    # Use torch.index_select to select the desired rows\n",
        "    new_tensor = torch.index_select(embedding_vector, 0, index_vector)\n",
        "\n",
        "    return new_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "a12L7vEmmCJq"
      },
      "outputs": [],
      "source": [
        "def contrastive_loss(source_embeddings, target_embeddings, labels, margin=1.0):\n",
        "    \"\"\"\n",
        "    Computes the contrastive loss, a type of loss function used to train models in tasks like matching or similarity learning.\n",
        "\n",
        "    Args:\n",
        "        source_embeddings (torch.Tensor): Embeddings of the source graphs, shape [batch_size, embedding_size].\n",
        "        target_embeddings (torch.Tensor): Embeddings of the target graphs, shape [batch_size, embedding_size].\n",
        "        labels (torch.Tensor): Binary labels indicating if the pairs are matched (1) or not (0), shape [batch_size].\n",
        "        margin (float): Margin value for the contrastive loss. Defaults to 1.0.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The contrastive loss value.\n",
        "    \"\"\"\n",
        "    # Calculate the pairwise Euclidean distance between source and target embeddings\n",
        "    distances = F.pairwise_distance(source_embeddings, target_embeddings)\n",
        "\n",
        "    # Compute the contrastive loss:\n",
        "    # - For matched pairs (label == 1), the loss is the squared distance between embeddings.\n",
        "    # - For non-matched pairs (label == 0), the loss is based on how far apart the embeddings are,\n",
        "    #   but penalizes them only if the distance is less than the margin.\n",
        "    loss = torch.mean(\n",
        "        labels * 0.4 * distances.pow(2) +  # For positive pairs, minimize the distance (squared)\n",
        "        (1 - labels) * 0.4 * torch.max(torch.zeros_like(distances), margin - distances).pow(2)  # For negative pairs, maximize the distance (up to the margin)\n",
        "    )\n",
        "\n",
        "    return loss  # Return the computed contrastive loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_ggVYlTiO_WA"
      },
      "outputs": [],
      "source": [
        "def compute_mrr_and_hits(reference_file, predicted_file, output_file, k_values=[1, 5, 10]):\n",
        "    \"\"\"\n",
        "    Compute Mean Reciprocal Rank (MRR) and Hits@k metrics for ontology matching results.\n",
        "\n",
        "    Args:\n",
        "        reference_file (str): Path to the reference test candidate file (usually 'test.cands.tsv').\n",
        "        predicted_file (str): Path to the prediction results (with columns: SrcEntity, TgtEntity, Score).\n",
        "        output_file (str): Path to save ranked candidate predictions with scores.\n",
        "        k_values (list): List of integers specifying which Hits@k metrics to compute.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with MRR and Hits@k scores.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load reference candidate mappings: each row = (SrcEntity, CorrectTgtEntity, [CandidateTgtEntities])\n",
        "    test_candidate_mappings = read_table(reference_file).values.tolist()\n",
        "\n",
        "    # Load predictions and ensure Score is float\n",
        "    predicted_data = pd.read_csv(predicted_file, sep=\"\\t\")\n",
        "    predicted_data[\"Score\"] = predicted_data[\"Score\"].apply(\n",
        "        lambda x: float(x.strip(\"[]\")) if isinstance(x, str) else float(x)\n",
        "    )\n",
        "\n",
        "    # Create a dictionary mapping (SrcEntity, TgtEntity) -> predicted score\n",
        "    score_lookup = {\n",
        "        (row[\"SrcEntity\"], row[\"TgtEntity\"]): row[\"Score\"]\n",
        "        for _, row in predicted_data.iterrows()\n",
        "    }\n",
        "\n",
        "    ranking_results = []\n",
        "\n",
        "    # Rank the candidates for each source entity\n",
        "    for src_ref_class, tgt_ref_class, tgt_cands in test_candidate_mappings:\n",
        "        # Safely parse the candidate list (tgt_cands is a stringified list)\n",
        "        try:\n",
        "            tgt_cands = eval(tgt_cands)\n",
        "        except Exception:\n",
        "            tgt_cands = []\n",
        "\n",
        "        # Score each candidate (use a large negative default if not found)\n",
        "        scored_cands = [\n",
        "            (tgt_cand, score_lookup.get((src_ref_class, tgt_cand), -1e9))\n",
        "            for tgt_cand in tgt_cands\n",
        "        ]\n",
        "\n",
        "        # Sort candidates by score descending\n",
        "        scored_cands = sorted(scored_cands, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Store the ranking result\n",
        "        ranking_results.append((src_ref_class, tgt_ref_class, scored_cands))\n",
        "\n",
        "    # Save ranked predictions for inspection/debugging\n",
        "    pd.DataFrame(ranking_results, columns=[\"SrcEntity\", \"TgtEntity\", \"TgtCandidates\"]).to_csv(\n",
        "        output_file, sep=\"\\t\", index=False\n",
        "    )\n",
        "\n",
        "    # === Evaluation: compute MRR and Hits@k ===\n",
        "    total_entities = len(ranking_results)\n",
        "    reciprocal_ranks = []\n",
        "    hits_at_k = {k: 0 for k in k_values}\n",
        "\n",
        "    for src_entity, tgt_ref_class, tgt_cands in ranking_results:\n",
        "        ranked_candidates = [cand[0] for cand in tgt_cands]  # candidate URIs only\n",
        "        if tgt_ref_class in ranked_candidates:\n",
        "            rank = ranked_candidates.index(tgt_ref_class) + 1\n",
        "            reciprocal_ranks.append(1 / rank)\n",
        "            for k in k_values:\n",
        "                if rank <= k:\n",
        "                    hits_at_k[k] += 1\n",
        "        else:\n",
        "            reciprocal_ranks.append(0)  # No correct match in candidate list\n",
        "\n",
        "    # Compute final metrics\n",
        "    mrr = sum(reciprocal_ranks) / total_entities\n",
        "    hits_at_k = {k: hits / total_entities for k, hits in hits_at_k.items()}\n",
        "\n",
        "    return {\"MRR\": mrr, \"Hits@k\": hits_at_k}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zmzBcuHZDOs3"
      },
      "outputs": [],
      "source": [
        "def save_gated_embeddings(gated_model, embeddings_src, x_src, embeddings_tgt, x_tgt,\n",
        "                          indexed_dict_src, indexed_dict_tgt,\n",
        "                          output_file_src, output_file_tgt):\n",
        "    \"\"\"\n",
        "    Compute and save the final entity embeddings generated by the GatedCombination model\n",
        "    for both source and target ontologies. Outputs include entity URIs and their final vectors.\n",
        "    Measures and prints the execution time of the entire operation.\n",
        "\n",
        "    Args:\n",
        "        gated_model (nn.Module): The trained GatedCombination model.\n",
        "        embeddings_src (Tensor): Structural embeddings for the source ontology.\n",
        "        x_src (Tensor): Semantic embeddings for the source ontology.\n",
        "        embeddings_tgt (Tensor): Structural embeddings for the target ontology.\n",
        "        x_tgt (Tensor): Semantic embeddings for the target ontology.\n",
        "        indexed_dict_src (dict): Index-to-URI mapping for the source ontology.\n",
        "        indexed_dict_tgt (dict): Index-to-URI mapping for the target ontology.\n",
        "        output_file_src (str): Path to save source embeddings (TSV).\n",
        "        output_file_tgt (str): Path to save target embeddings (TSV).\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    import torch\n",
        "    import time\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Use GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    gated_model = gated_model.to(device)\n",
        "    gated_model.eval()\n",
        "\n",
        "    # Move inputs to the same device\n",
        "    embeddings_src = embeddings_src.to(device)\n",
        "    x_src = x_src.to(device)\n",
        "    embeddings_tgt = embeddings_tgt.to(device)\n",
        "    x_tgt = x_tgt.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # === Source ontology ===\n",
        "        gate_src = torch.sigmoid(gated_model.gate_A_fc(embeddings_src))\n",
        "        final_src = embeddings_src * gate_src + x_src * (1 - gate_src)\n",
        "        final_src = final_src.cpu().numpy()\n",
        "\n",
        "        # === Target ontology ===\n",
        "        gate_tgt = torch.sigmoid(gated_model.gate_B_fc(embeddings_tgt))\n",
        "        final_tgt = embeddings_tgt * gate_tgt + x_tgt * (1 - gate_tgt)\n",
        "        final_tgt = final_tgt.cpu().numpy()\n",
        "\n",
        "    # Create DataFrames with Concept URI and embedding values\n",
        "    df_src = pd.DataFrame(final_src)\n",
        "    df_src.insert(0, \"Concept\", [indexed_dict_src[i] for i in range(len(df_src))])\n",
        "\n",
        "    df_tgt = pd.DataFrame(final_tgt)\n",
        "    df_tgt.insert(0, \"Concept\", [indexed_dict_tgt[i] for i in range(len(df_tgt))])\n",
        "\n",
        "    # Save embeddings to file\n",
        "    df_src.to_csv(output_file_src, sep='\\t', index=False)\n",
        "    df_tgt.to_csv(output_file_tgt, sep='\\t', index=False)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"✅ Gated embeddings saved:\\n- Source: {output_file_src}\\n- Target: {output_file_tgt}\")\n",
        "    print(f\"⏱️ Execution time: {elapsed_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_KZdtF46GHL4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def filter_ignored_class(src_emb_path, tgt_emb_path, src_onto, tgt_onto):\n",
        "    \"\"\"\n",
        "    Filters the source and target embedding files by removing concepts considered \"ignored classes\"\n",
        "    (e.g., owl:Thing, deprecated entities, etc.) based on both source and target ontologies.\n",
        "\n",
        "    Args:\n",
        "        src_emb_path (str): Path to the TSV file containing source embeddings with 'Concept' column.\n",
        "        tgt_emb_path (str): Path to the TSV file containing target embeddings with 'Concept' column.\n",
        "        src_onto (Ontology): Source ontology object loaded with DeepOnto.\n",
        "        tgt_onto (Ontology): Target ontology object loaded with DeepOnto.\n",
        "\n",
        "    Returns:\n",
        "        (str, str): Paths to the cleaned source and target embedding files.\n",
        "    \"\"\"\n",
        "\n",
        "    # === Load the embedding files ===\n",
        "    df_src = pd.read_csv(src_emb_path, sep='\\t', dtype=str)\n",
        "    print(f\"🔍 Initial source file: {len(df_src)} rows\")\n",
        "\n",
        "    df_tgt = pd.read_csv(tgt_emb_path, sep='\\t', dtype=str)\n",
        "    print(f\"🔍 Initial target file: {len(df_tgt)} rows\")\n",
        "\n",
        "    # === Step 1: Retrieve ignored classes from both ontologies ===\n",
        "    ignored_class_index = get_ignored_class_index(src_onto)  # e.g., owl:Thing, non-usable classes\n",
        "    ignored_class_index.update(get_ignored_class_index(tgt_onto))  # Merge with target ontology's ignored classes\n",
        "    ignored_uris = set(str(uri).strip() for uri in ignored_class_index)\n",
        "\n",
        "    # === Step 2: Remove rows where the 'Concept' column matches ignored URIs ===\n",
        "    df_src_cleaned = df_src[~df_src['Concept'].isin(ignored_uris)].reset_index(drop=True)\n",
        "    df_tgt_cleaned = df_tgt[~df_tgt['Concept'].isin(ignored_uris)].reset_index(drop=True)\n",
        "\n",
        "    print(f\"✅ Source after removing ignored classes: {len(df_src_cleaned)} rows\")\n",
        "    print(f\"✅ Target after removing ignored classes: {len(df_tgt_cleaned)} rows\")\n",
        "\n",
        "    # === Step 3: Save the cleaned embedding files ===\n",
        "    output_file_src = src_emb_path.replace(\".tsv\", \"_cleaned.tsv\")\n",
        "    output_file_tgt = tgt_emb_path.replace(\".tsv\", \"_cleaned.tsv\")\n",
        "\n",
        "    df_src_cleaned.to_csv(output_file_src, sep='\\t', index=False)\n",
        "    df_tgt_cleaned.to_csv(output_file_tgt, sep='\\t', index=False)\n",
        "\n",
        "    print(f\"📁 Cleaned source file saved to: {output_file_src}\")\n",
        "    print(f\"📁 Cleaned target file saved to: {output_file_tgt}\")\n",
        "\n",
        "    return output_file_src, output_file_tgt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_9YDcnTbKaHk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "def encode_embeddings_with_concept_column(encoder_model, input_file, output_file):\n",
        "    \"\"\"\n",
        "    Applies an encoder model to a set of embeddings (while preserving the 'Concept' column),\n",
        "    and saves the encoded results in the same tabular format.\n",
        "\n",
        "    Args:\n",
        "        encoder_model: A PyTorch model (e.g., LinearEncoder, MLPEncoder, etc.)\n",
        "        input_file (str): Path to the input TSV file containing 'Concept' and embedding vectors.\n",
        "        output_file (str): Path to save the encoded embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    # Select device (GPU if available, else CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Move the encoder model to the selected device and set it to evaluation mode\n",
        "    encoder_model = encoder_model.to(device)\n",
        "    encoder_model.eval()\n",
        "\n",
        "    # Load the input TSV file containing concept URIs and embeddings\n",
        "    df = pd.read_csv(input_file, sep='\\t')\n",
        "\n",
        "    # Extract the 'Concept' column to preserve URIs\n",
        "    concepts = df['Concept'].tolist()\n",
        "\n",
        "    # Extract the numerical embedding values (excluding the 'Concept' column)\n",
        "    embedding_values = df.drop(columns=['Concept']).values\n",
        "\n",
        "    # Convert the embedding matrix into a PyTorch tensor and move to the device\n",
        "    embeddings = torch.FloatTensor(embedding_values).to(device)\n",
        "\n",
        "    # Pass the embeddings through the encoder model without computing gradients\n",
        "    with torch.no_grad():\n",
        "        encoded = encoder_model(embeddings).cpu().numpy()\n",
        "\n",
        "    # Reconstruct a new DataFrame with the encoded vectors and corresponding URIs\n",
        "    df_encoded = pd.DataFrame(encoded, columns=[f'dim_{i}' for i in range(encoded.shape[1])])\n",
        "    df_encoded.insert(0, \"Concept\", concepts)  # Re-insert the 'Concept' column at the first position\n",
        "\n",
        "    # Save the encoded embeddings to a TSV file\n",
        "    df_encoded.to_csv(output_file, sep='\\t', index=False)\n",
        "    print(f\"✅ Encoded embeddings saved to: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HigIe6n_lQ8X"
      },
      "source": [
        "# **FAISS Similarity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zaX6JH9wj_WR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "import time\n",
        "\n",
        "def load_embeddings(src_emb_path, tgt_emb_path):\n",
        "    df_src = pd.read_csv(src_emb_path, sep='\\t')\n",
        "    df_tgt = pd.read_csv(tgt_emb_path, sep='\\t')\n",
        "    uris_src = df_src[\"Concept\"].values\n",
        "    uris_tgt = df_tgt[\"Concept\"].values\n",
        "    src_vecs = df_src.drop(columns=[\"Concept\"]).values.astype('float32')\n",
        "    tgt_vecs = df_tgt.drop(columns=[\"Concept\"]).values.astype('float32')\n",
        "    return uris_src, uris_tgt, src_vecs, tgt_vecs\n",
        "\n",
        "def save_results(uris_src, uris_tgt, indices, scores, output_file, top_k):\n",
        "    rows = []\n",
        "    for i, (ind_row, score_row) in enumerate(zip(indices, scores)):\n",
        "        src_uri = uris_src[i]\n",
        "        for j, tgt_idx in enumerate(ind_row):\n",
        "            tgt_uri = uris_tgt[tgt_idx]\n",
        "            score = score_row[j]\n",
        "            rows.append((src_uri, tgt_uri, score))\n",
        "    df_result = pd.DataFrame(rows, columns=[\"SrcEntity\", \"TgtEntity\", \"Score\"])\n",
        "    df_result.to_csv(output_file, sep='\\t', index=False)\n",
        "    print(f\"Top-{top_k} FAISS similarity results saved to: {output_file}\")\n",
        "\n",
        "def topk_faiss_l2(src_emb_path, tgt_emb_path, top_k=15, output_file=\"topk_l2.tsv\"):\n",
        "    print(\"🔹 Using L2 (Euclidean) distance with FAISS\")\n",
        "    start = time.time()\n",
        "\n",
        "    uris_src, uris_tgt, src_vecs, tgt_vecs = load_embeddings(src_emb_path, tgt_emb_path)\n",
        "    dim = src_vecs.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(tgt_vecs)\n",
        "    distances, indices = index.search(src_vecs, top_k)\n",
        "    similarity_scores = 1 / (1 + distances)\n",
        "\n",
        "    save_results(uris_src, uris_tgt, indices, similarity_scores, output_file, top_k)\n",
        "\n",
        "    print(f\"⏱️ Execution time: {time.time() - start:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjUYOFO7pdCg"
      },
      "source": [
        "# **Mappings Evaluation Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6m04nFw_R00"
      },
      "source": [
        "# **Precision, Recall, F1**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Strategy and Filtering Justification\n",
        "\n",
        "### Filtering Justification\n",
        "\n",
        "In the `evaluate_predictions` function, two important filtering steps are applied to ensure that the evaluation metrics (such as Precision, Recall, and F1-score) accurately reflect the model's performance:\n",
        "\n",
        "\n",
        "#### 1. Filtering Out Training-Only Entities\n",
        "\n",
        "We remove all predicted mappings involving source or target entities that are present **only in the training set** and not in the test set.\n",
        "\n",
        "This step is critical because:\n",
        "\n",
        "- In some datasets like **Bio-ML**, the same entity can appear in both training and test sets, although with **different correspondences**.\n",
        "- If we don't remove training-only entities, it can lead to **label leakage** and **metric distortion**.\n",
        "\n",
        "#### 2. Filtering on `SrcEntity` present in the test set\n",
        "\n",
        "The second step keeps only the predictions where the `SrcEntity` is included in the test reference set.\n",
        "\n",
        "- This eliminates **non-evaluable false positives**, i.e., predicted mappings for source entities that do not appear in the test set and therefore have no ground-truth correspondences. Including such predictions **unfairly penalizes precision and F1-score**, even though they are technically not verifiable errors.\n",
        "\n",
        "- It focuses the evaluation on entities with defined ground-truth mappings, which is critical for computing metrics such as :\n",
        "\n",
        "$P_{\\text{test}} = \\frac{|\\mathcal{M}_{\\text{out}} \\cap \\mathcal{M}_{\\text{test}}|}{|\\mathcal{M}_{\\text{out}} \\setminus (\\mathcal{M}_{\\text{ref}} \\setminus \\mathcal{M}_{\\text{test}})|}$.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "_GW0Am-TmVMR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "CdP6iYirLJW6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from deeponto.align.mapping import EntityMapping, ReferenceMapping\n",
        "from deeponto.align.evaluation import AlignmentEvaluator\n",
        "\n",
        "def evaluate_predictions(\n",
        "    topk_file,\n",
        "    train_file,\n",
        "    test_file,\n",
        "    src_onto,\n",
        "    tgt_onto,\n",
        "    threshold=0.0\n",
        "):\n",
        "    # === Step 1: Load input files ===\n",
        "    df = pd.read_csv(topk_file, sep='\\t', dtype=str)\n",
        "    train_df = pd.read_csv(train_file, sep=\"\\t\", dtype=str)\n",
        "    test_df = pd.read_csv(test_file, sep=\"\\t\", dtype=str)\n",
        "\n",
        "    # === Step 2: Remove URIs only present in training set ===\n",
        "    train_uris = set(train_df['SrcEntity']) | set(train_df['TgtEntity'])\n",
        "    test_uris = set(test_df['SrcEntity']) | set(test_df['TgtEntity'])\n",
        "    uris_to_exclude = train_uris - test_uris\n",
        "    df = df[~(df['SrcEntity'].isin(uris_to_exclude) | df['TgtEntity'].isin(uris_to_exclude))].reset_index(drop=True)\n",
        "\n",
        "    # === Step 3: Keep only source entities from the test set ===\n",
        "    src_entities_test = set(test_df['SrcEntity'])\n",
        "    df = df[df['SrcEntity'].isin(src_entities_test)].reset_index(drop=True)\n",
        "\n",
        "    # === Step 4: Save filtered Top-K predictions ===\n",
        "    output_file1 = topk_file.replace(\".tsv\", \"_filtered.tsv\")\n",
        "    df.to_csv(output_file1, sep='\\t', index=False)\n",
        "\n",
        "    # === Step 5: Convert score column to float and sort by descending score\n",
        "    df['Score'] = df['Score'].apply(lambda x: float(x.strip(\"[]\")) if isinstance(x, str) else float(x))\n",
        "    df_sorted = df.sort_values(by='Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # === Step 6: Apply greedy 1-1 matching constraint with score threshold\n",
        "    matched_sources = set()\n",
        "    matched_targets = set()\n",
        "    result = []\n",
        "    for _, row in df_sorted.iterrows():\n",
        "        src, tgt, score = row['SrcEntity'], row['TgtEntity'], row['Score']\n",
        "        if src not in matched_sources and tgt not in matched_targets and score >= threshold:\n",
        "            result.append((src, tgt, score))\n",
        "            matched_sources.add(src)\n",
        "            matched_targets.add(tgt)\n",
        "\n",
        "    # === Step 7: Save final Top-1 predictions ===\n",
        "    matching_results_df = pd.DataFrame(result, columns=['SrcEntity', 'TgtEntity', 'Score'])\n",
        "    output_file2 = topk_file.replace(\".tsv\", f\"_predictions.tsv\")\n",
        "    matching_results_df.to_csv(output_file2, sep='\\t', index=False)\n",
        "\n",
        "    print(f\"   ➤ Mappings file:   {output_file2}\")\n",
        "\n",
        "    # === Step 8: Evaluate against reference mappings\n",
        "    preds = EntityMapping.read_table_mappings(output_file2)\n",
        "    refs = ReferenceMapping.read_table_mappings(test_file)\n",
        "\n",
        "    preds_set = {p.to_tuple() for p in preds}\n",
        "    refs_set = {r.to_tuple() for r in refs}\n",
        "    correct = len(preds_set & refs_set)\n",
        "\n",
        "    results = AlignmentEvaluator.f1(preds, refs)\n",
        "\n",
        "    # === Step 9: Print evaluation metrics\n",
        "    print(\"\\n🎯 Evaluation Summary:\")\n",
        "    print(f\"   - Correct mappings:     {correct}\")\n",
        "    print(f\"   - Total predictions:    {len(preds)}\")\n",
        "    print(f\"   - Total references:     {len(refs)}\")\n",
        "    print(f\"📊 Precision:              {results['P']:.3f}\")\n",
        "    print(f\"📊 Recall:                 {results['R']:.3f}\")\n",
        "    print(f\"📊 F1-score:               {results['F1']:.3f}\\n\")\n",
        "\n",
        "    return output_file2, results, correct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPuzmu6f_Y8W"
      },
      "source": [
        "# **Precision@k, Recall@k, F1@k**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "UwdxR-ZzAgS3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "def evaluate_topk(topk_file, train_file, test_file, k=1, threshold=0.0):\n",
        "    \"\"\"\n",
        "    Evaluate Top-K predictions using Precision, Recall, and F1-score,\n",
        "    after filtering out training-only URIs, keeping only test sources, and applying 1-1 constraint.\n",
        "\n",
        "    Args:\n",
        "        topk_file (str): Path to the top-k prediction file (TSV with SrcEntity, TgtEntity, Score)\n",
        "        train_file (str): Path to the training mappings file (TSV)\n",
        "        test_file (str): Path to the test mappings file (TSV)\n",
        "        k (int): Value of K for top-k evaluation\n",
        "        threshold (float): Minimum score to consider a prediction valid\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing Precision@K, Recall@K, and F1@K\n",
        "    \"\"\"\n",
        "\n",
        "    # === Step 1: Load input files ===\n",
        "    df = pd.read_csv(topk_file, sep='\\t', dtype=str)\n",
        "    train_df = pd.read_csv(train_file, sep='\\t', dtype=str)\n",
        "    test_df = pd.read_csv(test_file, sep='\\t', dtype=str)\n",
        "\n",
        "    # === Step 2: Remove URIs only present in the training set ===\n",
        "    train_uris = set(train_df['SrcEntity']) | set(train_df['TgtEntity'])\n",
        "    test_uris = set(test_df['SrcEntity']) | set(test_df['TgtEntity'])\n",
        "    uris_to_exclude = train_uris - test_uris\n",
        "    df = df[~(df['SrcEntity'].isin(uris_to_exclude) | df['TgtEntity'].isin(uris_to_exclude))].reset_index(drop=True)\n",
        "\n",
        "    # === Step 3: Keep only source entities from the test set ===\n",
        "    src_entities_test = set(test_df['SrcEntity'])\n",
        "    df = df[df['SrcEntity'].isin(src_entities_test)].reset_index(drop=True)\n",
        "\n",
        "    # === Step 4: Convert score column to float and sort ===\n",
        "    df['Score'] = df['Score'].apply(lambda x: float(x.strip(\"[]\")) if isinstance(x, str) else float(x))\n",
        "    df_sorted = df.sort_values(by='Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # === Step 5: Apply 1-to-1 constraint (greedy strategy with optional threshold)\n",
        "    matched_sources = set()\n",
        "    matched_targets = set()\n",
        "    result = []\n",
        "\n",
        "    for _, row in df_sorted.iterrows():\n",
        "        src, tgt, score = row['SrcEntity'], row['TgtEntity'], row['Score']\n",
        "        if src not in matched_sources and tgt not in matched_targets and score >= threshold:\n",
        "            result.append((src, tgt, score))\n",
        "            matched_sources.add(src)\n",
        "            matched_targets.add(tgt)\n",
        "\n",
        "    # === Step 6: Create and save Top-K prediction dataframe\n",
        "    matching_results_df = pd.DataFrame(result, columns=['SrcEntity', 'TgtEntity', 'Score'])\n",
        "    output_file = topk_file.replace(\".tsv\", \"_predictions.tsv\")\n",
        "    matching_results_df.to_csv(output_file, sep='\\t', index=False)\n",
        "\n",
        "    # === Step 7: Build reference dictionary from test set\n",
        "    ref_dict = defaultdict(set)\n",
        "    for _, row in test_df.iterrows():\n",
        "        ref_dict[row['SrcEntity']].add(row['TgtEntity'])\n",
        "\n",
        "    # === Step 8: Select Top-K predictions for each source entity\n",
        "    matching_results_df['Score'] = matching_results_df['Score'].astype(float)\n",
        "    topk_df = matching_results_df.sort_values(by='Score', ascending=False).groupby('SrcEntity').head(k)\n",
        "\n",
        "    # === Step 9: Compute Precision@K, Recall@K, F1@K\n",
        "    total_tp = total_pred = total_ref = 0\n",
        "\n",
        "    for src, group in topk_df.groupby('SrcEntity'):\n",
        "        predicted = set(group['TgtEntity'])\n",
        "        true = ref_dict.get(src, set())\n",
        "        tp = len(predicted & true)\n",
        "        total_tp += tp\n",
        "        total_pred += len(predicted)\n",
        "        total_ref += len(true)\n",
        "\n",
        "    precision = total_tp / total_pred if total_pred else 0.0\n",
        "    recall = total_tp / total_ref if total_ref else 0.0\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-8) if precision + recall > 0 else 0.0\n",
        "\n",
        "    # === Step 10: Print metrics\n",
        "\n",
        "    print(f\"📊 Precision@{k}:            {precision:.3f}\")\n",
        "    print(f\"📊 Recall@{k}:               {recall:.3f}\")\n",
        "    print(f\"📊 F1@{k}:                   {f1:.3f}\\n\")\n",
        "\n",
        "    return {\n",
        "        f'Precision@{k}': round(precision, 3),\n",
        "        f'Recall@{k}': round(recall, 3),\n",
        "        f'F1@{k}': round(f1, 3)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "WAsAVJEy3o9a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "def encode_embeddings_with_concept_column(encoder_model, input_file, output_file):\n",
        "    \"\"\"\n",
        "    Applies an encoder model to a set of embeddings (while preserving the 'Concept' column),\n",
        "    and saves the encoded results in the same tabular format.\n",
        "\n",
        "    Args:\n",
        "        encoder_model: A PyTorch model (e.g., LinearEncoder, MLPEncoder, etc.)\n",
        "        input_file (str): Path to the input TSV file containing 'Concept' and embedding vectors.\n",
        "        output_file (str): Path to save the encoded embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    # Select device (GPU if available, else CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Move the encoder model to the selected device and set it to evaluation mode\n",
        "    encoder_model = encoder_model.to(device)\n",
        "    encoder_model.eval()\n",
        "\n",
        "    # Load the input TSV file containing concept URIs and embeddings\n",
        "    df = pd.read_csv(input_file, sep='\\t')\n",
        "\n",
        "    # Extract the 'Concept' column to preserve URIs\n",
        "    concepts = df['Concept'].tolist()\n",
        "\n",
        "    # Extract the numerical embedding values (excluding the 'Concept' column)\n",
        "    embedding_values = df.drop(columns=['Concept']).values\n",
        "\n",
        "    # Convert the embedding matrix into a PyTorch tensor and move to the device\n",
        "    embeddings = torch.FloatTensor(embedding_values).to(device)\n",
        "\n",
        "    # Pass the embeddings through the encoder model without computing gradients\n",
        "    with torch.no_grad():\n",
        "        encoded = encoder_model(embeddings).cpu().numpy()\n",
        "\n",
        "    # Reconstruct a new DataFrame with the encoded vectors and corresponding URIs\n",
        "    df_encoded = pd.DataFrame(encoded, columns=[f'dim_{i}' for i in range(encoded.shape[1])])\n",
        "    df_encoded.insert(0, \"Concept\", concepts)  # Re-insert the 'Concept' column at the first position\n",
        "\n",
        "    # Save the encoded embeddings to a TSV file\n",
        "    df_encoded.to_csv(output_file, sep='\\t', index=False)\n",
        "    print(f\"✅ Encoded embeddings saved to: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HyWMsw1MVhz"
      },
      "source": [
        "# **Main Code**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC37FlwGDqGM"
      },
      "source": [
        "\n",
        "\n",
        "# Reading semantic node embeddings provided by the ENE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "FuEfSnw5mod0"
      },
      "outputs": [],
      "source": [
        "# Read the source embeddings from a CSV file into a pandas DataFrame\n",
        "df_embbedings_src = pd.read_csv(src_Emb, index_col=0)\n",
        "\n",
        "# Convert the DataFrame to a NumPy array, which will remove the index and store the data as a raw matrix\n",
        "numpy_array = df_embbedings_src.to_numpy()\n",
        "\n",
        "# Convert the NumPy array into a PyTorch FloatTensor, which is the format required for PyTorch operations\n",
        "x_src = torch.FloatTensor(numpy_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "STUwqMUXmlG2"
      },
      "outputs": [],
      "source": [
        "# Read the target embeddings from a CSV file into a pandas DataFrame\n",
        "df_embbedings_tgt = pd.read_csv(tgt_Emb, index_col=0)\n",
        "\n",
        "# Convert the DataFrame to a NumPy array, which removes the index and converts the data to a raw matrix\n",
        "numpy_array = df_embbedings_tgt.to_numpy()\n",
        "\n",
        "# Convert the NumPy array into a PyTorch FloatTensor, which is required for PyTorch operations\n",
        "x_tgt = torch.FloatTensor(numpy_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZIu9P08DqGN"
      },
      "source": [
        "# Reading adjacency Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "pH69Up40mycz"
      },
      "outputs": [],
      "source": [
        "# Read the source adjacency matrix from a CSV file into a pandas DataFrame\n",
        "df_ma1 = pd.read_csv(src_Adjacence, index_col=0)\n",
        "\n",
        "# Convert the DataFrame to a list of lists (Python native list format)\n",
        "ma1 = df_ma1.values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "hYCmAO5Ymzpl"
      },
      "outputs": [],
      "source": [
        "# Read the target adjacency matrix from a CSV file into a pandas DataFrame\n",
        "df_ma2 = pd.read_csv(tgt_Adjacence, index_col=0)\n",
        "\n",
        "# Convert the DataFrame to a list of lists (Python native list format)\n",
        "ma2 = df_ma2.values.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSyksqh3TrU-"
      },
      "source": [
        "# Convert Adjacency matrix (in list format) to an undirected edge index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "uVt-Pce5m5ll"
      },
      "outputs": [],
      "source": [
        "# Convert the source adjacency matrix (in list format) to an undirected edge index for PyTorch Geometric\n",
        "edge_src = adjacency_matrix_to_undirected_edge_index(ma1)\n",
        "\n",
        "# Convert the target adjacency matrix (in list format) to an undirected edge index for PyTorch Geometric\n",
        "edge_tgt = adjacency_matrix_to_undirected_edge_index(ma2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9wMTRdqT4aY"
      },
      "source": [
        "# GIT Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "eqiEKCLSMVh3"
      },
      "outputs": [],
      "source": [
        "def train_model_gnn(model, x_src, edge_src, x_tgt, edge_tgt,\n",
        "                    tensor_term1, tensor_term2, tensor_score,\n",
        "                    learning_rate, weight_decay_value, num_epochs, print_interval=10):\n",
        "    \"\"\"\n",
        "    Trains a graph neural network (GNN) model using source and target embeddings and contrastive loss.\n",
        "\n",
        "    Args:\n",
        "        model: The GNN model to be trained.\n",
        "        x_src (torch.Tensor): Source node embeddings.\n",
        "        edge_src (torch.Tensor): Source graph edges.\n",
        "        x_tgt (torch.Tensor): Target node embeddings.\n",
        "        edge_tgt (torch.Tensor): Target graph edges.\n",
        "        tensor_term1 (torch.Tensor): Indices of the source nodes to be compared.\n",
        "        tensor_term2 (torch.Tensor): Indices of the target nodes to be compared.\n",
        "        tensor_score (torch.Tensor): Labels indicating if the pairs are matched (1) or not (0).\n",
        "        learning_rate (float): Learning rate for the optimizer.\n",
        "        weight_decay_value (float): Weight decay (L2 regularization) value for the optimizer.\n",
        "        num_epochs (int): Number of epochs for training.\n",
        "        print_interval (int): Interval at which training progress is printed (every `print_interval` epochs).\n",
        "\n",
        "    Returns:\n",
        "        model: The trained GNN model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Set device (GPU or CPU) for computation\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Step 2: Move the model and all inputs to the selected device\n",
        "    model.to(device)\n",
        "    x_tgt = x_tgt.to(device)               # Target node embeddings\n",
        "    edge_tgt = edge_tgt.to(device)         # Target graph edges\n",
        "    x_src = x_src.to(device)               # Source node embeddings\n",
        "    edge_src = edge_src.to(device)         # Source graph edges\n",
        "    tensor_term1 = tensor_term1.to(device) # Indices for source nodes\n",
        "    tensor_term2 = tensor_term2.to(device) # Indices for target nodes\n",
        "    tensor_score = tensor_score.to(device) # Ground truth labels\n",
        "\n",
        "    # Step 3: Define optimizer with learning rate and regularization\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay_value)\n",
        "\n",
        "    # Step 4: Initialize list to store training losses\n",
        "    train_losses = []\n",
        "\n",
        "    # Record the start time of training\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Step 5: Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        # Zero out gradients from the previous iteration\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: Compute embeddings for source and target graphs\n",
        "        out1 = model(x_src, edge_src)  # Updated source embeddings\n",
        "        out2 = model(x_tgt, edge_tgt)  # Updated target embeddings\n",
        "\n",
        "        # Extract specific rows of embeddings for terms being compared\n",
        "        src_embeddings = select_rows_by_index(out1, tensor_term1)\n",
        "        tgt_embeddings = select_rows_by_index(out2, tensor_term2)\n",
        "\n",
        "        # Compute contrastive loss based on the embeddings and ground truth labels\n",
        "        loss = contrastive_loss(src_embeddings, tgt_embeddings, tensor_score)\n",
        "\n",
        "        # Backward pass: Compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the model's parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Append the loss for this iteration to the list\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        # Print loss every `print_interval` epochs\n",
        "        if (epoch + 1) % print_interval == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item()}\")\n",
        "\n",
        "    # Step 6: Record end time of training\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Step 7: Plot the training loss over time\n",
        "    plt.semilogy(range(1, num_epochs + 1), train_losses, label=\"Training Loss\", marker='o')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Print the total training time\n",
        "    training_time = end_time - start_time\n",
        "    print(f\"Training complete! Total training time: {training_time:.2f} seconds\")\n",
        "\n",
        "    # Step 8: Return the trained model\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "6_tzUG_emtBg"
      },
      "outputs": [],
      "source": [
        "# Initialize the GIT_mod model with the dimensionality of the target embeddings\n",
        "# The first argument is the dimensionality of the target node embeddings (x_tgt.shape[1])\n",
        "# The second argument (1) represents the number of RGIT layers in the model\n",
        "GIT_model = RGIT_mod(x_tgt.shape[1], 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "wVo-s7UQssSp"
      },
      "outputs": [],
      "source": [
        "# Reading the training pairs from a CSV file into a pandas DataFrame\n",
        "df_embbedings = pd.read_csv(train_file, index_col=0)\n",
        "\n",
        "# Extract the 'SrcEntity' and 'TgtEntity' columns as NumPy arrays and convert them to integers\n",
        "tensor_term1 = df_embbedings['SrcEntity'].values.astype(int)  # Source entity indices\n",
        "tensor_term2 = df_embbedings['TgtEntity'].values.astype(int)  # Target entity indices\n",
        "\n",
        "# Extract the 'Score' column as a NumPy array and convert it to floats\n",
        "tensor_score = df_embbedings['Score'].values.astype(float)  # Scores (labels) indicating if pairs match (1) or not (0)\n",
        "\n",
        "# Convert the NumPy arrays to PyTorch LongTensors (for indices) and FloatTensors (for scores)\n",
        "tensor_term1_o = torch.from_numpy(tensor_term1).type(torch.LongTensor)  # Source entity tensor\n",
        "tensor_term2_o = torch.from_numpy(tensor_term2).type(torch.LongTensor)  # Target entity tensor\n",
        "tensor_score_o = torch.from_numpy(tensor_score).type(torch.FloatTensor)  # Score tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "agHlFNesMVh3",
        "outputId": "23843b33-9ae6-48dd-9283-62204c88fc4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Training Loss: 0.0117909274995327\n",
            "Epoch [20/1000], Training Loss: 0.010791116394102573\n",
            "Epoch [30/1000], Training Loss: 0.010297775268554688\n",
            "Epoch [40/1000], Training Loss: 0.010021638125181198\n",
            "Epoch [50/1000], Training Loss: 0.009846646338701248\n",
            "Epoch [60/1000], Training Loss: 0.009721150621771812\n",
            "Epoch [70/1000], Training Loss: 0.00962160900235176\n",
            "Epoch [80/1000], Training Loss: 0.009538465179502964\n",
            "Epoch [90/1000], Training Loss: 0.009467151015996933\n",
            "Epoch [100/1000], Training Loss: 0.009404763579368591\n",
            "Epoch [110/1000], Training Loss: 0.009348774328827858\n",
            "Epoch [120/1000], Training Loss: 0.009299135766923428\n",
            "Epoch [130/1000], Training Loss: 0.009255132637917995\n",
            "Epoch [140/1000], Training Loss: 0.00921623595058918\n",
            "Epoch [150/1000], Training Loss: 0.009182633832097054\n",
            "Epoch [160/1000], Training Loss: 0.00915358867496252\n",
            "Epoch [170/1000], Training Loss: 0.009128760546445847\n",
            "Epoch [180/1000], Training Loss: 0.009107462130486965\n",
            "Epoch [190/1000], Training Loss: 0.009089161641895771\n",
            "Epoch [200/1000], Training Loss: 0.009073690511286259\n",
            "Epoch [210/1000], Training Loss: 0.0090605728328228\n",
            "Epoch [220/1000], Training Loss: 0.009049355052411556\n",
            "Epoch [230/1000], Training Loss: 0.009039723314344883\n",
            "Epoch [240/1000], Training Loss: 0.00903154257684946\n",
            "Epoch [250/1000], Training Loss: 0.009024610742926598\n",
            "Epoch [260/1000], Training Loss: 0.009018581360578537\n",
            "Epoch [270/1000], Training Loss: 0.009016242809593678\n",
            "Epoch [280/1000], Training Loss: 0.009009333327412605\n",
            "Epoch [290/1000], Training Loss: 0.009005934931337833\n",
            "Epoch [300/1000], Training Loss: 0.009001960046589375\n",
            "Epoch [310/1000], Training Loss: 0.00899898074567318\n",
            "Epoch [320/1000], Training Loss: 0.008997052907943726\n",
            "Epoch [330/1000], Training Loss: 0.008993897587060928\n",
            "Epoch [340/1000], Training Loss: 0.008991759270429611\n",
            "Epoch [350/1000], Training Loss: 0.00899527408182621\n",
            "Epoch [360/1000], Training Loss: 0.008990385569632053\n",
            "Epoch [370/1000], Training Loss: 0.008987054228782654\n",
            "Epoch [380/1000], Training Loss: 0.008984681218862534\n",
            "Epoch [390/1000], Training Loss: 0.00898450892418623\n",
            "Epoch [400/1000], Training Loss: 0.00898134708404541\n",
            "Epoch [410/1000], Training Loss: 0.008986917324364185\n",
            "Epoch [420/1000], Training Loss: 0.00897879246622324\n",
            "Epoch [430/1000], Training Loss: 0.008977902121841908\n",
            "Epoch [440/1000], Training Loss: 0.008976556360721588\n",
            "Epoch [450/1000], Training Loss: 0.008980820886790752\n",
            "Epoch [460/1000], Training Loss: 0.00897735171020031\n",
            "Epoch [470/1000], Training Loss: 0.008976873010396957\n",
            "Epoch [480/1000], Training Loss: 0.00897540058940649\n",
            "Epoch [490/1000], Training Loss: 0.008973347954452038\n",
            "Epoch [500/1000], Training Loss: 0.008971994742751122\n",
            "Epoch [510/1000], Training Loss: 0.00897289626300335\n",
            "Epoch [520/1000], Training Loss: 0.008974018506705761\n",
            "Epoch [530/1000], Training Loss: 0.00897416565567255\n",
            "Epoch [540/1000], Training Loss: 0.00897214375436306\n",
            "Epoch [550/1000], Training Loss: 0.008970122784376144\n",
            "Epoch [560/1000], Training Loss: 0.008969091810286045\n",
            "Epoch [570/1000], Training Loss: 0.008969797752797604\n",
            "Epoch [580/1000], Training Loss: 0.008971604518592358\n",
            "Epoch [590/1000], Training Loss: 0.00897158682346344\n",
            "Epoch [600/1000], Training Loss: 0.008969643153250217\n",
            "Epoch [610/1000], Training Loss: 0.008967647328972816\n",
            "Epoch [620/1000], Training Loss: 0.008966474793851376\n",
            "Epoch [630/1000], Training Loss: 0.008967527188360691\n",
            "Epoch [640/1000], Training Loss: 0.008968918584287167\n",
            "Epoch [650/1000], Training Loss: 0.008969453163444996\n",
            "Epoch [660/1000], Training Loss: 0.008967414498329163\n",
            "Epoch [670/1000], Training Loss: 0.008965067565441132\n",
            "Epoch [680/1000], Training Loss: 0.008963617496192455\n",
            "Epoch [690/1000], Training Loss: 0.008964259177446365\n",
            "Epoch [700/1000], Training Loss: 0.008965984918177128\n",
            "Epoch [710/1000], Training Loss: 0.00896679051220417\n",
            "Epoch [720/1000], Training Loss: 0.008964969776570797\n",
            "Epoch [730/1000], Training Loss: 0.008962705731391907\n",
            "Epoch [740/1000], Training Loss: 0.008961287327110767\n",
            "Epoch [750/1000], Training Loss: 0.008962519466876984\n",
            "Epoch [760/1000], Training Loss: 0.008964266628026962\n",
            "Epoch [770/1000], Training Loss: 0.008965265937149525\n",
            "Epoch [780/1000], Training Loss: 0.008963530883193016\n",
            "Epoch [790/1000], Training Loss: 0.00896152388304472\n",
            "Epoch [800/1000], Training Loss: 0.008959705010056496\n",
            "Epoch [810/1000], Training Loss: 0.008959006518125534\n",
            "Epoch [820/1000], Training Loss: 0.008960372768342495\n",
            "Epoch [830/1000], Training Loss: 0.008962340652942657\n",
            "Epoch [840/1000], Training Loss: 0.00896206684410572\n",
            "Epoch [850/1000], Training Loss: 0.00895973201841116\n",
            "Epoch [860/1000], Training Loss: 0.00895761139690876\n",
            "Epoch [870/1000], Training Loss: 0.008956795558333397\n",
            "Epoch [880/1000], Training Loss: 0.008958716876804829\n",
            "Epoch [890/1000], Training Loss: 0.008960769511759281\n",
            "Epoch [900/1000], Training Loss: 0.008960152976214886\n",
            "Epoch [910/1000], Training Loss: 0.00895784143358469\n",
            "Epoch [920/1000], Training Loss: 0.00895613431930542\n",
            "Epoch [930/1000], Training Loss: 0.008955932222306728\n",
            "Epoch [940/1000], Training Loss: 0.008958028629422188\n",
            "Epoch [950/1000], Training Loss: 0.00895988941192627\n",
            "Epoch [960/1000], Training Loss: 0.008958892896771431\n",
            "Epoch [970/1000], Training Loss: 0.008956750854849815\n",
            "Epoch [980/1000], Training Loss: 0.008955147117376328\n",
            "Epoch [990/1000], Training Loss: 0.008955048397183418\n",
            "Epoch [1000/1000], Training Loss: 0.0089569091796875\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAGwCAYAAADsYcIbAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAATKFJREFUeJzt3X1clHW+//H3zCAgCCii3Ch4U65KJCQKUZqKmGLLZrW7bZliu0d/GbYV65adTtnNdrPbbmspx7LT6q62ybpH3W7MUqxMD6WilIZZmqkloISConIzc/3+cJkiQIZhmBnk9Xw8eDzkmu91zWeuSN5+v9f3+zUZhmEIAAAAXsHs6QIAAADwHcIZAACAFyGcAQAAeBHCGQAAgBchnAEAAHgRwhkAAIAXIZwBAAB4ER9PF4DWsdlsOnr0qIKCgmQymTxdDgAAcIBhGDp16pSioqJkNl+4b4xw1sEcPXpU0dHRni4DAAA44ciRI+rbt+8F2xDOOpigoCBJ5//jBgcHe7gaAADgiMrKSkVHR9t/j18I4ayDqR/KDA4OJpwBANDBOPJIEhMCAAAAvAjhDAAAwIsQzgAAALwIz5wBAOAgq9Wq2tpaT5cBL+Xr69viMhmOIJwBANACwzBUUlKikydPeroUeDGz2awBAwbI19e3TdchnAEA0IL6YNa7d28FBASwCDgaqV8kvri4WDExMW36GSGcAQBwAVar1R7Mevbs6ely4MV69eqlo0ePqq6uTl26dHH6OkwIAADgAuqfMQsICPBwJfB29cOZVqu1TdchnAEA4ACGMtESV/2MMKwJSZLVZmjbwXIdO3VOvYP8lTQgVBYzfxEBAOBuhDNo/Z5iPfp6kYorztmPRYb4a35GrCbFRXqwMgAAOh+GNTu59XuKNXvFzgbBTJJKKs5p9oqdWr+n2EOVAcDFxWozlH/gW/2r8BvlH/hWVpvh6ZJarX///lqwYIHD7d977z2ZTCaWIGkles46MavN0KOvF6mpvx4MSSZJj75epAmxEQxxAkAbuHuEoqVnn+bPn69HHnmk1dfdvn27AgMDHW5/1VVXqbi4WCEhIa1+r9Z47733NG7cOJ04cULdu3dv1/dyB8JZJ7btYHmjHrPvMyQVV5zTtoPlSrmE6eMA4Iz6EYof/kO4foRi8W3DXR7Qiou/G/XIzc3Vww8/rH379tmPdevWzf5nwzBktVrl49NyJOjVq1er6vD19VVERESrzgHDmp3asVPNBzNn2gFAZ2AYhs7U1Dn0depcrea/9mmzIxSS9MhrRTp1rtah6xmGY0OhERER9q+QkBCZTCb795999pmCgoL01ltvKTExUX5+ftqyZYsOHDig66+/XuHh4erWrZtGjhypjRs3NrjuD4c1TSaT/ud//kc33HCDAgICNGjQIL322mv21384rLls2TJ1795db7/9toYOHapu3bpp0qRJDcJkXV2dfv3rX6t79+7q2bOn7r//fmVmZmrKlCkOffamnDhxQtOnT1ePHj0UEBCg9PR0ffHFF/bXDx06pIyMDPXo0UOBgYG67LLLtG7dOvu5U6dOVa9evdS1a1cNGjRIS5cudboWR9Bz1on1DvJ3aTsA6AzO1loV+/DbLrmWIamk8pwuf+Qdh9oXPTZRAb6u+dU9b948/fGPf9TAgQPVo0cPHTlyRJMnT9YTTzwhPz8//e1vf1NGRob27dunmJiYZq/z6KOP6g9/+IOeeeYZLVy4UFOnTtWhQ4cUGhraZPszZ87oj3/8o5YvXy6z2azbbrtNc+fO1SuvvCJJ+v3vf69XXnlFS5cu1dChQ/Xcc89p7dq1GjdunNOfdcaMGfriiy/02muvKTg4WPfff78mT56soqIidenSRVlZWaqpqdHmzZsVGBiooqIie+/iQw89pKKiIr311lsKCwvT/v37dfbsWadrcQThrBNLGhCqyBB/lVSca/JfdSZJESHnl9UAAFxcHnvsMU2YMMH+fWhoqOLj4+3fP/7441qzZo1ee+01zZkzp9nrzJgxQ7fccosk6cknn9Tzzz+vbdu2adKkSU22r62t1QsvvKBLLrlEkjRnzhw99thj9tcXLlyoBx54QDfccIMkadGiRfZeLGfUh7KtW7fqqquukiS98sorio6O1tq1a/Wzn/1Mhw8f1k033aTLL79ckjRw4ED7+YcPH9YVV1yhESNGSDrfe9jeCGedmMVs0vyMWM1esbPRa/WPks7PiGUyAAB8T9cuFhU9NtGhttsOlmvG0u0ttlt2+0iH/iHctYvFofd1RH3YqHf69Gk98sgjevPNN1VcXKy6ujqdPXtWhw8fvuB1hg0bZv9zYGCggoODdezYsWbbBwQE2IOZJEVGRtrbV1RUqLS0VElJSfbXLRaLEhMTZbPZWvX56u3du1c+Pj5KTk62H+vZs6cGDx6svXv3SpJ+/etfa/bs2XrnnXeUlpamm266yf65Zs+erZtuukk7d+7UtddeqylTpthDXnvhmbNOblJcpBbfNly9uvk1OB4R4t8uD6kCQEdnMpkU4Ovj0NfoQb0UGeKv5v6Ja9L5WZujB/Vy6Hqu3KXgh7Mu586dqzVr1ujJJ5/UBx98oMLCQl1++eWqqam54HV+uIekyWS6YJBqqr2jz9K1l//4j//Ql19+qWnTpmn37t0aMWKEFi5cKElKT0/XoUOHdO+99+ro0aMaP3685s6d2671EM6gSXGRWnVHiiSpi8WkV2deqS33pxLMAKCN6kcoJDUKaN42QrF161bNmDFDN9xwgy6//HJFREToq6++cmsNISEhCg8P1/bt3/U2Wq1W7dzZeITHUUOHDlVdXZ0++ugj+7Fvv/1W+/btU2xsrP1YdHS07rjjDq1evVq/+c1v9NJLL9lf69WrlzIzM7VixQotWLBAS5YscboeRzCsCUlSF5/zOd1kMrFsBgC4UP0IxQ/XOYvwsp1YBg0apNWrVysjI0Mmk0kPPfSQ00OJbXHXXXfpqaee0qWXXqohQ4Zo4cKFOnHihEO9hrt371ZQUJD9e5PJpPj4eF1//fWaOXOmXnzxRQUFBWnevHnq06ePrr/+eknSPffco/T0dP3oRz/SiRMn9O6772ro0KGSpIcffliJiYm67LLLVF1drTfeeMP+WnshnEGSZPn3D72tA65YDQDeblJcpCbERnj1HsbPPvusfvnLX+qqq65SWFiY7r//flVWVrq9jvvvv18lJSWaPn26LBaLZs2apYkTJ8piafl5u2uuuabB9xaLRXV1dVq6dKnuvvtu/fjHP1ZNTY2uueYarVu3zj7EarValZWVpa+//lrBwcGaNGmS/vznP0s6v1bbAw88oK+++kpdu3bV6NGjtXLlStd/8O8xGZ4e6EWrVFZWKiQkRBUVFQoODnbZdY+dOqekJ/JkMkkHn7rOZdcFgI7u3LlzOnjwoAYMGCB/f5YWcjebzaahQ4fq5z//uR5//HFPl3NBF/pZac3vb3rOIOm7njPDOL/AoisfOgUAwFGHDh3SO++8ozFjxqi6ulqLFi3SwYMHdeutt3q6NLdhQgAkqUHXekfcjBcAcHEwm81atmyZRo4cqauvvlq7d+/Wxo0b2/05L29CzxkkSebvhzPD4AcDAOAR0dHR2rp1q6fL8Ch6ziBJMn9vGNMDk3MAwOvxiDZa4qqfEcIZJH33zJl0vucMAHBe/Yy+M2fOeLgSeLv6BXsdmVl6IYxeQZJk/l5MtxHOAMDOYrGoe/fu9i2GAgICmDSFRmw2m44fP66AgAD5+LQtXhHOIKlhzxlrnQFAQxEREZJ0wT0jAbPZrJiYmDaHd8IZJDFbEwAuxGQyKTIyUr1791Ztba2ny4GX8vX1ldnc9ifGCGeQdP4vHpPp/DpnPHMGAE2zWCxtfp4IaAkTAmD33RZOHi4EAIBOjHAGu/q1zug5AwDAcwhnsGPzcwAAPI9wBrv6SQFMCAAAwHMIZx525MgRjR07VrGxsRo2bJhWrVrlsVrqJ2wyrAkAgOcwW9PDfHx8tGDBAiUkJKikpESJiYmaPHmyAgMD3V5Lfc8Zw5oAAHgO4czDIiMjFRkZKen8IodhYWEqLy/3aDij5wwAAM/x+LDm5s2blZGRoaioKJlMJq1du7ZV5z/99NMymUy65557PFJbTk6O+vfvL39/fyUnJ2vbtm1Ov19BQYGsVquio6PbULXz6jc/55kzAAA8x+PhrKqqSvHx8crJyWn1udu3b9eLL76oYcOGXbDd1q1bm1zRuaioSKWlpU7Xlpubq+zsbM2fP187d+5UfHy8Jk6c2GB7j4SEBMXFxTX6Onr0aINrlZeXa/r06VqyZMkFP0t7+m5Y02MlAADQ6Xl8WDM9PV3p6emtPu/06dOaOnWqXnrpJf3ud79rtp3NZlNWVpYGDRqklStX2ld23rdvn1JTU5Wdna377rvPqdqeffZZzZw5U7fffrsk6YUXXtCbb76pv/zlL5o3b54kqbCwsMXPUl1drSlTpmjevHm66qqrmmyTk5OjnJwcWa3WFq/nLHvPGcOaAAB4jMd7zpyVlZWl6667TmlpaRdsZzabtW7dOu3atUvTp0+XzWbTgQMHlJqaqilTpjQbzFpSU1OjgoKCBu9vNpuVlpam/Px8h69jGIZmzJih1NRUTZs2rdl2WVlZKioq0vbt252q1xEspQEAgOd5vOfMGStXrtTOnTsdDipRUVHatGmTRo8erVtvvVX5+flKS0vT4sWLna6hrKxMVqtV4eHhDY6Hh4frs88+c/g6W7duVW5uroYNG2Z/pm358uW6/PLLna7NWfZhTXrOAADwmA4Xzo4cOaK7775bGzZskL+/v8PnxcTEaPny5RozZowGDhyol19+WaZ/D+N50qhRo2Tzkoe87Ouc0XMGAIDHdLhhzYKCAh07dkzDhw+Xj4+PfHx89P777+v555+Xj49Ps89klZaWatasWcrIyNCZM2d07733tqmOsLAwWSyWRhMKSktLFRER0aZrewrrnAEA4HkdLpyNHz9eu3fvVmFhof1rxIgRmjp1qgoLC+0P/H9fWVmZxo8fr6FDh2r16tXKy8tTbm6u5s6d63Qdvr6+SkxMVF5env2YzWZTXl6eUlJSnL6uJzEhAAAAz/P4sObp06e1f/9++/cHDx5UYWGhQkNDFRMTo0WLFmnNmjX2EBQUFKS4uLgG1wgMDFTPnj0bHZfOB6b09HT169dPubm58vHxUWxsrDZs2KDU1FT16dOn2V60lmrLzs5WZmamRowYoaSkJC1YsEBVVVX22ZsdDRMCAADwPI+Hsx07dmjcuHH277OzsyVJmZmZWrZsmcrKynTgwAGnr282m/Xkk09q9OjR8vX1tR+Pj4/Xxo0b1atXL6dru/nmm3X8+HE9/PDDKikpUUJCgtavX99okkBHYLUZOltzfki46GiFRg/qZQ9rAADAfUyGwRhWR1JZWamQkBBVVFQoODjYJddcv6dYj75epOKKc/ZjkSH+mp8Rq0lxkS55DwAAOrPW/P7ucM+cwbXW7ynW7BU7GwQzSSqpOKfZK3Zq/Z5iD1UGAEDnRDjrxKw2Q4++XqSmuk7rjz36ehHPoAEA4EaEs05s28HyRj1m32dIKq44p20Hy91XFAAAnRzhrBM7dqr5YOZMOwAA0HaEs06sd5BjOyw42g4AALQd4awTSxoQqsgQfzW3YIZJ52dtJg0IdWdZAAB0aoSzTsxiNml+RqwkNQpo9d/Pz4hlvTMAANyIcNbJTYqL1OLbhisipOHQZUSIvxbfNpx1zgAAcDPCGTQpLlJb7k/V6EvDJElTk6O15f5UghkAAB5AOIOk80Ockd3P955FdQ9gKBMAAA8hnMHOx3L+x6HWavNwJQAAdF6EM9h1+XdvWZ2VHQEAAPAUwhns7D1nNnrOAADwFMIZ7Hws9JwBAOBphDPYdTGf/3Go45kzAAA8hnAGu/qes1obPWcAAHgK4Qx2XSz0nAEA4GmEM9j5MFsTAACPI5zB7rvZmoQzAAA8hXAGuy722ZoMawIA4CmEM9j5mOt3CKDnDAAATyGcwe7f2UzfnDyj/APfysrwJgAAbkc4gyRp/Z5iPbXuM0nS3uJTuuWlDzXq95u0fk+xhysDAKBzIZxB6/cUa/aKnao4W9vgeEnFOc1esZOABgCAGxHOOjmrzdCjrxepqQHM+mOPvl7EECcAAG5COOvkth0sV3HFuWZfNyQVV5zTtoPl7isKAIBOjHDWyR071Xwwc6YdAABoG8JZJ9c7yN+l7QAAQNsQzjq5pAGhigzxl6mZ102SIkP8lTQg1J1lAQDQaRHOOjmL2aT5GbFNvlYf2OZnxMpibi6+AQAAVyKcQZPiIrX4tuEK6+bb4HhEiL8W3zZck+IiPVQZAACdj4+nC4B3mBQXqSERwRr7x/fUxWLS336ZrKQBofSYAQDgZoQz2AX4WiRJdTZDVw4MlclEMAMAwN0Y1oSdr8/5HwfDOB/QAACA+xHOYOfnY7H/ubrO5sFKAADovAhnsKvvOZOkGsIZAAAeQTiDncVsks+/JwAQzgAA8AzCmYcdOXJEY8eOVWxsrIYNG6ZVq1Z5tJ763rPqOqtH6wAAoLNitqaH+fj4aMGCBUpISFBJSYkSExM1efJkBQYGur0Wq81Q/coZ2w+Wq2+PAJbSAADAzeg587DIyEglJCRIkiIiIhQWFqby8nK317F+T7FG/X6TTlef7zGb+89PNOr3m7R+T7HbawEAoDPzeDjbvHmzMjIyFBUVJZPJpLVr17Z4zuLFizVs2DAFBwcrODhYKSkpeuuttzxSW05Ojvr37y9/f38lJydr27ZtTr9fQUGBrFaroqOj21B1663fU6zZK3aquOJcg+MlFec0e8VOAhoAAG7k8XBWVVWl+Ph45eTkOHxO37599fTTT6ugoEA7duxQamqqrr/+en366adNtt+6datqa2sbHS8qKlJpaanTteXm5io7O1vz58/Xzp07FR8fr4kTJ+rYsWP2NgkJCYqLi2v0dfTo0QbXKi8v1/Tp07VkyRJHboHLWG2GHn29SE2talZ/7NHXi2Rl3TMAANzCZBiG1/zWNZlMWrNmjaZMmdLqc0NDQ/XMM8/oV7/6VYPjNptNw4cP16BBg7Ry5UpZLOfX8tq3b5/GjBmj7Oxs3XfffU7VlpycrJEjR2rRokX294qOjtZdd92lefPmOVx7dXW1JkyYoJkzZ2ratGkXbFtZWamQkBBVVFQoODjY4fdoTv6Bb3XLSx+22O7VmVcq5ZKebX4/AAA6o9b8/vZ4z1lbWa1WrVy5UlVVVUpJSWn0utls1rp167Rr1y5Nnz5dNptNBw4cUGpqqqZMmeJQMGtKTU2NCgoKlJaW1uC90tLSlJ+f7/B1DMPQjBkzlJqaesFglpOTo9jYWI0cOdKpeptz7NS5lhu1oh0AAGibDhvOdu/erW7dusnPz0933HGH1qxZo9jY2CbbRkVFadOmTdqyZYtuvfVWpaamKi0tTYsXL3b6/cvKymS1WhUeHt7geHh4uEpKShy+ztatW5Wbm6u1a9cqISFBCQkJ2r17d6N2WVlZKioq0vbt252uuSm9g/xd2g4AALRNh11KY/DgwSosLFRFRYX++c9/KjMzU++//36zAS0mJkbLly/XmDFjNHDgQL388stesbH3qFGjZLN5bsHXpAGhigzxV0nFuSafOzNJigjxV9KAUHeXBgBAp9Rhe858fX116aWXKjExUU899ZTi4+P13HPPNdu+tLRUs2bNUkZGhs6cOaN77723Te8fFhYmi8XSaEJBaWmpIiIi2nRtd7KYTZqfcT7Q/jCq1n8/PyOW9c4AAHCTDhvOfshms6m6urrJ18rKyjR+/HgNHTpUq1evVl5ennJzczV37lyn38/X11eJiYnKy8trUENeXl6Tz755s0lxkVp823BFhDQcuowI8dfi24ZrUlykhyoDAKDz8fiw5unTp7V//3779wcPHlRhYaFCQ0MVExOjRYsWac2aNQ1C0AMPPKD09HTFxMTo1KlT+vvf/6733ntPb7/9dqPr22w2paenq1+/fsrNzZWPj49iY2O1YcMGpaamqk+fPs32orVUW3Z2tjIzMzVixAglJSVpwYIFqqqq0u233+7CO+Qek+IiNSE2QjOWbtMHX5Tp1qRoPT7lcnrMAABwM4+Hsx07dmjcuHH277OzsyVJmZmZWrZsmcrKynTgwIEG5xw7dkzTp09XcXGxQkJCNGzYML399tuaMGFCo+ubzWY9+eSTGj16tHx9fe3H4+PjtXHjRvXq1cvp2m6++WYdP35cDz/8sEpKSpSQkKD169c3miTQUVjMJvXt0VWSFBnSlWAGAIAHeNU6Z2iZq9c5+6H5/9qjv+Yf0pxxl2ruxMEuvz4AAJ1Rp1rnDK7l1+X8Ir01Vs/NIAUAoDMjnKEBn38PZe4trlT+gW/ZtgkAADfz+DNn8B7r9xTrb/mHJEkffFGmD74oU2SIv+ZnxDJjEwAAN6HnDJLOB7PZK3bqdHVdg+MlFec0e8VOrd9T7KHKAADoXAhnkNVm6NHXi5rcIaD+2KOvFzHECQCAGxDOoG0Hy1Vc0fzG5oak4opz2naw3H1FAQDQSRHOoGOnmg9mzrQDAADOI5xBvYP8W27UinYAAMB5hDMoaUCoIkP8G218Xs8kKTLEX0kDQt1ZFgAAnRLhDLKYTZqfEdvka/WBbX5GLNs5AQDgBoQzSDq/8fni24arZ6Bvg+MRIf5afNtw1jkDAMBNWIQWdpPiIhUdGqDrnt+iID+LlkwfqaQBofSYAQDgRoQzNNDN7/yPhM2QUi7p6eFqAADofBjWRANdLOd/JM7UWJV/oIyFZwEAcDPCGezW7ynWDf+9VdL5hWdveekjjfr9JrZuAgDAjQhnkPTd3pqlldUNjrO3JgAA7kU4A3trAgDgRQhnYG9NAAC8COEM7K0JAIAXIZyBvTUBAPAihDOwtyYAAF6EcIYGe2v+MKCxtyYAAO5FOIOk7/bWjAhpOHTJ3poAALgX2zfBblJcpCbERmhKzhbt/qZSd469RL+5djA9ZgAAuBE9Z2jAYjYpIqSrJKlvjwCCGQAAbkY4QyP+Pud/LLZ/Va78A9+y+CwAAG7EsCYaWL+nWBv3HpMkrdn1jdbs+kaRIf6anxHLc2cAALgBPWewq99f82yttcFx9tcEAMB9CGeQxP6aAAB4C8IZJLG/JgAA3oJwBknsrwkAgLcgnEES+2sCAOAtCGeQxP6aAAB4C8IZJDXcX/OH2F8TAAD3IZzBrn5/zdBA3wbH2V8TAAD3YRFaNDApLlLdu/rqFy99qG5+Ft2b9iNNS+kvXx9yPAAA7sBvXDSwfk+x5ry6U5J0utqqx9/cqzHPvMsCtAAAuAnhDHb1OwSUna5pcJwdAgAAcB/CGSSxQwAAAN6CcAZJ7BAAAIC3IJxBEjsEAADgLQhnkMQOAQAAeAvCmQcdOXJEY8eOVWxsrIYNG6ZVq1Z5rBZ2CAAAwDsQzjzIx8dHCxYsUFFRkd555x3dc889qqqq8kgt398h4IcBjR0CAABwH8KZB0VGRiohIUGSFBERobCwMJWXe+6B+/odAiJCGg5dskMAAADu4/FwtnnzZmVkZCgqKkomk0lr1669YPunnnpKI0eOVFBQkHr37q0pU6Zo3759HqsrJydH/fv3l7+/v5KTk7Vt2zan3q+goEBWq1XR0dFtqLrtJsVF6v3fjlN0j66SpJuG99H7vx1HMAMAwE08Hs6qqqoUHx+vnJwch9q///77ysrK0ocffqgNGzaotrZW11577QWHA7du3ara2tpGx4uKilRaWup0Xbm5ucrOztb8+fO1c+dOxcfHa+LEiTp27Ji9TUJCguLi4hp9HT161N6mvLxc06dP15IlSxy5Be1q/Z5ijXnmXR05cVaS9L87v2GHAAAA3MhkGIbXrCpqMpm0Zs0aTZkyxeFzjh8/rt69e+v999/XNddc0+h1m82m4cOHa9CgQVq5cqUsFoskad++fRozZoyys7N13333OVVXcnKyRo4cqUWLFtnfKzo6WnfddZfmzZvnUP3V1dWaMGGCZs6cqWnTpjXbLicnRzk5ObJarfr8889VUVGh4OBgh97DUfU7BPzwB6L+KTOGNgEAcE5lZaVCQkIc+v3t8Z6ztqqoqJAkhYY2PYvQbDZr3bp12rVrl6ZPny6bzaYDBw4oNTVVU6ZMaTGYNaempkYFBQVKS0tr8F5paWnKz8936BqGYWjGjBlKTU29YDCTpKysLBUVFWn79u1O1dsSdggAAMA7dOhwZrPZdM899+jqq69WXFxcs+2ioqK0adMmbdmyRbfeeqtSU1OVlpamxYsXO/3eZWVlslqtCg8Pb3A8PDxcJSUlDl1j69atys3N1dq1a5WQkKCEhATt3r3b6Zragh0CAADwDj6eLqAtsrKytGfPHm3ZsqXFtjExMVq+fLnGjBmjgQMH6uWXX5bJ5NllIUaNGiWbzebRGuqxQwAAAN6hw/aczZkzR2+88Ybeffdd9e3bt8X2paWlmjVrljIyMnTmzBnde++9bXr/sLAwWSyWRhMKSktLFRER0aZrewI7BAAA4B06XDgzDENz5szRmjVrtGnTJg0YMKDFc8rKyjR+/HgNHTpUq1evVl5ennJzczV37lyn6/D19VViYqLy8vLsx2w2m/Ly8pSSkuL0dT2FHQIAAPAOHg9np0+fVmFhoQoLCyVJBw8eVGFhoQ4fPixJWrRokcaPH29vn5WVpRUrVujvf/+7goKCVFJSopKSEp09e7bJ69tsNqWnp6tfv37Kzc2Vj4+PYmNjtWHDBi1dulR//vOfnapLkrKzs/XSSy/pr3/9q/bu3avZs2erqqpKt99+uwvujHuxQwAAAN7B40tpvPfeexo3blyj45mZmVq2bJkeeeQRLVu2TF999ZUkNfuc2NKlSzVjxowmX9uwYYNGjx4tf/+GQ3K7du1Sr169mhwWbamueosWLdIzzzyjkpISJSQk6Pnnn1dycnIzn7btWjMV1xnr9xTr0deLGkwOCA3sot9dH6fJw6Jc/n4AAHQGrfn97fFwhtZp73AmSes+Kdb9//uJTlXX2Y9FhvhrfkYs65wBAOCETrXOGVxr/Z5iZf19Z4NgJkklFec0e8VOdgoAAKCdEc5gx0K0AAB4HuEMdixECwCA5xHOYMdCtAAAeB7hDHYsRAsAgOcRzmDHQrQAAHge4Qx231+ItjksRAsAQPsinKGBSXGRmnXNAP0wf5lN0qxrBrDOGQAA7YxwhgbW7ynWks0H9cPVMgxDWrL5IOucAQDQzghnsGOdMwAAPI9wBjvWOQMAwPMIZ7BjnTMAADyPcAY71jkDAMDzCGewY50zAAA8z6lwduTIEX399df277dt26Z77rlHS5YscVlhcL/vr3PWVEAzxDpnAAC0N6fC2a233qp3331XklRSUqIJEyZo27ZtevDBB/XYY4+5tEC416S4SC2+bbhCAro0eq17E8cAAIBrORXO9uzZo6SkJEnSP/7xD8XFxen//u//9Morr2jZsmWurA8eUnGmtsljs1fsZK0zAADakVPhrLa2Vn5+fpKkjRs36ic/+YkkaciQISou5hd3R8ZaZwAAeJZT4eyyyy7TCy+8oA8++EAbNmzQpEmTJElHjx5Vz549XVog3Iu1zgAA8Cynwtnvf/97vfjiixo7dqxuueUWxcfHS5Jee+01+3AnOibWOgMAwLN8nDlp7NixKisrU2VlpXr06GE/PmvWLAUEBLisOLgfa50BAOBZTvWcnT17VtXV1fZgdujQIS1YsED79u1T7969XVog3Iu1zgAA8Cynwtn111+vv/3tb5KkkydPKjk5WX/60580ZcoULV682KUFwr3q1zpr7nF/1joDAKB9ORXOdu7cqdGjR0uS/vnPfyo8PFyHDh3S3/72Nz3//PMuLRAAAKAzcSqcnTlzRkFBQZKkd955RzfeeKPMZrOuvPJKHTp0yKUFwr3ql9JojkkspQEAQHtyKpxdeumlWrt2rY4cOaK3335b1157rSTp2LFjCg4OdmmBcC+W0gAAwLOcCmcPP/yw5s6dq/79+yspKUkpKSmSzveiXXHFFS4tEO7FUhoAAHiWU0tp/PSnP9WoUaNUXFxsX+NMksaPH68bbrjBZcXB/VhKAwAAz3IqnElSRESEIiIi9PXXX0uS+vbtywK0F4H6pTRKKs41O2OTpTQAAGg/Tg1r2mw2PfbYYwoJCVG/fv3Ur18/de/eXY8//rhsNpura4Qb1S+lcSE/iY9kKQ0AANqJU+HswQcf1KJFi/T0009r165d2rVrl5588kktXLhQDz30kKtrhJtNiovUrGsGNPv6ks0HtX4PG9wDANAeTIZhtHpNhKioKL3wwgv6yU9+0uD4v/71L91555365ptvXFYgGqqsrFRISIgqKirabWas1WZo1O83NTtr0yQpIsRfW+5PpQcNAAAHtOb3t1M9Z+Xl5RoyZEij40OGDFF5OUssdHQspwEAgOc4Fc7i4+O1aNGiRscXLVqkYcOGtbkoeBbLaQAA4DlOzdb8wx/+oOuuu04bN260r3GWn5+vI0eOaN26dS4tEO7HchoAAHiOUz1nY8aM0eeff64bbrhBJ0+e1MmTJ3XjjTfq008/1fLly11dI9ysfjmNC2E5DQAA2odTEwKa8/HHH2v48OGyWq2uuiR+wB0TAiTpqXVFenHzwWZf/3/XDNADky+85AYAADiv3ScE4OJmtRl67eMLL5Xx2sfFbH4OAEA7IJyhkZZma0rM1gQAoL0QztAIszUBAPCcVs3WvPHGGy/4+smTJ9tSC7wEszUBAPCcVoWzkJCQFl+fPn16mwqC5zmy+Xn3gC7M1gQAoB20KpwtXbq0veqAF6nf/PyOFTubbXPyTK02FJVoUlykGysDAODixzNnaNKE2Ah1D+jS7OsmSY++XsSMTQAAXIxwhiZtO1iuk2dqm32d/TUBAGgfhDM0iRmbAAB4BuHMw44cOaKxY8cqNjZWw4YN06pVqzxdkiRmbAIA4ClObXwO1/Hx8dGCBQuUkJCgkpISJSYmavLkyQoMDPRoXUkDQtU9oMsFhzaZsQkAgOsRzjwsMjJSkZHnZzxGREQoLCxM5eXlHg9njjB5ugAAAC5CHh/W3Lx5szIyMhQVFSWTyaS1a9e2yzntVVtOTo769+8vf39/JScna9u2bU6/X0FBgaxWq6Kjo9tQtWu0NCFAkk6cqWVCAAAALubxcFZVVaX4+Hjl5OS02zlbt25VbW3joFFUVKTS0lKn3yc3N1fZ2dmaP3++du7cqfj4eE2cOFHHjh2zt0lISFBcXFyjr6NHjza4Vnl5uaZPn64lS5Y49JnaGxMCAADwDI8Pa6anpys9Pb3dzrHZbMrKytKgQYO0cuVKWSwWSdK+ffuUmpqq7Oxs3XfffU69z7PPPquZM2fq9ttvlyS98MILevPNN/WXv/xF8+bNkyQVFha2WGN1dbWmTJmiefPm6aqrrmqyTU5OjnJycmS1Wlu8niswIQAAAM/weM9ZezObzVq3bp127dql6dOny2az6cCBA0pNTdWUKVOaDWYtqampUUFBgdLS0hq8V1pamvLz8x2+jmEYmjFjhlJTUzVt2rRm22VlZamoqEjbt293qt7Wqt/CqSUnqmrcUA0AAJ3HRR/OJCkqKkqbNm3Sli1bdOuttyo1NVVpaWlavHix09csKyuT1WpVeHh4g+Ph4eEqKSlx+Dpbt25Vbm6u1q5dq4SEBCUkJGj37t1O1+UqFrNJD103tMV2j7/JLgEAALiSx4c13SUmJkbLly/XmDFjNHDgQL388ssymTw/33DUqFGy2WyeLqNJPQL9WmxTv0tAyiU93VARAAAXv07RcyZJpaWlmjVrljIyMnTmzBnde++9bbpeWFiYLBZLowkFpaWlioiIaNO1vQWTAgAAcL9OEc7Kyso0fvx4DR06VKtXr1ZeXp5yc3M1d+5cp6/p6+urxMRE5eXl2Y/ZbDbl5eUpJSXFFWV7HJMCAABwP48Pa54+fVr79++3f3/w4EEVFhYqNDRUMTExWrRokdasWdMgBLV0zvfZbDalp6erX79+ys3NlY+Pj2JjY7VhwwalpqaqT58+zfaitfQ+2dnZyszM1IgRI5SUlKQFCxaoqqrKPnuzo2OXAAAA3M/j4WzHjh0aN26c/fvs7GxJUmZmppYtW6aysjIdOHCgVed8n9ls1pNPPqnRo0fL19fXfjw+Pl4bN25Ur169nK7t5ptv1vHjx/Xwww+rpKRECQkJWr9+faNJAhczzz+1BwDAxcVkGAZT7TqQyspKhYSEqKKiQsHBwe36XvkHvtUtL33YYrtXZ17JhAAAAC6gNb+/O8UzZ3AOEwIAAHA/whmaxYQAAADcj3CGZiX26yFzCw+VmU3n2wEAANcgnKFZBYdOqKXF/23G+XYAAMA1CGdoFs+cAQDgfoQzNMvRZ8m+KjvTzpUAANB5EM7QrKQBoYoIbnl/zZXbD7P5OQAALkI4Q7MsZpNuSYppsV395ucAAKDtCGe4oP5hgQ6147kzAABcg3CGC2KtMwAA3ItwhgtirTMAANyLcIYLYq0zAADci3CGC2KtMwAA3ItwhgtirTMAANyLcIYLYq0zAADci3CGC2KtMwAA3Itwhhax1hkAAO5DOEOLwgJbHtZsTTsAANA8whla1sI6Z61uBwAAmkU4Q4vKTle7tB0AAGge4QwtYjkNAADch3CGFrGcBgAA7kM4Q4tYTgMAAPchnMEhLKcBAIB7EM7gEJbTAADAPQhncAzLaQAA4BaEMzjE0WUy8vaWtnMlAABc3AhncIijy2n8q/AoMzYBAGgDwhkckjQgVKGBXVps921VDTM2AQBoA8IZHGIxm3R9fJRDbUsqzrZzNQAAXLwIZ3BY3x4BDrUrr6pp50oAALh4Ec7gsNBuji2T4Wg7AADQGOEMDuvtYOhytB0AAGiMcAbHObiG2favmBAAAICzCGdwmKNrnS3L/4rlNAAAcBLhDA5zdK2zk2dqWU4DAAAnEc7gsKQBoereteW1ziQ2QAcAwFmEMzjMYjYp86p+DrVlA3QAAJxDOEOrJA3o6VA7JgUAAOAcwhlahUkBAAC0L8IZWoVJAQAAtC/CGVolaUCoQvx9HGrLHpsAALQe4QytYjGbNCE23KG2W/eXtXM1AABcfAhnaLWrB/VyqN3Gvcd47gwAgFYinKHVIoIdfO7sLM+dAQDQWoQztBrPnQEA0H4IZ2g1njsDAKD9EM7gFJ47AwCgfRDO4BSeOwMAoH0QzuCU1jx39s6nxe1cDQAAFw/CGZzSmufO/nfnNwxtAgDgIMKZmx05ckRjx45VbGyshg0bplWrVnm6JKc5+txZ5bk6hjYBAHCQY+NScBkfHx8tWLBACQkJKikpUWJioiZPnqzAwEBPl9Zqjj53JrGkBgAAjiKcuVlkZKQiIyMlSREREQoLC1N5eXmHDGdJA0LVzc+s09W2FtuWna52Q0UAAHR8DGv+wObNm5WRkaGoqCiZTCatXbu2UZucnBz1799f/v7+Sk5O1rZt25x6r4KCAlmtVkVHR7exas+wmE0adaljQ5sFh0+0czUAAFwcCGc/UFVVpfj4eOXk5DT5em5urrKzszV//nzt3LlT8fHxmjhxoo4dO2Zvk5CQoLi4uEZfR48etbcpLy/X9OnTtWTJknb/TO3p0t5BDrXLK2K9MwAAHGEyDIPfmM0wmUxas2aNpkyZYj+WnJyskSNHatGiRZIkm82m6Oho3XXXXZo3b55D162urtaECRM0c+ZMTZs2rcW21dXfDQlWVlYqOjpaFRUVCg4Obv2HcrGt+8s09X8+cqjtPeMH6Z4JP2rnigAA8D6VlZUKCQlx6Pc3PWetUFNTo4KCAqWlpdmPmc1mpaWlKT8/36FrGIahGTNmKDU1tcVgJklPPfWUQkJC7F/eNgR65cCe8vNx7Mfof7Z8Se8ZAAAtIJy1QllZmaxWq8LDG67vFR4erpKSEoeusXXrVuXm5mrt2rVKSEhQQkKCdu/e3Wz7Bx54QBUVFfavI0eOtOkzuJrFbFLqkN4OtT1dbWVJDQAAWsBsTTcbNWqUbLaWZzfW8/Pzk5+fXztW1Ha3XdlPb+1xLJy+82mxUi7p2c4VAQDQcdFz1gphYWGyWCwqLS1tcLy0tFQREREeqsrzrhzYU/5dHPtReuWjwwxtAgBwAYSzVvD19VViYqLy8vLsx2w2m/Ly8pSSkuLByjzLYjbplpGOPQtXYzW0MO+Ldq4IAICOi3D2A6dPn1ZhYaEKCwslSQcPHlRhYaEOHz4sScrOztZLL72kv/71r9q7d69mz56tqqoq3X777R6s2vOuvSzS4bY57+6n9wwAgGbwzNkP7NixQ+PGjbN/n52dLUnKzMzUsmXLdPPNN+v48eN6+OGHVVJSooSEBK1fv77RJIHOJmlAqAL9LKqqtrbYttZ2vveMZTUAAGiMdc46mNask+JuCzbs04K8/Q619fcx69PHJsliNrVzVQAAeB7rnMEj7hr/I3VxMGydq7PpwwPftnNFAAB0PIQzuIzFbFLWuEscbv+3D79qv2IAAOigCGdwqbvG/0gWB0cq8/aWMjEAAIAfIJzBpSxmkybEOjY5os4mltUAAOAHCGdwuWkp/R1uy7IaAAA0RDiDy53fDN2xsc36ZTUAAMB5hDO4nMVs0uwxjk8MWLjpC3rPAAD4N8IZ2kVrltWwGtLdr+5q54oAAOgYCGdoF61dVuON3cWqqbO1Y0UAAHQMhDO0m9b0nknSA6s/acdqAADoGAhnaDet7T37353f8OwZAKDTI5yhXbVmUVpJ+tnire1XDAAAHQDhDO3KYjYpa6zjvWc7j1To8TeK2rEiAAC8G+EM7e7uCYNb1Xv28paDWvdJcfsVBACAFyOcod1ZzCbdlXppq865e+Uunj8DAHRKhDO4RWtnbtbaDNY+AwB0SoQzuIXFbNKffx7fqnNY+wwA0BkRzuA2P07oo+ExIa0655rfb2qnagAA8E6EM7jVqjuulk8rfupKTlXrl0u3tV9BAAB4GcIZ3MpiNun5X1zRqnM27TuuR1//tJ0qAgDAuxDO4HaTh0Vpclx4q85ZuvUrPfEm658BAC5+hDN4xMJbE1u19pkkvfQB658BAC5+hDN4hMVs0nM3J7T6vF+/upP1zwAAFzXCGTzmxwl9NH5IWKvOqTOkOa8UtFNFAAB4HuEMHvXyjGQN6Nm1Vee89WmpHn+DCQIAgIsT4Qwet/E341q1vIYkvbzlKwIaAOCiRDiDxzmzvIZEQAMAXJwIZ/AKk4dF6Vej+rX6PAIaAOBiQziD13jox3EaHt267Z0kAhoA4OJCOINXWTW7dds71SOgAQAuFoQzeBVnnz+TCGgAgIsD4QxeZ/KwKM0c3d+pcwloAICOjnAGr/TgdZfp9qtbP0FAOh/QHn19j4srAgDAPQhn8FrzM+I0fkgvp85duvWQfrn0IxdXBABA+yOcwau9PCPJ6YC2aV+ZrlvwnmsLAgCgnRHO4PVenpGk26/u79S5n5ZUacTj77BZOgCgwyCcoUOYn3GZfjWqv1PnllXV6tL/XKd1nxx1bVEAALQDwhk6jId+7PwkAUPSnX/fpcffYKIAAMC7Ec7QobRlkoAkvbyFiQIAAO9GOEOH05ZJAtL5iQJj/5DHc2gAAK9EOEOH9PKMJP1q1ACnz/+q/Jwu+c91evbtzwhpAACvQjhDh/XQj2P137cOb9M1nn/3gH70IJMFAADeg3CGDm3ysEgdeHKywgJ8nL6G1WCyAADAexDO0OFZzCbteHiionv4t+k6L285pLQ/vauaOpuLKgMAoPUIZ7hofHD/eI0bHNama+w/fkY/+q+3dOeKHTyLBgDwCMIZLipLb092ejeB71u3p5QJAwAAjyCc4aIzP+MyzRzt/EzO73v+3QO65D/X6U/rCWkAAPcgnOGi9OB152dyuuoHfOF7B+hJAwC4hckwDH7TdCCVlZUKCQlRRUWFgoODPV2O17PaDP1s8f9p55GTLr3ujQlRevqn8fL14d83AICWteb3N+GsgyGcOef1j4/q16/ukqt/2C8JC9QjP7lMV10aJovZ5OKrAwAuFoSzixjhzHnt1YtW74b4KP3+Z/SmAQAaI5xdxAhnbddevWj1egf56T9GDdCMqwcQ1AAAkghnFzXCmWu0dy9avd5BvvqPUQMJagDQyRHOLmKEM9d6/eOjuid3l6xu2BTAz2JSv54BunF4X/1y1EDCGgB0IoSzixjhzPWsNkPPbfhci97bL3eukkFYA4DOg3B2ESOctZ/6kLbw3f3t9jzahfiapUD/LurVzZfABgAXGcLZRYxw1v6sNkML3tmnRe8d8EhI+z5fs9TV1yIfs0mhgX6KjQrRTxP7snQHAHQwhLOLGOHMfTw13OmoEH+LArqYVV1nk2Ey0+MGAF6McHYRI5y5X31Iy3l/v1smDrhCF5MU5G+Rn49FkqHqOpushmQxSX4+FpnNJgX6+WhoJD1xAOAOhLMO4uTJk0pLS1NdXZ3q6up09913a+bMmRc8h3DmOVabof/7okzPbfpcOw6d9HQ5LhfYxSQfi9ke4JoKdZKhWpsU5N9FV13SU//148vU1dfSqvepqbNpef5XOlR+Rv1CAzQtpT89fQAueoSzDsJqtaq6uloBAQGqqqpSXFycduzYoZ49ezZ7DuHMO1hthrbsO655az5RcWW1p8vxKIukbv6WFkNddZ1Np2tsqrE2/iunq4/UvWuXJs+rsRrq4mPRJb0CNeuaSzRqUK9W9/IRCAF4GuGsAyovL9fw4cO1Y8cOhYWFNduOcOZ9aupsWrr1S+W8e0CV5+o8XU6n0NVH6tql6WFbX4tZNdbvvj9XZ+hMbePxaH+LFOBrafa8loKms+fZZFI3Px/F9wnW2Vqr9h+v0pmaOvUIOD/hY0p8lD47dkobPi3RsdPVCg/y14TYcA3uHaQ1hV/rs9LTCvb30bWxEZqW0l87DpbrnzuP6JuT59S3R1fdNLyvRvQP1d8/OkQYBbzIRR/OTp06pYceekhr1qzRsWPHdMUVV+i5557TyJEjXfYemzdv1jPPPKOCggIVFxdrzZo1mjJlSqN2OTk5euaZZ1RSUqL4+HgtXLhQSUlJDr/PyZMnNWbMGH3xxRd65plnlJWVdcH2hDPvdrbGqsfe2KM3PylW5Tmrp8sB7H4YRl0RNN1xnjfW5KrP4t/FR127mFRrk87WWmW12tTVt4t6desiSSo/U2sP7kMighTgZ9HnJadVeuqszCazwoP9NX5ob0nSxqJSlZ46p0BfHw2JCNaQyCDtPVqhXV9XqIvFrKsG9tQDk2NVcLBcL35wQMWV5xQV0lWzrhmoxH6hemLdp/rwy3L5Wsy64Yo+uu3K/nrlo6+0oeiYJEPXxkZoxtUDJEl//b+D2v7VCQX6WnTj8L5KHthTHx34Vv+762udqbFqZP9QZV7VXxazSR8e+Fb5X5ZJMinlkp4a2T9UBYdO6Nipc+od5K+kAaGd5nnXiz6c3XzzzdqzZ48WL16sqKgorVixQn/+859VVFSkPn36NGq/detWJSUlqUuXLg2OFxUVqWfPngoPD290zltvvaWtW7cqMTFRN954Y5PhLDc3V9OnT9cLL7yg5ORkLViwQKtWrdK+ffvUu/f5/2ESEhJUV9e4N+Wdd95RVFSU/fvS0lLdeOONWr16dZP11COcdRz1PWrrdxdrb8kpnavrcP+rAUC7C/Ezy2I2eUXYru/ZHh7TQz8bEe3SyVIXdTg7e/asgoKC9K9//UvXXXed/XhiYqLS09P1u9/9rkF7m82m4cOHa9CgQVq5cqUslvMPL+/bt09jxoxRdna27rvvvgu+p8lkajKcJScna+TIkVq0aJH9vaKjo3XXXXdp3rx5rf5sd955p1JTU/XTn/600Ws5OTnKycmR1WrV559/TjjrgAhrANCxBPpa9Kefx2tSXGSbr9WacNbhHkKoq6uT1WqVv79/g+Ndu3bVli1bGrU3m81at26ddu3apenTp8tms+nAgQNKTU3VlClTWgxmzampqVFBQYHS0tIavFdaWpry8/MdukZpaalOnTolSaqoqNDmzZs1ePDgJttmZWWpqKhI27dvd6peeJ6vj1n/b8ylWjNntD773WR9/rt0PZA+WMOjQxQZ3EX+Pp2jax8AOoqqGqvuWLFT6/cUu/V9fdz6bi4QFBSklJQUPf744xo6dKjCw8P16quvKj8/X5deemmT50RFRWnTpk0aPXq0br31VuXn5ystLU2LFy92uo6ysjJZrdZGQ5Dh4eH67LPPHLrGoUOHNGvWLBmGIcMwdNddd+nyyy93uiZ0LPVh7f+N+e7ntr537e09JSquOKPqWluzD7QDANzj0deLNCE2wm3Px3W4cCZJy5cv1y9/+Uv16dNHFotFw4cP1y233KKCgoJmz4mJidHy5cs1ZswYDRw4UC+//LJMJs/2VCQlJamwsNCjNcC7NBXYpO+W7nhh834dOH5adVab/TmJE2frGCIFgHZUXHFO2w6WK+WS5pe6cqUOGc4uueQSvf/++6qqqlJlZaUiIyN18803a+DAgc2eU1paqlmzZikjI0Pbt2/Xvffeq4ULFzpdQ1hYmCwWi0pLSxu9T0REhNPXBZpiMZs0Zmhvjfn3zKwf+v7zbIfKq2S1Nf/wKz1xANB6x06dc9t7dchwVi8wMFCBgYE6ceKE3n77bf3hD39osl1ZWZnGjx+voUOHatWqVfr88881duxY+fn56Y9//KNT7+3r66vExETl5eXZJwrYbDbl5eVpzpw5zn4kwCnN9bg15/vDpyWVZ2XYWp715OdjUcW5OoIdgE6pd5B/y41cpEOGs7fffluGYWjw4MHav3+/fvvb32rIkCG6/fbbG7W12WxKT09Xv379lJubKx8fH8XGxmrDhg1KTU1Vnz59dO+99zY67/Tp09q/f7/9+4MHD6qwsFChoaGKiYmRJGVnZyszM1MjRoxQUlKSFixYoKqqqibrALxJa8Pc9zX1XFxrprd3sZjVJ8RfJtP5dZyqqmvla2n6vLO1Vp1kvTgAHhYZcn5NNnfpcEtpSNI//vEPPfDAA/r6668VGhqqm266SU888YRCQkKabL9hwwaNHj260QzPXbt2qVevXurbt2+jc9577z2NGzeu0fHMzEwtW7bM/v2iRYvsi9AmJCTo+eefV3Jycts+4AWwzhk6m/o9TVcVHNanRyt04kzNBYdtm1rXyGw2qWsXs3x9LKqps+pMrbVRb6E71lE6VW3V6Rp6HoGO5oXbhrd5OY2Lep2zzo5wBnRs9WHz+6up33ZlP+04WK5/7DjUYEX331w7RM+8s1f5B76V1Wboiugeun5YlDbsK1X+gTJVVTe9erxJLYfRjrqqvjfU5IrPYrUZqqqxiV/A3i3Qz6I//cz965wRzjoYwhkAXBysNkPbDpY32MrIajO0PP8r+76otyb307YD32rJli9Vea5W8X2768HrYiVJj72xp8ktl975tFSnqms1NKLhXq0/3N7ps+JKFRVX6lydTeFB320FteHTEh05eVZBfl2UNKCHokMDlFdUqkPlVTJkVlhgF8VHd9c3J87qwPHTstoMhQb6aXB4N52pqdP+41Wqqq6Vn4+PAnwb/iOhqTRqGN4TttkhAE4hnAEA0PFc1DsEAAAAXMwIZwAAAF6EcAYAAOBFCGcAAABehHAGAADgRQhnAAAAXoRwBgAA4EUIZwAAAF6EcAYAAOBFfDxdAFqnfkOHyspKD1cCAAAcVf9725GNmQhnHcypU6ckSdHR0R6uBAAAtNapU6cUEhJywTbsrdnB2Gw2HT16VEFBQTKZXLMZq3Q+0UdHR+vIkSPs2dnOuNfuwX12D+6ze3Cf3ae97rVhGDp16pSioqJkNl/4qTJ6zjoYs9msvn37ttv1g4OD+R/fTbjX7sF9dg/us3twn92nPe51Sz1m9ZgQAAAA4EUIZwAAAF6EcAZJkp+fn+bPny8/Pz9Pl3LR4167B/fZPbjP7sF9dh9vuNdMCAAAAPAi9JwBAAB4EcIZAACAFyGcAQAAeBHCGQAAgBchnEGSlJOTo/79+8vf31/Jycnatm2bp0vqUJ566imNHDlSQUFB6t27t6ZMmaJ9+/Y1aHPu3DllZWWpZ8+e6tatm2666SaVlpY2aHP48GFdd911CggIUO/evfXb3/5WdXV17vwoHcrTTz8tk8mke+65x36M++wa33zzjW677Tb17NlTXbt21eWXX64dO3bYXzcMQw8//LAiIyPVtWtXpaWl6YsvvmhwjfLyck2dOlXBwcHq3r27fvWrX+n06dPu/ihey2q16qGHHtKAAQPUtWtXXXLJJXr88ccb7L3IfXbO5s2blZGRoaioKJlMJq1du7bB6666r5988olGjx4tf39/RUdH6w9/+INrPoCBTm/lypWGr6+v8Ze//MX49NNPjZkzZxrdu3c3SktLPV1ahzFx4kRj6dKlxp49e4zCwkJj8uTJRkxMjHH69Gl7mzvuuMOIjo428vLyjB07dhhXXnmlcdVVV9lfr6urM+Li4oy0tDRj165dxrp164ywsDDjgQce8MRH8nrbtm0z+vfvbwwbNsy4++677ce5z21XXl5u9OvXz5gxY4bx0UcfGV9++aXx9ttvG/v377e3efrpp42QkBBj7dq1xscff2z85Cc/MQYMGGCcPXvW3mbSpElGfHy88eGHHxoffPCBcemllxq33HKLJz6SV3riiSeMnj17Gm+88YZx8OBBY9WqVUa3bt2M5557zt6G++ycdevWGQ8++KCxevVqQ5KxZs2aBq+74r5WVFQY4eHhxtSpU409e/YYr776qtG1a1fjxRdfbHP9hDMYSUlJRlZWlv17q9VqREVFGU899ZQHq+rYjh07Zkgy3n//fcMwDOPkyZNGly5djFWrVtnb7N2715Bk5OfnG4Zx/i8Ts9lslJSU2NssXrzYCA4ONqqrq937AbzcqVOnjEGDBhkbNmwwxowZYw9n3GfXuP/++41Ro0Y1+7rNZjMiIiKMZ555xn7s5MmThp+fn/Hqq68ahmEYRUVFhiRj+/bt9jZvvfWWYTKZjG+++ab9iu9ArrvuOuOXv/xlg2M33nijMXXqVMMwuM+u8sNw5qr7+t///d9Gjx49Gvy9cf/99xuDBw9uc80Ma3ZyNTU1KigoUFpamv2Y2WxWWlqa8vPzPVhZx1ZRUSFJCg0NlSQVFBSotra2wX0eMmSIYmJi7Pc5Pz9fl19+ucLDw+1tJk6cqMrKSn366adurN77ZWVl6brrrmtwPyXus6u89tprGjFihH72s5+pd+/euuKKK/TSSy/ZXz948KBKSkoa3OeQkBAlJyc3uM/du3fXiBEj7G3S0tJkNpv10Ucfue/DeLGrrrpKeXl5+vzzzyVJH3/8sbZs2aL09HRJ3Of24qr7mp+fr2uuuUa+vr72NhMnTtS+fft04sSJNtXIxuedXFlZmaxWa4NfVJIUHh6uzz77zENVdWw2m0333HOPrr76asXFxUmSSkpK5Ovrq+7duzdoGx4erpKSEnubpv471L+G81auXKmdO3dq+/btjV7jPrvGl19+qcWLFys7O1v/+Z//qe3bt+vXv/61fH19lZmZab9PTd3H79/n3r17N3jdx8dHoaGh3Od/mzdvniorKzVkyBBZLBZZrVY98cQTmjp1qiRxn9uJq+5rSUmJBgwY0Oga9a/16NHD6RoJZ4CLZWVlac+ePdqyZYunS7noHDlyRHfffbc2bNggf39/T5dz0bLZbBoxYoSefPJJSdIVV1yhPXv26IUXXlBmZqaHq7t4/OMf/9Arr7yiv//977rssstUWFioe+65R1FRUdznTo5hzU4uLCxMFoul0Wy20tJSRUREeKiqjmvOnDl644039O6776pv37724xEREaqpqdHJkycbtP/+fY6IiGjyv0P9azg/bHns2DENHz5cPj4+8vHx0fvvv6/nn39ePj4+Cg8P5z67QGRkpGJjYxscGzp0qA4fPizpu/t0ob83IiIidOzYsQav19XVqby8nPv8b7/97W81b948/eIXv9Dll1+uadOm6d5779VTTz0lifvcXlx1X9vz7xLCWSfn6+urxMRE5eXl2Y/ZbDbl5eUpJSXFg5V1LIZhaM6cOVqzZo02bdrUqKs7MTFRXbp0aXCf9+3bp8OHD9vvc0pKinbv3t3gL4QNGzYoODi40S/Kzmr8+PHavXu3CgsL7V8jRozQ1KlT7X/mPrfd1Vdf3WgpmM8//1z9+vWTJA0YMEAREREN7nNlZaU++uijBvf55MmTKigosLfZtGmTbDabkpOT3fApvN+ZM2dkNjf8NWyxWGSz2SRxn9uLq+5rSkqKNm/erNraWnubDRs2aPDgwW0a0pTEUho4v5SGn5+fsWzZMqOoqMiYNWuW0b179waz2XBhs2fPNkJCQoz33nvPKC4utn+dOXPG3uaOO+4wYmJijE2bNhk7duwwUlJSjJSUFPvr9Us8XHvttUZhYaGxfv16o1evXizx0ILvz9Y0DO6zK2zbts3w8fExnnjiCeOLL74wXnnlFSMgIMBYsWKFvc3TTz9tdO/e3fjXv/5lfPLJJ8b111/f5FIEV1xxhfHRRx8ZW7ZsMQYNGtTpl3j4vszMTKNPnz72pTRWr15thIWFGffdd5+9DffZOadOnTJ27dpl7Nq1y5BkPPvss8auXbuMQ4cOGYbhmvt68uRJIzw83Jg2bZqxZ88eY+XKlUZAQABLacB1Fi5caMTExBi+vr5GUlKS8eGHH3q6pA5FUpNfS5cutbc5e/asceeddxo9evQwAgICjBtuuMEoLi5ucJ2vvvrKSE9PN7p27WqEhYUZv/nNb4za2lo3f5qO5YfhjPvsGq+//roRFxdn+Pn5GUOGDDGWLFnS4HWbzWY89NBDRnh4uOHn52eMHz/e2LdvX4M23377rXHLLbcY3bp1M4KDg43bb7/dOHXqlDs/hlerrKw07r77biMmJsbw9/c3Bg4caDz44IMNlmbgPjvn3XffbfLv5MzMTMMwXHdfP/74Y2PUqFGGn5+f0adPH+Ppp592Sf0mw/jeUsQAAADwKJ45AwAA8CKEMwAAAC9COAMAAPAihDMAAAAvQjgDAADwIoQzAAAAL0I4AwAA8CKEMwAAAC9COAOADs5kMmnt2rWeLgOAixDOAKANZsyYIZPJ1Ohr0qRJni4NQAfl4+kCAKCjmzRpkpYuXdrgmJ+fn4eqAdDR0XMGAG3k5+eniIiIBl89evSQdH7IcfHixUpPT1fXrl01cOBA/fOf/2xw/u7du5WamqquXbuqZ8+emjVrlk6fPt2gzV/+8hdddtll8vPzU2RkpObMmdPg9bKyMt1www0KCAjQoEGD9Nprr7XvhwbQbghnANDOHnroId100036+OOPNXXqVP3iF7/Q3r17JUlVVVWaOHGievTooe3bt2vVqlXauHFjg/C1ePFiZWVladasWdq9e7dee+01XXrppQ3e49FHH9XPf/5zffLJJ5o8ebKmTp2q8vJyt35OAC5iAACclpmZaVgsFiMwMLDB1xNPPGEYhmFIMu64444G5yQnJxuzZ882DMMwlixZYvTo0cM4ffq0/fU333zTMJvNRklJiWEYhhEVFWU8+OCDzdYgyfiv//ov+/enT582JBlvvfWWyz4nAPfhmTMAaKNx48Zp8eLFDY6Fhoba/5ySktLgtZSUFBUWFkqS9u7dq/j4eAUGBtpfv/rqq2Wz2bRv3z6ZTCYdPXpU48ePv2ANw4YNs/85MDBQwcHBOnbsmLMfCYAHEc4AoI0CAwMbDTO6SteuXR1q16VLlwbfm0wm2Wy29igJQDvjmTMAaGcffvhho++HDh0qSRo6dKg+/vhjVVVV2V/funWrzGazBg8erKCgIPXv3195eXlurRmA59BzBgBtVF1drZKSkgbHfHx8FBYWJklatWqVRowYoVGjRumVV17Rtm3b9PLLL0uSpk6dqvnz5yszM1OPPPKIjh8/rrvuukvTpk1TeHi4JOmRRx7RHXfcod69eys9PV2nTp3S1q1bddddd7n3gwJwC8IZALTR+vXrFRkZ2eDY4MGD9dlnn0k6P5Ny5cqVuvPOOxUZGalXX31VsbGxkqSAgAC9/fbbuvvuuzVy5EgFBATopptu0rPPPmu/VmZmps6dO6c///nPmjt3rsLCwvTTn/7UfR8QgFuZDMMwPF0EAFysTCaT1qxZoylTpni6FAAdBM+cAQAAeBHCGQAAgBfhmTMAaEc8OQKgteg5AwAA8CKEMwAAAC9COAMAAPAihDMAAAAvQjgDAADwIoQzAAAAL0I4AwAA8CKEMwAAAC/y/wGSrWPCno1jrQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete! Total training time: 476.45 seconds\n"
          ]
        }
      ],
      "source": [
        "# Train the GNN model using the provided source and target graph embeddings, edges, and training data\n",
        "trained_model = train_model_gnn(\n",
        "    model=GIT_model,                # The GNN model to be trained (initialized earlier)\n",
        "    x_src=x_src,                    # Source node embeddings (tensor for source graph)\n",
        "    edge_src=edge_src,              # Source graph edges (undirected edge index for source graph)\n",
        "    x_tgt=x_tgt,                    # Target node embeddings (tensor for target graph)\n",
        "    edge_tgt=edge_tgt,              # Target graph edges (undirected edge index for target graph)\n",
        "    tensor_term1=tensor_term1_o,    # Indices of source entities for training\n",
        "    tensor_term2=tensor_term2_o,    # Indices of target entities for training\n",
        "    tensor_score=tensor_score_o,    # Scores (labels) indicating if pairs match (1) or not (0)\n",
        "    learning_rate=0.0001,            # Learning rate for the Adam optimizer\n",
        "    weight_decay_value=1e-4,        # Weight decay for L2 regularization to prevent overfitting\n",
        "    num_epochs=1000,                # Number of training epochs\n",
        "    print_interval=10               # Interval at which to print training progress (every 10 epochs)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYyhvjcTUaae"
      },
      "source": [
        "# GIT Application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "BZ_VqP6tq6iD"
      },
      "outputs": [],
      "source": [
        "# Determine if a GPU is available and move the computations to it; otherwise, use the CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Assuming the model has been trained and hyperparameters (x_src, edge_src, x_tgt, edge_tgt) are set\n",
        "\n",
        "# Move the trained GIT_model to the device (GPU or CPU)\n",
        "GIT_model.to(device)\n",
        "\n",
        "# Move the data tensors to the same device (GPU or CPU)\n",
        "x_tgt = x_tgt.to(device)         # Target node embeddings\n",
        "edge_tgt = edge_tgt.to(device)   # Target graph edges\n",
        "x_src = x_src.to(device)         # Source node embeddings\n",
        "edge_src = edge_src.to(device)   # Source graph edges\n",
        "\n",
        "# Set the model to evaluation mode; this disables dropout and batch normalization\n",
        "GIT_model.eval()\n",
        "\n",
        "# Pass the source and target embeddings through the trained GNN model to update the embeddings\n",
        "with torch.no_grad():  # Disable gradient computation (inference mode)\n",
        "    embeddings_tgt = GIT_model(x_tgt, edge_tgt)  # Get updated embeddings for the target graph\n",
        "    embeddings_src = GIT_model(x_src, edge_src)  # Get updated embeddings for the source graph\n",
        "\n",
        "# Detach the embeddings from the computation graph and move them back to the CPU\n",
        "# This step is useful if you need to use the embeddings for tasks outside PyTorch (e.g., saving to disk)\n",
        "embeddings_tgt = embeddings_tgt.detach().cpu()  # Target graph embeddings\n",
        "embeddings_src = embeddings_src.detach().cpu()  # Source graph embeddings\n",
        "\n",
        "# At this point, embeddings_tgt and embeddings_src contain the updated embeddings, ready for downstream tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Og5fdoGJrTCG"
      },
      "source": [
        "# Selecting embedding pairs to train the Gated Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "J0nTwc-dnjLn"
      },
      "outputs": [],
      "source": [
        "# Read the training pairs from a CSV file into a pandas DataFrame\n",
        "df_embeddings = pd.read_csv(train_file, index_col=0)\n",
        "\n",
        "# Extract columns and convert to NumPy arrays\n",
        "tensor_term1 = df_embeddings['SrcEntity'].values.astype(int)  # Source entity indices\n",
        "tensor_term2 = df_embeddings['TgtEntity'].values.astype(int)  # Target entity indices\n",
        "tensor_score = df_embeddings['Score'].values.astype(float)  # Matching scores\n",
        "\n",
        "# Split data into training and validation sets\n",
        "tensor_term1_train, tensor_term1_val, tensor_term2_train, tensor_term2_val, tensor_score_train, tensor_score_val = train_test_split(\n",
        "    tensor_term1, tensor_term2, tensor_score, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Convert split data to PyTorch tensors\n",
        "tensor_term1_train = torch.from_numpy(tensor_term1_train).type(torch.LongTensor)\n",
        "tensor_term2_train = torch.from_numpy(tensor_term2_train).type(torch.LongTensor)\n",
        "tensor_score_train = torch.from_numpy(tensor_score_train).type(torch.FloatTensor)\n",
        "\n",
        "tensor_term1_val = torch.from_numpy(tensor_term1_val).type(torch.LongTensor)\n",
        "tensor_term2_val = torch.from_numpy(tensor_term2_val).type(torch.LongTensor)\n",
        "tensor_score_val = torch.from_numpy(tensor_score_val).type(torch.FloatTensor)\n",
        "\n",
        "# Move the embeddings back to the CPU if not already there\n",
        "x_tgt = x_tgt.cpu()  # Target node embeddings\n",
        "x_src = x_src.cpu()  # Source node embeddings\n",
        "\n",
        "# Select embeddings for the training set\n",
        "X1_train = select_rows_by_index(embeddings_src, tensor_term1_train)\n",
        "X2_train = select_rows_by_index(x_src, tensor_term1_train)\n",
        "X3_train = select_rows_by_index(embeddings_tgt, tensor_term2_train)\n",
        "X4_train = select_rows_by_index(x_tgt, tensor_term2_train)\n",
        "\n",
        "# Select embeddings for the validation set\n",
        "X1_val = select_rows_by_index(embeddings_src, tensor_term1_val)\n",
        "X2_val = select_rows_by_index(x_src, tensor_term1_val)\n",
        "X3_val = select_rows_by_index(embeddings_tgt, tensor_term2_val)\n",
        "X4_val = select_rows_by_index(x_tgt, tensor_term2_val)\n",
        "\n",
        "# Now you have:\n",
        "# - Training tensors: X1_train, X2_train, X3_train, X4_train, tensor_score_train\n",
        "# - Validation tensors: X1_val, X2_val, X3_val, X4_val, tensor_score_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNdCgaTMExPK"
      },
      "source": [
        "# Gated Network Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Gof1eIPIWSVU"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def train_gated_combination_model(X1_t, X2_t, X3_t, X4_t, tensor_score_o,\n",
        "                                  X1_val, X2_val, X3_val, X4_val, tensor_score_val,\n",
        "                                  epochs=120, batch_size=32, learning_rate=0.001, weight_decay=1e-5):\n",
        "    \"\"\"\n",
        "    Trains the GatedCombination model with training and validation data, using ReduceLROnPlateau scheduler.\n",
        "    Also calculates and displays F1-score during training and validation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create datasets and DataLoaders\n",
        "    train_dataset = TensorDataset(X1_t, X2_t, X3_t, X4_t, tensor_score_o)\n",
        "    val_dataset = TensorDataset(X1_val, X2_val, X3_val, X4_val, tensor_score_val)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = GatedCombinationWithFaiss(X1_t.shape[1]).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # Use ReduceLROnPlateau scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0.0\n",
        "        y_true_train, y_pred_train = [], []\n",
        "\n",
        "        for batch_X1, batch_X2, batch_X3, batch_X4, batch_y in train_loader:\n",
        "            batch_X1, batch_X2, batch_X3, batch_X4, batch_y = (\n",
        "                batch_X1.to(device),\n",
        "                batch_X2.to(device),\n",
        "                batch_X3.to(device),\n",
        "                batch_X4.to(device),\n",
        "                batch_y.to(device),\n",
        "            )\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(batch_X1, batch_X2, batch_X3, batch_X4)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = F.binary_cross_entropy(outputs, batch_y.unsqueeze(1).float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            # Store true labels and predictions for F1-score\n",
        "            y_true_train.extend(batch_y.cpu().numpy())\n",
        "            y_pred_train.extend((outputs > 0.5).float().cpu().numpy())\n",
        "\n",
        "        train_loss = total_train_loss / len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Calculate F1-score for training\n",
        "        train_f1 = f1_score(y_true_train, y_pred_train)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        total_val_loss = 0.0\n",
        "        y_true_val, y_pred_val = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_X1, batch_X2, batch_X3, batch_X4, batch_y in val_loader:\n",
        "                batch_X1, batch_X2, batch_X3, batch_X4, batch_y = (\n",
        "                    batch_X1.to(device),\n",
        "                    batch_X2.to(device),\n",
        "                    batch_X3.to(device),\n",
        "                    batch_X4.to(device),\n",
        "                    batch_y.to(device),\n",
        "                )\n",
        "                outputs = model(batch_X1, batch_X2, batch_X3, batch_X4)\n",
        "\n",
        "                # Compute loss\n",
        "                val_loss = F.binary_cross_entropy(outputs, batch_y.unsqueeze(1).float())\n",
        "                total_val_loss += val_loss.item()\n",
        "\n",
        "                # Store true labels and predictions for F1-score\n",
        "                y_true_val.extend(batch_y.cpu().numpy())\n",
        "                y_pred_val.extend((outputs > 0.5).float().cpu().numpy())\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Calculate F1-score for validation\n",
        "        val_f1 = f1_score(y_true_val, y_pred_val)\n",
        "\n",
        "        # Step the scheduler with validation loss\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        # Print training and validation metrics\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}] Training Loss: {train_loss:.4f}, F1 Score: {train_f1:.4f} | \"\n",
        "              f\"Validation Loss: {avg_val_loss:.4f}, F1 Score: {val_f1:.4f}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Plotting training and validation loss\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_losses, label=\"Training Loss\", marker='o')\n",
        "    plt.plot(val_losses, label=\"Validation Loss\", marker='x')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Training complete! Total time: {end_time - start_time:.2f} seconds\")\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QrpFp6aDbtSW",
        "outputId": "0f190e19-fc34-421d-ddb4-7233ef78e5a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100] Training Loss: 0.2305, F1 Score: 0.1147 | Validation Loss: 0.1137, F1 Score: 0.2959\n",
            "Epoch [2/100] Training Loss: 0.1184, F1 Score: 0.2787 | Validation Loss: 0.1127, F1 Score: 0.3043\n",
            "Epoch [3/100] Training Loss: 0.1187, F1 Score: 0.2821 | Validation Loss: 0.1128, F1 Score: 0.3107\n",
            "Epoch [4/100] Training Loss: 0.1180, F1 Score: 0.2978 | Validation Loss: 0.1125, F1 Score: 0.3137\n",
            "Epoch [5/100] Training Loss: 0.1181, F1 Score: 0.3076 | Validation Loss: 0.1125, F1 Score: 0.3226\n",
            "Epoch [6/100] Training Loss: 0.1181, F1 Score: 0.3141 | Validation Loss: 0.1125, F1 Score: 0.3307\n",
            "Epoch [7/100] Training Loss: 0.1185, F1 Score: 0.3134 | Validation Loss: 0.1127, F1 Score: 0.3333\n",
            "Epoch [8/100] Training Loss: 0.1182, F1 Score: 0.3234 | Validation Loss: 0.1129, F1 Score: 0.3339\n",
            "Epoch [9/100] Training Loss: 0.1181, F1 Score: 0.3246 | Validation Loss: 0.1126, F1 Score: 0.3349\n",
            "Epoch [10/100] Training Loss: 0.1177, F1 Score: 0.3252 | Validation Loss: 0.1126, F1 Score: 0.3333\n",
            "Epoch [11/100] Training Loss: 0.1174, F1 Score: 0.3261 | Validation Loss: 0.1128, F1 Score: 0.3360\n",
            "Epoch [12/100] Training Loss: 0.1179, F1 Score: 0.3270 | Validation Loss: 0.1125, F1 Score: 0.3365\n",
            "Epoch [13/100] Training Loss: 0.1178, F1 Score: 0.3318 | Validation Loss: 0.1125, F1 Score: 0.3365\n",
            "Epoch [14/100] Training Loss: 0.1180, F1 Score: 0.3297 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [15/100] Training Loss: 0.1181, F1 Score: 0.3294 | Validation Loss: 0.1125, F1 Score: 0.3365\n",
            "Epoch [16/100] Training Loss: 0.1177, F1 Score: 0.3305 | Validation Loss: 0.1126, F1 Score: 0.3339\n",
            "Epoch [17/100] Training Loss: 0.1177, F1 Score: 0.3292 | Validation Loss: 0.1125, F1 Score: 0.3365\n",
            "Epoch [18/100] Training Loss: 0.1178, F1 Score: 0.3320 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [19/100] Training Loss: 0.1178, F1 Score: 0.3316 | Validation Loss: 0.1125, F1 Score: 0.3365\n",
            "Epoch [20/100] Training Loss: 0.1177, F1 Score: 0.3307 | Validation Loss: 0.1125, F1 Score: 0.3365\n",
            "Epoch [21/100] Training Loss: 0.1177, F1 Score: 0.3318 | Validation Loss: 0.1126, F1 Score: 0.3360\n",
            "Epoch [22/100] Training Loss: 0.1177, F1 Score: 0.3316 | Validation Loss: 0.1127, F1 Score: 0.3360\n",
            "Epoch [23/100] Training Loss: 0.1180, F1 Score: 0.3297 | Validation Loss: 0.1125, F1 Score: 0.3365\n",
            "Epoch [24/100] Training Loss: 0.1181, F1 Score: 0.3303 | Validation Loss: 0.1126, F1 Score: 0.3360\n",
            "Epoch [25/100] Training Loss: 0.1178, F1 Score: 0.3299 | Validation Loss: 0.1126, F1 Score: 0.3360\n",
            "Epoch [26/100] Training Loss: 0.1178, F1 Score: 0.3310 | Validation Loss: 0.1126, F1 Score: 0.3360\n",
            "Epoch [27/100] Training Loss: 0.1177, F1 Score: 0.3364 | Validation Loss: 0.1125, F1 Score: 0.3365\n",
            "Epoch [28/100] Training Loss: 0.1179, F1 Score: 0.3290 | Validation Loss: 0.1125, F1 Score: 0.3365\n",
            "Epoch [29/100] Training Loss: 0.1177, F1 Score: 0.3292 | Validation Loss: 0.1125, F1 Score: 0.3365\n",
            "Epoch [30/100] Training Loss: 0.1174, F1 Score: 0.3318 | Validation Loss: 0.1125, F1 Score: 0.3365\n",
            "Epoch [31/100] Training Loss: 0.1184, F1 Score: 0.3292 | Validation Loss: 0.1125, F1 Score: 0.3365\n",
            "Epoch [32/100] Training Loss: 0.1181, F1 Score: 0.3294 | Validation Loss: 0.1125, F1 Score: 0.3365\n",
            "Epoch [33/100] Training Loss: 0.1178, F1 Score: 0.3305 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [34/100] Training Loss: 0.1174, F1 Score: 0.3316 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [35/100] Training Loss: 0.1181, F1 Score: 0.3309 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [36/100] Training Loss: 0.1177, F1 Score: 0.3320 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [37/100] Training Loss: 0.1178, F1 Score: 0.3314 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [38/100] Training Loss: 0.1178, F1 Score: 0.3294 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [39/100] Training Loss: 0.1178, F1 Score: 0.3309 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [40/100] Training Loss: 0.1179, F1 Score: 0.3292 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [41/100] Training Loss: 0.1178, F1 Score: 0.3303 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [42/100] Training Loss: 0.1170, F1 Score: 0.3327 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [43/100] Training Loss: 0.1174, F1 Score: 0.3327 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [44/100] Training Loss: 0.1178, F1 Score: 0.3312 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [45/100] Training Loss: 0.1175, F1 Score: 0.3312 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [46/100] Training Loss: 0.1181, F1 Score: 0.3303 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [47/100] Training Loss: 0.1180, F1 Score: 0.3301 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [48/100] Training Loss: 0.1182, F1 Score: 0.3288 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [49/100] Training Loss: 0.1177, F1 Score: 0.3312 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [50/100] Training Loss: 0.1182, F1 Score: 0.3292 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [51/100] Training Loss: 0.1181, F1 Score: 0.3305 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [52/100] Training Loss: 0.1176, F1 Score: 0.3314 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [53/100] Training Loss: 0.1178, F1 Score: 0.3307 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [54/100] Training Loss: 0.1174, F1 Score: 0.3338 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [55/100] Training Loss: 0.1182, F1 Score: 0.3284 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [56/100] Training Loss: 0.1179, F1 Score: 0.3297 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [57/100] Training Loss: 0.1179, F1 Score: 0.3290 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [58/100] Training Loss: 0.1178, F1 Score: 0.3314 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [59/100] Training Loss: 0.1174, F1 Score: 0.3331 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [60/100] Training Loss: 0.1182, F1 Score: 0.3301 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [61/100] Training Loss: 0.1175, F1 Score: 0.3325 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [62/100] Training Loss: 0.1185, F1 Score: 0.3280 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [63/100] Training Loss: 0.1179, F1 Score: 0.3307 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [64/100] Training Loss: 0.1177, F1 Score: 0.3309 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [65/100] Training Loss: 0.1184, F1 Score: 0.3299 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [66/100] Training Loss: 0.1182, F1 Score: 0.3303 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [67/100] Training Loss: 0.1184, F1 Score: 0.3303 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [68/100] Training Loss: 0.1180, F1 Score: 0.3299 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [69/100] Training Loss: 0.1176, F1 Score: 0.3305 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [70/100] Training Loss: 0.1181, F1 Score: 0.3305 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [71/100] Training Loss: 0.1183, F1 Score: 0.3301 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [72/100] Training Loss: 0.1180, F1 Score: 0.3299 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [73/100] Training Loss: 0.1173, F1 Score: 0.3320 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [74/100] Training Loss: 0.1174, F1 Score: 0.3320 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [75/100] Training Loss: 0.1178, F1 Score: 0.3297 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [76/100] Training Loss: 0.1175, F1 Score: 0.3322 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [77/100] Training Loss: 0.1179, F1 Score: 0.3320 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [78/100] Training Loss: 0.1176, F1 Score: 0.3309 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [79/100] Training Loss: 0.1177, F1 Score: 0.3318 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [80/100] Training Loss: 0.1180, F1 Score: 0.3299 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [81/100] Training Loss: 0.1177, F1 Score: 0.3320 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [82/100] Training Loss: 0.1183, F1 Score: 0.3314 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [83/100] Training Loss: 0.1178, F1 Score: 0.3297 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [84/100] Training Loss: 0.1181, F1 Score: 0.3312 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [85/100] Training Loss: 0.1176, F1 Score: 0.3320 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [86/100] Training Loss: 0.1178, F1 Score: 0.3314 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [87/100] Training Loss: 0.1175, F1 Score: 0.3299 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [88/100] Training Loss: 0.1183, F1 Score: 0.3299 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [89/100] Training Loss: 0.1178, F1 Score: 0.3312 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [90/100] Training Loss: 0.1179, F1 Score: 0.3307 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [91/100] Training Loss: 0.1179, F1 Score: 0.3301 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [92/100] Training Loss: 0.1179, F1 Score: 0.3303 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [93/100] Training Loss: 0.1176, F1 Score: 0.3329 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [94/100] Training Loss: 0.1175, F1 Score: 0.3305 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [95/100] Training Loss: 0.1178, F1 Score: 0.3305 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [96/100] Training Loss: 0.1177, F1 Score: 0.3318 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [97/100] Training Loss: 0.1180, F1 Score: 0.3309 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [98/100] Training Loss: 0.1180, F1 Score: 0.3301 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [99/100] Training Loss: 0.1176, F1 Score: 0.3312 | Validation Loss: 0.1126, F1 Score: 0.3365\n",
            "Epoch [100/100] Training Loss: 0.1177, F1 Score: 0.3312 | Validation Loss: 0.1126, F1 Score: 0.3365\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete! Total time: 222.62 seconds\n"
          ]
        }
      ],
      "source": [
        "# Train the GatedCombination model using training and validation data\n",
        "trained_model = train_gated_combination_model(\n",
        "    X1_train,          # Updated source embeddings (after applying the GNN model)\n",
        "    X2_train,          # Original source embeddings (before applying the GNN model)\n",
        "    X3_train,          # Updated target embeddings (after applying the GNN model)\n",
        "    X4_train,          # Original target embeddings (before applying the GNN model)\n",
        "    tensor_score_train, # Ground truth labels for the training set (1 for matched pairs, 0 for unmatched pairs)\n",
        "\n",
        "    X1_val,            # Updated source embeddings for the validation set\n",
        "    X2_val,            # Original source embeddings for the validation set\n",
        "    X3_val,            # Updated target embeddings for the validation set\n",
        "    X4_val,            # Original target embeddings for the validation set\n",
        "    tensor_score_val,  # Ground truth labels for the validation set (1 for matched pairs, 0 for unmatched pairs)\n",
        "\n",
        "    epochs=100,        # Number of epochs (iterations over the entire training dataset)\n",
        "    batch_size=32,     # Number of training samples processed in one forward/backward pass\n",
        "    learning_rate=0.001, # Learning rate for the optimizer (controls step size during optimization)\n",
        "    weight_decay=1e-4 # Weight decay (L2 regularization) to prevent overfitting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAmLuMtGW9c2"
      },
      "source": [
        "# **Second Round Modifications**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVDqN5AxFu9m"
      },
      "source": [
        "# **Generate Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ITZZcElm8qRN"
      },
      "outputs": [],
      "source": [
        "# Build an indexed dictionary for the source ontology classes\n",
        "# src_class is the file path to the JSON file containing the source ontology classes\n",
        "indexed_dict_src = build_indexed_dict(src_class)\n",
        "\n",
        "# Build an indexed dictionary for the target ontology classes\n",
        "# tgt_class is the file path to the JSON file containing the target ontology classes\n",
        "indexed_dict_tgt = build_indexed_dict(tgt_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbSbmPlRDOs3",
        "outputId": "1a733987-553d-478c-b65e-de6cec71df83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Gated embeddings saved:\n",
            "- Source: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Data/ncit_final_embeddings.tsv\n",
            "- Target: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Data/doid_final_embeddings.tsv\n",
            "⏱️ Execution time: 23.01 seconds\n"
          ]
        }
      ],
      "source": [
        "# Define output file paths for final embeddings of source and target ontologies\n",
        "output_file_src = f\"{data_dir}/{src_ent}_final_embeddings.tsv\"\n",
        "output_file_tgt = f\"{data_dir}/{tgt_ent}_final_embeddings.tsv\"\n",
        "\n",
        "# Save the final gated embeddings for all concepts in source and target ontologies\n",
        "save_gated_embeddings(\n",
        "    gated_model=trained_model,          # The trained GatedCombination model\n",
        "    embeddings_src=embeddings_src,      # GNN-transformed embeddings for source entities\n",
        "    x_src=x_src,                        # Initial semantic embeddings for source entities\n",
        "    embeddings_tgt=embeddings_tgt,      # GNN-transformed embeddings for target entities\n",
        "    x_tgt=x_tgt,                        # Initial semantic embeddings for target entities\n",
        "    indexed_dict_src=indexed_dict_src,  # Index-to-URI mapping for source ontology\n",
        "    indexed_dict_tgt=indexed_dict_tgt,  # Index-to-URI mapping for target ontology\n",
        "    output_file_src=output_file_src,    # Destination file path for source embeddings\n",
        "    output_file_tgt=output_file_tgt     # Destination file path for target embeddings\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIDvbZj2GIGo"
      },
      "source": [
        "# **Filter No Used Concepts**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Gl_wUG9KADo",
        "outputId": "c7a100c1-78a4-4e44-9559-e0a42b897ce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Initial source file: 15992 rows\n",
            "🔍 Initial target file: 8480 rows\n",
            "✅ Source after removing ignored classes: 7065 rows\n",
            "✅ Target after removing ignored classes: 8463 rows\n",
            "📁 Cleaned source file saved to: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Data/ncit_final_embeddings_cleaned.tsv\n",
            "📁 Cleaned target file saved to: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Data/doid_final_embeddings_cleaned.tsv\n"
          ]
        }
      ],
      "source": [
        "# Call the function to filter out ignored concepts (e.g., owl:Thing, deprecated, etc.)\n",
        "# from the source and target ontology embeddings.\n",
        "\n",
        "# Input:\n",
        "# - src_emb_path: Path to the TSV file containing embeddings for the source ontology\n",
        "# - tgt_emb_path: Path to the TSV file containing embeddings for the target ontology\n",
        "# - src_onto / tgt_onto: DeepOnto ontology objects used to identify ignored concepts\n",
        "\n",
        "# Output:\n",
        "# - src_file: Path to the cleaned source embeddings (with ignored concepts removed)\n",
        "# - tgt_file: Path to the cleaned target embeddings (with ignored concepts removed)\n",
        "\n",
        "src_file, tgt_file = filter_ignored_class(\n",
        "    src_emb_path=f\"{data_dir}/{src_ent}_final_embeddings.tsv\",\n",
        "    tgt_emb_path=f\"{data_dir}/{tgt_ent}_final_embeddings.tsv\",\n",
        "    src_onto=src_onto,\n",
        "    tgt_onto=tgt_onto\n",
        "\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpUklR4xnVMH"
      },
      "source": [
        "# **Mappings Generation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljGEyKNOerBT"
      },
      "source": [
        "# **Using faiss l2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOSRYREwerBi",
        "outputId": "7e5229b5-ae46-40e5-ea24-11f051654376"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Using L2 (Euclidean) distance with FAISS\n",
            "Top-10 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Results/ncit2doid_top_10_mappings.tsv\n",
            "⏱️ Execution time: 6.33 seconds\n"
          ]
        }
      ],
      "source": [
        "# Compute the top-10 most similar mappings using l2 distance\n",
        "# between ResMLP-encoded embeddings of the source and target ontologies.\n",
        "# The input embeddings were previously encoded using the ResMLPEncoder,\n",
        "# and the similarity score is computed as the inverse of the l2 distance.\n",
        "# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.\n",
        "topk_faiss_l2(\n",
        "    src_emb_path=f\"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv\",\n",
        "    tgt_emb_path=f\"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv\",\n",
        "    top_k=10,\n",
        "    output_file=f\"{results_dir}/{task}_top_10_mappings.tsv\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD-mvVjaerBh"
      },
      "source": [
        "# **Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Global Metrics: Precision, Recall and F1 score**"
      ],
      "metadata": {
        "id": "r8GRfT_pR1kD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WZKJM46erBi",
        "outputId": "3557adc3-d4b9-4539-c9d0-11c0f644bd97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ➤ Mappings file:   /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Results/ncit2doid_top_10_mappings_predictions.tsv\n",
            "\n",
            "🎯 Evaluation Summary:\n",
            "   - Correct mappings:     2888\n",
            "   - Total predictions:    3233\n",
            "   - Total references:     3280\n",
            "📊 Precision:              0.893\n",
            "📊 Recall:                 0.880\n",
            "📊 F1-score:               0.887\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run the evaluation on the predicted mappings using a filtering and evaluation function.\n",
        "\n",
        "output_file, metrics, correct = evaluate_predictions(\n",
        "    topk_file=f\"{results_dir}/{task}_top_10_mappings.tsv\",\n",
        "    # Path to the TSV file containing predicted mappings with scores (before filtering).\n",
        "\n",
        "    train_file=f\"{dataset_dir}/refs_equiv/train.tsv\",\n",
        "    # Path to the training reference file (used to exclude mappings involving train-only entities).\n",
        "\n",
        "    test_file=f\"{dataset_dir}/refs_equiv/test.tsv\",\n",
        "    # Path to the test reference file (used as the gold standard for evaluation).\n",
        "\n",
        "    src_onto=src_onto,\n",
        "    # The source ontology object, used to detect ignored classes or perform additional filtering.\n",
        "\n",
        "    tgt_onto=tgt_onto,\n",
        "    # The target ontology object, used similarly for filtering ignored or irrelevant classes.\n",
        ")\n",
        "\n",
        "# This function returns:\n",
        "# - `output_file`: the path to the filtered and evaluated output file.\n",
        "# - `metrics`: a tuple containing (Precision, Recall, F1-score).\n",
        "# - `correct`: the number of correctly predicted mappings found in the gold standard.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "collapsed_sections": [
        "zqEXsgPGMVhw",
        "zxCn5ztKVztw"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}