{"cells":[{"cell_type":"markdown","metadata":{"id":"MkAX88H3RNRW"},"source":["# **Package Installation**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":217679,"status":"ok","timestamp":1748118885712,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"bSuJvX5_qNhr","outputId":"02f14711-01e4-4172-afc6-5fe19e878b72"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch==2.6.0\n","  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n","Collecting torchvision==0.21.0\n","  Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n","Collecting filelock (from torch==2.6.0)\n","  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n","Collecting typing-extensions>=4.10.0 (from torch==2.6.0)\n","  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n","Collecting networkx (from torch==2.6.0)\n","  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n","Collecting jinja2 (from torch==2.6.0)\n","  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n","Collecting fsspec (from torch==2.6.0)\n","  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0)\n","  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n","Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.6.0)\n","  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6.0)\n","  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting triton==3.2.0 (from torch==2.6.0)\n","  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n","Collecting sympy==1.13.1 (from torch==2.6.0)\n","  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n","Collecting numpy (from torchvision==0.21.0)\n","  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.21.0)\n","  Downloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n","Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch==2.6.0)\n","  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n","Collecting MarkupSafe>=2.0 (from jinja2->torch==2.6.0)\n","  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n","Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m392.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl (4.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n","Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n","Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision\n","  Attempting uninstall: mpmath\n","    Found existing installation: mpmath 1.3.0\n","    Uninstalling mpmath-1.3.0:\n","      Successfully uninstalled mpmath-1.3.0\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.13.2\n","    Uninstalling typing_extensions-4.13.2:\n","      Successfully uninstalled typing_extensions-4.13.2\n","  Attempting uninstall: sympy\n","    Found existing installation: sympy 1.13.1\n","    Uninstalling sympy-1.13.1:\n","      Successfully uninstalled sympy-1.13.1\n","  Attempting uninstall: pillow\n","    Found existing installation: pillow 11.2.1\n","    Uninstalling pillow-11.2.1:\n","      Successfully uninstalled pillow-11.2.1\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: networkx\n","    Found existing installation: networkx 3.4.2\n","    Uninstalling networkx-3.4.2:\n","      Successfully uninstalled networkx-3.4.2\n","  Attempting uninstall: MarkupSafe\n","    Found existing installation: MarkupSafe 3.0.2\n","    Uninstalling MarkupSafe-3.0.2:\n","      Successfully uninstalled MarkupSafe-3.0.2\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.2\n","    Uninstalling fsspec-2025.3.2:\n","      Successfully uninstalled fsspec-2025.3.2\n","  Attempting uninstall: filelock\n","    Found existing installation: filelock 3.18.0\n","    Uninstalling filelock-3.18.0:\n","      Successfully uninstalled filelock-3.18.0\n","  Attempting uninstall: jinja2\n","    Found existing installation: Jinja2 3.1.6\n","    Uninstalling Jinja2-3.1.6:\n","      Successfully uninstalled Jinja2-3.1.6\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.6.0+cpu\n","    Uninstalling torch-2.6.0+cpu:\n","      Successfully uninstalled torch-2.6.0+cpu\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.21.0+cpu\n","    Uninstalling torchvision-0.21.0+cpu:\n","      Successfully uninstalled torchvision-0.21.0+cpu\n","Successfully installed MarkupSafe-3.0.2 filelock-3.18.0 fsspec-2025.5.1 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.6 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pillow-11.2.1 sympy-1.13.1 torch-2.6.0 torchvision-0.21.0 triton-3.2.0 typing-extensions-4.13.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL"]},"id":"061b665be110413c94c88f62194252db"}},"metadata":{}}],"source":["# Reinstall a specific version of PyTorch (v2.6.0) and torchvision (v0.21.0)\n","# The \"--force-reinstall\" flag ensures that the packages are reinstalled even if the correct version is already present.\n","# This is useful to resolve environment issues or when dependencies need to be reset.\n","!pip install torch==2.6.0 torchvision==0.21.0 --force-reinstall"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25121,"status":"ok","timestamp":1748118932925,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"ItSvFeEAfLBF","outputId":"3f74285e-49ba-41b4-ce02-a2165c6f0be9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.2.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.2.6)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n","Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.1)\n","Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.2.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n","Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n","Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n","Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.2.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n","Requirement already satisfied: torch-geometric==2.4.0 in /usr/local/lib/python3.11/dist-packages (2.4.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (4.67.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (2.2.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (1.15.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (3.1.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (2.32.3)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (3.2.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (1.6.1)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (5.9.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric==2.4.0) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.4.0) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.4.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.4.0) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.4.0) (2025.4.26)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->torch-geometric==2.4.0) (1.5.0)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->torch-geometric==2.4.0) (3.6.0)\n","Looking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html\n","Requirement already satisfied: torch-scatter in /usr/local/lib/python3.11/dist-packages (2.1.2+pt20cpu)\n","Requirement already satisfied: torch-sparse in /usr/local/lib/python3.11/dist-packages (0.6.18+pt20cpu)\n","Requirement already satisfied: torch-cluster in /usr/local/lib/python3.11/dist-packages (1.6.3+pt20cpu)\n","Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.11/dist-packages (1.2.2+pt20cpu)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.3)\n","Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (2.2.6)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting deeponto\n","  Downloading deeponto-0.9.3-py3-none-any.whl.metadata (16 kB)\n","Collecting JPype1 (from deeponto)\n","  Downloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Collecting yacs (from deeponto)\n","  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from deeponto) (2.6.0)\n","Collecting anytree (from deeponto)\n","  Downloading anytree-2.13.0-py3-none-any.whl.metadata (8.0 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from deeponto) (8.2.0)\n","Collecting dill (from deeponto)\n","  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from deeponto) (2.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deeponto) (2.2.6)\n","Requirement already satisfied: scikit_learn in /usr/local/lib/python3.11/dist-packages (from deeponto) (1.6.1)\n","Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.11/dist-packages (from deeponto) (4.51.3)\n","Collecting datasets (from deeponto)\n","  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (from deeponto) (3.8.5)\n","Collecting pprintpp (from deeponto)\n","  Downloading pprintpp-0.4.0-py2.py3-none-any.whl.metadata (7.9 kB)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from deeponto) (3.4.2)\n","Collecting lxml (from deeponto)\n","  Downloading lxml-5.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.5 kB)\n","Collecting textdistance (from deeponto)\n","  Downloading textdistance-4.6.3-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (from deeponto) (7.7.1)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from deeponto) (6.17.1)\n","Collecting enlighten (from deeponto)\n","  Downloading enlighten-1.14.1-py2.py3-none-any.whl.metadata (18 kB)\n","Collecting rdflib (from deeponto)\n","  Downloading rdflib-7.1.4-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from deeponto) (3.9.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (3.18.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (20.0.0)\n","Collecting dill (from deeponto)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (3.5.0)\n","Collecting multiprocess<0.70.17 (from datasets->deeponto)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto)\n","  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (0.31.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (6.0.2)\n","Collecting blessed>=1.17.7 (from enlighten->deeponto)\n","  Downloading blessed-1.21.0-py2.py3-none-any.whl.metadata (13 kB)\n","Collecting prefixed>=0.3.2 (from enlighten->deeponto)\n","  Downloading prefixed-0.9.0-py2.py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (1.8.0)\n","Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (7.34.0)\n","Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (6.1.12)\n","Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (0.1.7)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (1.6.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (5.9.5)\n","Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (24.0.1)\n","Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (6.4.2)\n","Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (5.7.1)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->deeponto) (0.2.0)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->deeponto) (3.6.10)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->deeponto) (3.0.15)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->deeponto) (1.5.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->deeponto) (2024.11.6)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->deeponto) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->deeponto) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->deeponto) (2025.2)\n","Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib->deeponto) (3.2.3)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn->deeponto) (1.15.3)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn->deeponto) (3.6.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (1.0.12)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (2.0.11)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (3.0.9)\n","Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (8.3.6)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (2.5.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (0.15.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (2.11.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (3.1.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (75.2.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (3.5.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (4.13.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->deeponto) (1.3.0)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]->deeponto) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]->deeponto) (0.5.3)\n","Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]->deeponto) (1.6.0)\n","Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from blessed>=1.17.7->enlighten->deeponto) (0.2.13)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (3.11.15)\n","Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel->deeponto)\n","  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (5.2.1)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (3.0.51)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (2.19.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (0.2.0)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (4.9.0)\n","Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel->deeponto) (5.7.2)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->deeponto) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->deeponto) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->deeponto) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->deeponto) (0.4.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->deeponto) (1.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->deeponto) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->deeponto) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->deeponto) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->deeponto) (2025.4.26)\n","Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->deeponto) (1.3.0)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->deeponto) (0.1.5)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy->deeponto) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy->deeponto) (14.0.0)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->deeponto) (0.21.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->deeponto) (7.1.0)\n","Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->deeponto) (6.5.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy->deeponto) (3.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (1.20.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->deeponto) (0.8.4)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel->deeponto) (4.3.8)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->deeponto) (1.2.1)\n","Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (23.1.0)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (5.10.4)\n","Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (7.16.6)\n","Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.8.3)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.18.1)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.21.1)\n","Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.3.1)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->deeponto) (0.7.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->deeponto) (3.0.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->deeponto) (1.17.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->deeponto) (0.1.2)\n","Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.2.4)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (4.13.4)\n","Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (6.2.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.7.1)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.3.0)\n","Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (3.1.3)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.10.2)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.5.1)\n","Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (2.21.1)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (4.23.0)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (21.2.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.5.1)\n","Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.4.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (2025.4.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.24.0)\n","Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.16.0)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.17.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (2.7)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (2.22)\n","Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (4.9.0)\n","Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.8.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.3.1)\n","Downloading deeponto-0.9.3-py3-none-any.whl (89.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading anytree-2.13.0-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading enlighten-1.14.1-py2.py3-none-any.whl (42 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lxml-5.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m119.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pprintpp-0.4.0-py2.py3-none-any.whl (16 kB)\n","Downloading rdflib-7.1.4-py3-none-any.whl (565 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.1/565.1 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading textdistance-4.6.3-py3-none-any.whl (31 kB)\n","Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Downloading blessed-1.21.0-py2.py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading prefixed-0.9.0-py2.py3-none-any.whl (13 kB)\n","Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: prefixed, pprintpp, yacs, textdistance, rdflib, lxml, JPype1, jedi, fsspec, dill, blessed, anytree, multiprocess, enlighten, datasets, deeponto\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.5.1\n","    Uninstalling fsspec-2025.5.1:\n","      Successfully uninstalled fsspec-2025.5.1\n","Successfully installed JPype1-1.5.2 anytree-2.13.0 blessed-1.21.0 datasets-3.6.0 deeponto-0.9.3 dill-0.3.8 enlighten-1.14.1 fsspec-2025.3.0 jedi-0.19.2 lxml-5.4.0 multiprocess-0.70.16 pprintpp-0.4.0 prefixed-0.9.0 rdflib-7.1.4 textdistance-4.6.3 yacs-0.1.8\n"]}],"source":["# === Base Libraries ===\n","!pip install numpy --upgrade\n","!pip install pandas\n","!pip install optuna\n","\n","# === FAISS (for Approximate Nearest Neighbor Search) ===\n","!pip install faiss-cpu        # CPU version (recommended unless using GPU)\n","# !pip install faiss-gpu      # Uncomment if running on CUDA-enabled GPU\n","\n","# === PyTorch Geometric and dependencies ===\n","!pip install torch-geometric==2.4.0\n","!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n","# Optional: latest dev version from GitHub\n","!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n","\n","# === DeepOnto (Ontology Matching Toolkit) ===\n","!pip install deeponto\n","# Optionally install custom version from a GitHub repository\n","# !pip install git+https://github.com/<username>/deeponto.git\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":434,"status":"ok","timestamp":1748118933367,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"nFonRjT5fMCv"},"outputs":[],"source":["# Import pandas for working with tabular data (e.g., CSV, TSV files)\n","import pandas as pd\n","\n","# Import numpy for numerical operations and efficient array handling\n","import numpy as np\n","\n","# Import json for reading and writing JSON-formatted files (useful for config or ontology structures)\n","import json\n","\n","# Import pickle for serializing and deserializing Python objects (e.g., saving models or processed data)\n","import pickle\n","\n","# Import warnings to control or suppress warning messages during runtime\n","import warnings\n","\n","# Import gc (garbage collector) for managing memory manually when dealing with large datasets\n","import gc\n","\n","# Ignore all warning messages to keep the output clean\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":23100,"status":"ok","timestamp":1748118956505,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"uchfZJP2fZwe"},"outputs":[],"source":["# Import PyTorch core library for tensor operations and model definition\n","import torch\n","\n","# Import commonly used PyTorch components\n","from torch import Tensor, optim  # Tensor type and optimization algorithms (e.g., SGD, Adam)\n","\n","# Import PyTorch's neural network module (base class for defining models)\n","import torch.nn as nn\n","\n","# Import PyTorch's functional API for operations like activations and loss functions\n","import torch.nn.functional as F\n","\n","# Import DataLoader utilities for batching and loading datasets during training\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","# === PyTorch Geometric (PyG) modules for graph-based learning ===\n","\n","# Basic graph data structure from PyG\n","from torch_geometric.data import Data\n","\n","# PyG-specific DataLoader for batching graphs\n","from torch_geometric.loader import DataLoader as GeoDataLoader\n","\n","# Import graph convolution layers and pooling functions from PyG\n","from torch_geometric.nn import (\n","    GCNConv,             # Graph Convolutional Network layer\n","    GINConv,             # Graph Isomorphism Network convolution\n","    global_mean_pool,    # Global mean pooling over node embeddings\n","    global_add_pool,     # Global sum pooling over node embeddings\n","    MessagePassing       # Base class for defining custom GNN layers\n",")\n","\n","# Explicitly re-import MessagePassing (optional if already above)\n","from torch_geometric.nn.conv import MessagePassing\n","\n","# Graph utility functions from PyG\n","from torch_geometric.utils import (\n","    to_undirected,       # Converts a directed graph to undirected\n","    softmax              # Softmax over edges (e.g., for attention)\n",")\n","\n","# Initialization utilities for GNN layers\n","from torch_geometric.nn.inits import (\n","    reset,               # Reset parameters\n","    glorot,              # Glorot (Xavier) weight initialization\n","    zeros                # Zero initialization\n",")\n","\n","# Typing utilities from PyG for adjacency and tensor specifications\n","from torch_geometric.typing import (\n","    Adj, OptTensor, PairTensor, SparseTensor\n",")\n","\n","# Dense linear transformation layer from PyG (alternative to torch.nn.Linear)\n","from torch_geometric.nn.dense.linear import Linear\n","\n","# Additional PyTorch neural network components\n","from torch.nn import (\n","    Linear,             # Fully connected (dense) layer\n","    PReLU,              # Parametric ReLU activation\n","    Sequential,         # Layer container for building sequential models\n","    BatchNorm1d,        # Batch normalization for 1D inputs\n","    Dropout             # Dropout regularization\n",")"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":87,"status":"ok","timestamp":1748118956748,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"ziMBSWE8ff1N"},"outputs":[],"source":["# Import matplotlib for creating visualizations (e.g., loss curves, evaluation metrics, embedding projections)\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2141,"status":"ok","timestamp":1748118958893,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"JeAvp6PNfiLh"},"outputs":[],"source":["# Import function to split data into training and testing sets\n","from sklearn.model_selection import train_test_split\n","\n","# Import encoder to convert categorical labels into integer values (useful for classification tasks)\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Import evaluation metrics for classification and regression tasks\n","from sklearn.metrics import (\n","    f1_score,            # Harmonic mean of precision and recall; useful for imbalanced classification\n","    precision_score,     # Measures the proportion of true positives among all predicted positives\n","    accuracy_score,      # Measures overall correctness of predictions (classification)\n","    mean_squared_error,  # Measures average squared difference between predicted and actual values (regression)\n","    mean_absolute_error  # Measures average absolute difference between predicted and actual values (regression)\n",")"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":75001,"status":"ok","timestamp":1748119033908,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"jm1rMZvmfl2M","outputId":"566814e2-be6f-44a2-fc2c-19471593efb9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Please enter the maximum memory located to JVM [8g]: 8g\n","\n"]}],"source":["# Import the Ontology class for loading and manipulating OWL ontologies\n","from deeponto.onto import Ontology\n","\n","# Import all components related to OAEI (Ontology Alignment Evaluation Initiative) benchmarking\n","from deeponto.align.oaei import *\n","\n","# Import data structures for representing mappings between ontology entities\n","from deeponto.align.mapping import EntityMapping, ReferenceMapping\n","# - EntityMapping: represents a predicted alignment (one or more mappings)\n","# - ReferenceMapping: represents the gold standard/reference alignments\n","\n","# Import the evaluator to compute Precision, Recall, and F1-score for alignments\n","from deeponto.align.evaluation import AlignmentEvaluator\n","\n","# Utility function to read TSV/CSV tables as mapping or data frames\n","from deeponto.utils import read_table"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":54,"status":"ok","timestamp":1748119033983,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"JYhwr3Q_ft2N"},"outputs":[],"source":["# Import Optuna, a hyperparameter optimization framework for automating model tuning using strategies like Bayesian optimization\n","import optuna"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1748119033992,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"UmSCo5Olfzuz"},"outputs":[],"source":["# Import the math module for mathematical functions (e.g., sqrt, log, exp)\n","import math\n","\n","# Import the time module for measuring execution time of code blocks or functions\n","import time\n","\n","# Import typing annotations for function signatures and code clarity\n","from typing import Optional, Tuple, Union, Callable\n","# - Optional[T]: denotes a value that could be of type T or None\n","# - Tuple: fixed-size ordered collection of elements\n","# - Union: allows multiple possible types (e.g., Union[int, str])\n","# - Callable: represents a function or method type"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1748119034004,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"9WNn0OMQW2CS"},"outputs":[],"source":["# Import Python's built-in random module for generating pseudo-random numbers\n","import random\n","\n","# Set the seed for PyTorch's random number generator to ensure reproducibility\n","import torch\n","torch.manual_seed(42)\n","\n","# Set the seed for NumPy's random number generator to ensure reproducibility\n","import numpy as np\n","np.random.seed(42)\n","\n","# Set the seed for Python's built-in random module to ensure reproducibility\n","random.seed(42)"]},{"cell_type":"markdown","metadata":{"id":"-abbBHOoRdWl"},"source":["# **Paths Definition**"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23911,"status":"ok","timestamp":1748119057919,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"AVgl_Bb42naS","outputId":"653cd836-a7fa-4a7f-c49e-993e5f43c633"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["# Importing the 'drive' module from Google Colab to interact with Google Drive\n","from google.colab import drive\n","\n","# Mount the user's Google Drive to the Colab environment\n","# After running this, a link will appear to authorize access, and Google Drive will be mounted at '/content/gdrive'\n","drive.mount('/content/gdrive')\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1748119057928,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"36ttssQ3W7cx"},"outputs":[],"source":["# Define the source ontology name\n","src_ent = \"ncit\"\n","\n","# Define the target ontology name\n","tgt_ent = \"doid\"\n","\n","# Define the task name for this ontology matching process\n","task = \"ncit2doid\"\n","\n","# Define the similarity threshold for validating matches\n","thres = 0.50"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1748119057934,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"SJpvkdwVSQye"},"outputs":[],"source":["dir = \"/content/gdrive/My Drive/BioGITOM-VLDB/\"\n","\n","# Define the directory for the dataset containing source and target ontologies\n","dataset_dir = f\"{dir}/Datasets/{task}\"\n","\n","# Define the data directory for storing embeddings, adjacency matrices, and related files\n","data_dir = f\"{dir}/{task}/Data\"\n","\n","# Define the directory for storing the results\n","results_dir = f\"{dir}/{task}/Results\""]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":22003,"status":"ok","timestamp":1748119079940,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"eFDNSFef23er"},"outputs":[],"source":["# Load the Source ontology using the Ontology class from DeepOnto\n","# This initializes the source ontology by loading its .owl file.\n","src_onto = Ontology(f\"{dataset_dir}/{src_ent}.owl\")\n","\n","# Load the Target ontology using the Ontology class from DeepOnto\n","# This initializes the target ontology by loading its .owl file.\n","tgt_onto = Ontology(f\"{dataset_dir}/{tgt_ent}.owl\")\n","\n","# Define the file path for the Source embeddings CSV file\n","# Embeddings for the source ontology entities are stored in this file.\n","src_Emb = f\"{data_dir}/{src_ent}_All_Mini_emb.csv\"\n","\n","# Define the file path for the Target embeddings CSV file\n","# Embeddings for the target ontology entities are stored in this file.\n","tgt_Emb = f\"{data_dir}/{tgt_ent}_All_Mini_emb.csv\"\n","\n","# Define the file path for the Source adjacency matrix\n","# This file represents the relationships (edges) between entities in the source ontology.\n","src_Adjacence = f\"{data_dir}/{src_ent}_adjacence.csv\"\n","\n","# Define the file path for the Target adjacency matrix\n","# This file represents the relationships (edges) between entities in the target ontology.\n","tgt_Adjacence = f\"{data_dir}/{tgt_ent}_adjacence.csv\"\n","\n","# Define the file path for the JSON file containing the Source ontology class labels\n","# This file maps the source ontology entities to their labels or names.\n","src_class = f\"{data_dir}/{src_ent}_classes.json\"\n","\n","# Define the file path for the JSON file containing the Target ontology class labels\n","# This file maps the target ontology entities to their labels or names.\n","tgt_class = f\"{data_dir}/{tgt_ent}_classes.json\"\n","\n","# Define the file path for the train data\n","train_file = f\"{data_dir}/{task}_train.csv\"\n","\n","# Define the file path for the test data\n","# The test file contains reference mappings (ground truth) between the source and target ontologies.\n","test_file = f\"{dataset_dir}/refs_equiv/test.tsv\"\n","\n","# Define the file path for the candidate mappings used during testing\n","# This file includes the candidate pairs (source and target entities) for ranking based metrics.\n","test_cands = f\"{dataset_dir}/refs_equiv/test.cands.tsv\"\n","cands_path = f\"{data_dir}/{task}_cands.csv\"\n","\n","# Define the path where the prediction results will be saved in TSV format\n","# This file will store the final predictions (mappings) between source and target entities.\n","prediction_path = f\"{results_dir}/{task}_matching_results_All_Mini.tsv\"\n","\n","# Define the path where all prediction results will be saved in TSV format\n","# This file will store detailed prediction results, including all candidate scores.\n","all_predictions_path = f\"{results_dir}/{task}_all_predictions_All_Mini.tsv\"\n","\n","# Define the path where formatted ranking predictions will be saved in TSV format\n","# This file will contain predictions formatted for evaluation using ranking-based metrics.\n","formatted_predictions_path = f\"{results_dir}/{task}_formatted_predictions_All_Mini.tsv\""]},{"cell_type":"markdown","metadata":{"id":"zqEXsgPGMVhw"},"source":["# **GIT Architecture**\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":782,"status":"ok","timestamp":1748119080729,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"A_d6XCsUMVhx"},"outputs":[],"source":["# RGIT class definition which inherits from PyTorch Geometric's MessagePassing class\n","class RGIT(MessagePassing):\n","\n","    _alpha: OptTensor  # Define _alpha as an optional tensor for storing attention weights\n","\n","    def __init__(\n","        self,\n","        nn: Callable,  # Neural network to be used in the final layer of the GNN\n","        in_channels: Union[int, Tuple[int, int]],  # Input dimension, can be a single or pair of integers\n","        out_channels: int,  # Output dimension of the GNN\n","        eps: float = 0.,  # GIN parameter: epsilon for GIN aggregation\n","        train_eps: bool = False,  # GIN parameter: whether epsilon should be learnable\n","        heads: int = 1,  # Transformer parameter: number of attention heads\n","        dropout: float = 0.,  # Dropout rate for attention weights\n","        edge_dim: Optional[int] = None,  # Dimension for edge attributes (optional)\n","        bias: bool = True,  # Whether to use bias in linear layers\n","        root_weight: bool = True,  # GIN parameter: whether to apply root weight in aggregation\n","        **kwargs,  # Additional arguments passed to the parent class\n","    ):\n","        # Set the aggregation type to 'add' and initialize the parent class with node_dim=0\n","        kwargs.setdefault('aggr', 'add')\n","        super().__init__(node_dim=0, **kwargs)\n","\n","        # Initialize input/output dimensions, neural network, and GIN/transformer parameters\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.nn = nn  # Neural network used by the GNN\n","        self.initial_eps = eps  # Initial value of epsilon for GIN\n","\n","        # Set epsilon to be learnable or fixed\n","        if train_eps:\n","            self.eps = torch.nn.Parameter(torch.empty(1))  # Learnable epsilon\n","        else:\n","            self.register_buffer('eps', torch.empty(1))  # Non-learnable epsilon (fixed)\n","\n","        # Initialize transformer-related parameters\n","        self.heads = heads\n","        self.dropout = dropout\n","        self.edge_dim = edge_dim\n","        self._alpha = None  # Placeholder for attention weights\n","\n","        # Handle case where in_channels is a single integer or a tuple\n","        if isinstance(in_channels, int):\n","            in_channels = (in_channels, in_channels)\n","\n","        # Define the linear layers for key, query, and value for the transformer mechanism\n","        self.lin_key = Linear(in_channels[0], heads * out_channels)\n","        self.lin_query = Linear(in_channels[1], heads * out_channels)\n","        self.lin_value = Linear(in_channels[0], heads * out_channels)\n","\n","        # Define linear transformation for edge embeddings if provided\n","        if edge_dim is not None:\n","            self.lin_edge = Linear(edge_dim, heads * out_channels, bias=False)\n","        else:\n","            self.lin_edge = self.register_parameter('lin_edge', None)\n","\n","        # Reset all parameters to their initial values\n","        self.reset_parameters()\n","\n","    # Function to reset model parameters\n","    def reset_parameters(self):\n","        super().reset_parameters()  # Call parent class reset method\n","        self.lin_key.reset_parameters()  # Reset key linear layer\n","        self.lin_query.reset_parameters()  # Reset query linear layer\n","        self.lin_value.reset_parameters()  # Reset value linear layer\n","        if self.edge_dim:\n","            self.lin_edge.reset_parameters()  # Reset edge linear layer if used\n","        reset(self.nn)  # Reset the neural network provided\n","        self.eps.data.fill_(self.initial_eps)  # Initialize epsilon with the starting value\n","\n","    # Forward function defining how the input data flows through the model\n","    def forward(self, x: Union[Tensor, PairTensor], edge_index: Adj,\n","                edge_attr: OptTensor = None, return_attention_weights=None):\n","        # Unpack number of heads and output channels\n","        H, C = self.heads, self.out_channels\n","\n","        # If x is a tensor, treat it as a pair of tensors (source and target embeddings)\n","        if isinstance(x, Tensor):\n","            x: PairTensor = (x, x)\n","\n","        # Extract source node embeddings\n","        x_t = x[0]\n","\n","        # Apply linear transformations and reshape query, key, and value for multi-head attention\n","        query = self.lin_query(x[1]).view(-1, H, C)\n","        key = self.lin_key(x[0]).view(-1, H, C)\n","        value = self.lin_value(x[0]).view(-1, H, C)\n","\n","        # Propagate messages through the graph using the propagate function\n","        out = self.propagate(edge_index, query=query, key=key, value=value,\n","                             edge_attr=edge_attr, size=None)\n","\n","        # Retrieve attention weights and reset them\n","        alpha = self._alpha\n","        self._alpha = None  # Reset _alpha after use\n","        out = out.mean(dim=1)  # Take the mean over all attention heads\n","\n","        # Apply GIN aggregation by adding epsilon-scaled original node embeddings\n","        out = out + (1 + self.eps) * x_t\n","        return self.nn(out)  # Pass through the neural network\n","\n","    # Message passing function which calculates attention and combines messages\n","    def message(self, query_i: Tensor, key_j: Tensor, value_j: Tensor,\n","                edge_attr: OptTensor, index: Tensor, ptr: OptTensor,\n","                size_i: Optional[int]) -> Tensor:\n","        # If edge attributes are used, apply linear transformation and add them to the key\n","        if self.lin_edge is not None:\n","            assert edge_attr is not None\n","            edge_attr = self.lin_edge(edge_attr).view(-1, self.heads, self.out_channels)\n","            key_j = key_j + edge_attr\n","\n","        # Calculate attention (alpha) using the dot product between query and key\n","        alpha = (query_i * key_j).sum(dim=-1) / math.sqrt(self.out_channels)\n","        alpha = softmax(alpha, index, ptr, size_i)  # Apply softmax to normalize attention\n","        self._alpha = alpha  # Store attention weights\n","        alpha = F.dropout(alpha, p=self.dropout, training=self.training)  # Apply dropout\n","\n","        # Calculate the output message by applying attention to the value\n","        out = value_j\n","        if edge_attr is not None:\n","            out = out + edge_attr  # Add edge embeddings to the output if present\n","        out = out * alpha.view(-1, self.heads, 1)  # Scale by attention weights\n","        return out\n","\n","    # String representation function for debugging or printing\n","    def __repr__(self) -> str:\n","        return (f'{self.__class__.__name__}({self.in_channels}, '\n","                f'{self.out_channels}, heads={self.heads})')"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1748119080738,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"qwFv6RgHmGCf"},"outputs":[],"source":["# Define the RGIT_mod class, a multi-layer GNN that uses both RGIT and linear layers\n","class RGIT_mod(torch.nn.Module):\n","    \"\"\"Multi-layer RGIT with optional linear layers\"\"\"\n","\n","    # Initialize the model with hidden dimension, number of RGIT layers, and number of linear layers\n","    def __init__(self, dim_h, num_layers, num_linear_layers=1):\n","        super(RGIT_mod, self).__init__()\n","        self.num_layers = num_layers  # Number of RGIT layers\n","        self.num_linear_layers = num_linear_layers  # Number of linear layers\n","        self.linears = torch.nn.ModuleList()  # List to store linear layers\n","        self.rgit_layers = torch.nn.ModuleList()  # List to store RGIT layers\n","\n","        # Create a list of Linear and PReLU layers (for encoding entity names)\n","        for _ in range(num_linear_layers):\n","            self.linears.append(Linear(dim_h, dim_h))  # Linear transformation layer\n","            self.linears.append(PReLU(num_parameters=dim_h))  # Parametric ReLU activation function\n","\n","        # Create a list of RGIT layers\n","        for _ in range(num_layers):\n","            self.rgit_layers.append(RGIT(  # Each RGIT layer contains a small MLP with Linear and PReLU\n","                Sequential(Linear(dim_h, dim_h), PReLU(num_parameters=dim_h),\n","                           Linear(dim_h, dim_h), PReLU(num_parameters=dim_h)), dim_h, dim_h))\n","\n","    # Forward pass through the model\n","    def forward(self, x, edge_index):\n","        # Apply the linear layers first to the input\n","        for layer in self.linears:\n","            x = layer(x)\n","\n","        # Then apply the RGIT layers for message passing\n","        for layer in self.rgit_layers:\n","            x = layer(x, edge_index)\n","\n","        return x  # Return the final node embeddings after all layers\n"]},{"cell_type":"markdown","metadata":{"id":"zxCn5ztKVztw"},"source":["# **Gated Network Architecture**"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":112,"status":"ok","timestamp":1748119080854,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"7MKQUv7o7zay"},"outputs":[],"source":["# Import required libraries\n","import torch\n","import torch.nn as nn\n","import faiss\n","import numpy as np\n","\n","class GatedCombinationWithFaiss(nn.Module):\n","    def __init__(self, input_dim):\n","        super().__init__()\n","\n","        # Linear layers to compute gating values for the source and target embeddings\n","        self.gate_A_fc = nn.Linear(input_dim, input_dim)\n","        self.gate_B_fc = nn.Linear(input_dim, input_dim)\n","\n","        # Final linear layer to map similarity score to prediction (sigmoid output)\n","        self.fc = nn.Linear(1, 1)\n","\n","    def faiss_l2(self, a, b):\n","        \"\"\"\n","        Compute L2 distances using FAISS (non-differentiable).\n","        This function converts tensors to NumPy, builds a FAISS index, and performs a search.\n","        Only use this during inference or evaluation — not for training.\n","\n","        Args:\n","            a (Tensor): Query vectors (batch_size x dim)\n","            b (Tensor): Database vectors (batch_size x dim)\n","\n","        Returns:\n","            Tensor: L2 distances between aligned rows (one-to-one)\n","        \"\"\"\n","        # Detach tensors from the computation graph and move to CPU\n","        a_np = a.detach().cpu().numpy().astype(np.float32)\n","        b_np = b.detach().cpu().numpy().astype(np.float32)\n","\n","        # Create a FAISS index for L2 distance\n","        index = faiss.IndexFlatL2(a_np.shape[1])\n","        index.add(b_np)\n","\n","        # Perform 1-NN search\n","        distances, _ = index.search(a_np, 1)  # shape: (batch_size, 1)\n","\n","        # Convert back to PyTorch tensor on the original device\n","        return torch.tensor(distances[:, 0], dtype=torch.float32, device=a.device)\n","\n","    def forward(self, x1, x2, x3, x4, return_embeddings=False):\n","        \"\"\"\n","        Forward pass through the gated combination model.\n","        Combines original and transformed embeddings using learned gates.\n","\n","        Args:\n","            x1, x2: original and GNN-transformed embeddings for source entities\n","            x3, x4: original and GNN-transformed embeddings for target entities\n","            return_embeddings (bool): if True, return gated embeddings instead of prediction\n","\n","        Returns:\n","            Tensor: similarity score (if return_embeddings=False)\n","            OR\n","            Tuple[Tensor, Tensor]: gated source and target embeddings (if return_embeddings=True)\n","        \"\"\"\n","\n","        # Compute gate for source embeddings\n","        gate_values1 = torch.sigmoid(self.gate_A_fc(x1))\n","        a = x1 * gate_values1 + x2 * (1 - gate_values1)\n","\n","        # Compute gate for target embeddings\n","        gate_values2 = torch.sigmoid(self.gate_B_fc(x3))\n","        b = x3 * gate_values2 + x4 * (1 - gate_values2)\n","\n","        if return_embeddings:\n","            return a, b\n","\n","        # Compute non-differentiable distance with FAISS (1-to-1)\n","        distance = self.faiss_l2(a, b)\n","\n","        # Pass through a sigmoid layer for binary classification output\n","        out = torch.sigmoid(self.fc(distance.unsqueeze(1)))\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"QpwWQ2ndKGOA"},"source":["# **Encoder Definition**"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":103,"status":"ok","timestamp":1748119080960,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"zYDHQY8fJ6YA"},"outputs":[],"source":["# === Transformer-based Encoder ===\n","class TransformerEncoder(nn.Module):\n","    def __init__(self, embedding_dim, nhead=4, num_layers=1):\n","        super(TransformerEncoder, self).__init__()\n","\n","        # Define a single Transformer encoder layer\n","        # d_model: input/output embedding dimension\n","        # nhead: number of attention heads\n","        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=nhead)\n","\n","        # Stack multiple Transformer encoder layers\n","        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x (Tensor): Input tensor of shape [batch_size, embedding_dim]\n","\n","        Returns:\n","            Tensor: Encoded output of shape [batch_size, embedding_dim]\n","        \"\"\"\n","\n","        # Step 1: Add a sequence length dimension (required format: [seq_len, batch_size, embedding_dim])\n","        x = x.unsqueeze(1)             # Shape: [batch_size, 1, embedding_dim]\n","        x = x.transpose(0, 1)          # Shape: [1, batch_size, embedding_dim] (PyTorch expects seq_len first)\n","\n","        # Step 2: Pass through the Transformer encoder\n","        x = self.transformer_encoder(x)\n","\n","        # Step 3: Remove the sequence dimension to return to shape [batch_size, embedding_dim]\n","        x = x.transpose(0, 1).squeeze(1)\n","\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"aLJ5j9FNMVhy"},"source":["# **Utility functions**"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":100,"status":"ok","timestamp":1748119081021,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"k0L86DgUQjMU"},"outputs":[],"source":["def adjacency_matrix_to_undirected_edge_index(adjacency_matrix):\n","    \"\"\"\n","    Converts an adjacency matrix into an undirected edge index for use in graph-based neural networks.\n","\n","    Args:\n","        adjacency_matrix: A 2D list or array representing the adjacency matrix of a graph.\n","\n","    Returns:\n","        edge_index_undirected: A PyTorch tensor representing the undirected edges.\n","    \"\"\"\n","    # Convert each element in the adjacency matrix to an integer (from boolean or float)\n","    adjacency_matrix = [[int(element) for element in sublist] for sublist in adjacency_matrix]\n","\n","    # Convert the adjacency matrix into a PyTorch LongTensor (used for indexing)\n","    edge_index = torch.tensor(adjacency_matrix, dtype=torch.long)\n","\n","    # Transpose the edge_index tensor so that rows represent edges in the form [source, target]\n","    edge_index = edge_index.t().contiguous()\n","\n","    # Convert the directed edge_index into an undirected edge_index, meaning both directions are added (i.e., (i, j) and (j, i))\n","    edge_index_undirected = to_undirected(edge_index)\n","\n","    return edge_index_undirected  # Return the undirected edge index"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":80,"status":"ok","timestamp":1748119081098,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"YvmOxkLcpf9w"},"outputs":[],"source":["def build_indexed_dict(file_path):\n","    \"\"\"\n","    Builds a dictionary with numeric indexes for each key from a JSON file.\n","\n","    Args:\n","        file_path (str): The path to the JSON file.\n","\n","    Returns:\n","        indexed_dict (dict): A new dictionary where each key from the JSON file is assigned a numeric index.\n","    \"\"\"\n","    # Load the JSON file into a Python dictionary\n","    with open(file_path, 'r') as file:\n","        data = json.load(file)\n","\n","    # Create a new dictionary with numeric indexes as keys and the original JSON keys as values\n","    indexed_dict = {index: key for index, key in enumerate(data.keys())}\n","\n","    return indexed_dict  # Return the newly created dictionary"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":32,"status":"ok","timestamp":1748119081206,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"QgFINoPGl9Wg"},"outputs":[],"source":["def select_rows_by_index(embedding_vector, index_vector):\n","    \"\"\"\n","    Select rows from an embedding vector using an index vector.\n","\n","    Args:\n","        embedding_vector (torch.Tensor): 2D tensor representing the embedding vector with shape [num_rows, embedding_size].\n","        index_vector (torch.Tensor): 1D tensor representing the index vector.\n","\n","    Returns:\n","        torch.Tensor: New tensor with selected rows from the embedding vector.\n","    \"\"\"\n","    # Use torch.index_select to select the desired rows\n","    new_tensor = torch.index_select(embedding_vector, 0, index_vector)\n","\n","    return new_tensor"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1748119081213,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"a12L7vEmmCJq"},"outputs":[],"source":["def contrastive_loss(source_embeddings, target_embeddings, labels, margin=1.0):\n","    \"\"\"\n","    Computes the contrastive loss, a type of loss function used to train models in tasks like matching or similarity learning.\n","\n","    Args:\n","        source_embeddings (torch.Tensor): Embeddings of the source graphs, shape [batch_size, embedding_size].\n","        target_embeddings (torch.Tensor): Embeddings of the target graphs, shape [batch_size, embedding_size].\n","        labels (torch.Tensor): Binary labels indicating if the pairs are matched (1) or not (0), shape [batch_size].\n","        margin (float): Margin value for the contrastive loss. Defaults to 1.0.\n","\n","    Returns:\n","        torch.Tensor: The contrastive loss value.\n","    \"\"\"\n","    # Calculate the pairwise Euclidean distance between source and target embeddings\n","    distances = F.pairwise_distance(source_embeddings, target_embeddings)\n","\n","    # Compute the contrastive loss:\n","    # - For matched pairs (label == 1), the loss is the squared distance between embeddings.\n","    # - For non-matched pairs (label == 0), the loss is based on how far apart the embeddings are,\n","    #   but penalizes them only if the distance is less than the margin.\n","    loss = torch.mean(\n","        labels * 0.4 * distances.pow(2) +  # For positive pairs, minimize the distance (squared)\n","        (1 - labels) * 0.4 * torch.max(torch.zeros_like(distances), margin - distances).pow(2)  # For negative pairs, maximize the distance (up to the margin)\n","    )\n","\n","    return loss  # Return the computed contrastive loss\n"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1748119081219,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"_ggVYlTiO_WA"},"outputs":[],"source":["def compute_mrr_and_hits(reference_file, predicted_file, output_file, k_values=[1, 5, 10]):\n","    \"\"\"\n","    Compute Mean Reciprocal Rank (MRR) and Hits@k metrics for ontology matching results.\n","\n","    Args:\n","        reference_file (str): Path to the reference test candidate file (usually 'test.cands.tsv').\n","        predicted_file (str): Path to the prediction results (with columns: SrcEntity, TgtEntity, Score).\n","        output_file (str): Path to save ranked candidate predictions with scores.\n","        k_values (list): List of integers specifying which Hits@k metrics to compute.\n","\n","    Returns:\n","        dict: A dictionary with MRR and Hits@k scores.\n","    \"\"\"\n","\n","    # Load reference candidate mappings: each row = (SrcEntity, CorrectTgtEntity, [CandidateTgtEntities])\n","    test_candidate_mappings = read_table(reference_file).values.tolist()\n","\n","    # Load predictions and ensure Score is float\n","    predicted_data = pd.read_csv(predicted_file, sep=\"\\t\")\n","    predicted_data[\"Score\"] = predicted_data[\"Score\"].apply(\n","        lambda x: float(x.strip(\"[]\")) if isinstance(x, str) else float(x)\n","    )\n","\n","    # Create a dictionary mapping (SrcEntity, TgtEntity) -> predicted score\n","    score_lookup = {\n","        (row[\"SrcEntity\"], row[\"TgtEntity\"]): row[\"Score\"]\n","        for _, row in predicted_data.iterrows()\n","    }\n","\n","    ranking_results = []\n","\n","    # Rank the candidates for each source entity\n","    for src_ref_class, tgt_ref_class, tgt_cands in test_candidate_mappings:\n","        # Safely parse the candidate list (tgt_cands is a stringified list)\n","        try:\n","            tgt_cands = eval(tgt_cands)\n","        except Exception:\n","            tgt_cands = []\n","\n","        # Score each candidate (use a large negative default if not found)\n","        scored_cands = [\n","            (tgt_cand, score_lookup.get((src_ref_class, tgt_cand), -1e9))\n","            for tgt_cand in tgt_cands\n","        ]\n","\n","        # Sort candidates by score descending\n","        scored_cands = sorted(scored_cands, key=lambda x: x[1], reverse=True)\n","\n","        # Store the ranking result\n","        ranking_results.append((src_ref_class, tgt_ref_class, scored_cands))\n","\n","    # Save ranked predictions for inspection/debugging\n","    pd.DataFrame(ranking_results, columns=[\"SrcEntity\", \"TgtEntity\", \"TgtCandidates\"]).to_csv(\n","        output_file, sep=\"\\t\", index=False\n","    )\n","\n","    # === Evaluation: compute MRR and Hits@k ===\n","    total_entities = len(ranking_results)\n","    reciprocal_ranks = []\n","    hits_at_k = {k: 0 for k in k_values}\n","\n","    for src_entity, tgt_ref_class, tgt_cands in ranking_results:\n","        ranked_candidates = [cand[0] for cand in tgt_cands]  # candidate URIs only\n","        if tgt_ref_class in ranked_candidates:\n","            rank = ranked_candidates.index(tgt_ref_class) + 1\n","            reciprocal_ranks.append(1 / rank)\n","            for k in k_values:\n","                if rank <= k:\n","                    hits_at_k[k] += 1\n","        else:\n","            reciprocal_ranks.append(0)  # No correct match in candidate list\n","\n","    # Compute final metrics\n","    mrr = sum(reciprocal_ranks) / total_entities\n","    hits_at_k = {k: hits / total_entities for k, hits in hits_at_k.items()}\n","\n","    return {\"MRR\": mrr, \"Hits@k\": hits_at_k}"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1748119081225,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"zmzBcuHZDOs3"},"outputs":[],"source":["def save_gated_embeddings(gated_model, embeddings_src, x_src, embeddings_tgt, x_tgt,\n","                          indexed_dict_src, indexed_dict_tgt,\n","                          output_file_src, output_file_tgt):\n","    \"\"\"\n","    Compute and save the final entity embeddings generated by the GatedCombination model\n","    for both source and target ontologies. Outputs include entity URIs and their final vectors.\n","    Measures and prints the execution time of the entire operation.\n","\n","    Args:\n","        gated_model (nn.Module): The trained GatedCombination model.\n","        embeddings_src (Tensor): Structural embeddings for the source ontology.\n","        x_src (Tensor): Semantic embeddings for the source ontology.\n","        embeddings_tgt (Tensor): Structural embeddings for the target ontology.\n","        x_tgt (Tensor): Semantic embeddings for the target ontology.\n","        indexed_dict_src (dict): Index-to-URI mapping for the source ontology.\n","        indexed_dict_tgt (dict): Index-to-URI mapping for the target ontology.\n","        output_file_src (str): Path to save source embeddings (TSV).\n","        output_file_tgt (str): Path to save target embeddings (TSV).\n","    \"\"\"\n","    import pandas as pd\n","    import torch\n","    import time\n","\n","    start_time = time.time()\n","\n","    # Use GPU if available\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    gated_model = gated_model.to(device)\n","    gated_model.eval()\n","\n","    # Move inputs to the same device\n","    embeddings_src = embeddings_src.to(device)\n","    x_src = x_src.to(device)\n","    embeddings_tgt = embeddings_tgt.to(device)\n","    x_tgt = x_tgt.to(device)\n","\n","    with torch.no_grad():\n","        # === Source ontology ===\n","        gate_src = torch.sigmoid(gated_model.gate_A_fc(embeddings_src))\n","        final_src = embeddings_src * gate_src + x_src * (1 - gate_src)\n","        final_src = final_src.cpu().numpy()\n","\n","        # === Target ontology ===\n","        gate_tgt = torch.sigmoid(gated_model.gate_B_fc(embeddings_tgt))\n","        final_tgt = embeddings_tgt * gate_tgt + x_tgt * (1 - gate_tgt)\n","        final_tgt = final_tgt.cpu().numpy()\n","\n","    # Create DataFrames with Concept URI and embedding values\n","    df_src = pd.DataFrame(final_src)\n","    df_src.insert(0, \"Concept\", [indexed_dict_src[i] for i in range(len(df_src))])\n","\n","    df_tgt = pd.DataFrame(final_tgt)\n","    df_tgt.insert(0, \"Concept\", [indexed_dict_tgt[i] for i in range(len(df_tgt))])\n","\n","    # Save embeddings to file\n","    df_src.to_csv(output_file_src, sep='\\t', index=False)\n","    df_tgt.to_csv(output_file_tgt, sep='\\t', index=False)\n","\n","    elapsed_time = time.time() - start_time\n","    print(f\"✅ Gated embeddings saved:\\n- Source: {output_file_src}\\n- Target: {output_file_tgt}\")\n","    print(f\"⏱️ Execution time: {elapsed_time:.2f} seconds\")\n"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":738,"status":"ok","timestamp":1748119081967,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"_KZdtF46GHL4"},"outputs":[],"source":["import pandas as pd\n","\n","def filter_ignored_class(src_emb_path, tgt_emb_path, src_onto, tgt_onto):\n","    \"\"\"\n","    Filters the source and target embedding files by removing concepts considered \"ignored classes\"\n","    (e.g., owl:Thing, deprecated entities, etc.) based on both source and target ontologies.\n","\n","    Args:\n","        src_emb_path (str): Path to the TSV file containing source embeddings with 'Concept' column.\n","        tgt_emb_path (str): Path to the TSV file containing target embeddings with 'Concept' column.\n","        src_onto (Ontology): Source ontology object loaded with DeepOnto.\n","        tgt_onto (Ontology): Target ontology object loaded with DeepOnto.\n","\n","    Returns:\n","        (str, str): Paths to the cleaned source and target embedding files.\n","    \"\"\"\n","\n","    # === Load the embedding files ===\n","    df_src = pd.read_csv(src_emb_path, sep='\\t', dtype=str)\n","    print(f\"🔍 Initial source file: {len(df_src)} rows\")\n","\n","    df_tgt = pd.read_csv(tgt_emb_path, sep='\\t', dtype=str)\n","    print(f\"🔍 Initial target file: {len(df_tgt)} rows\")\n","\n","    # === Step 1: Retrieve ignored classes from both ontologies ===\n","    ignored_class_index = get_ignored_class_index(src_onto)  # e.g., owl:Thing, non-usable classes\n","    ignored_class_index.update(get_ignored_class_index(tgt_onto))  # Merge with target ontology's ignored classes\n","    ignored_uris = set(str(uri).strip() for uri in ignored_class_index)\n","\n","    # === Step 2: Remove rows where the 'Concept' column matches ignored URIs ===\n","    df_src_cleaned = df_src[~df_src['Concept'].isin(ignored_uris)].reset_index(drop=True)\n","    df_tgt_cleaned = df_tgt[~df_tgt['Concept'].isin(ignored_uris)].reset_index(drop=True)\n","\n","    print(f\"✅ Source after removing ignored classes: {len(df_src_cleaned)} rows\")\n","    print(f\"✅ Target after removing ignored classes: {len(df_tgt_cleaned)} rows\")\n","\n","    # === Step 3: Save the cleaned embedding files ===\n","    output_file_src = src_emb_path.replace(\".tsv\", \"_cleaned.tsv\")\n","    output_file_tgt = tgt_emb_path.replace(\".tsv\", \"_cleaned.tsv\")\n","\n","    df_src_cleaned.to_csv(output_file_src, sep='\\t', index=False)\n","    df_tgt_cleaned.to_csv(output_file_tgt, sep='\\t', index=False)\n","\n","    print(f\"📁 Cleaned source file saved to: {output_file_src}\")\n","    print(f\"📁 Cleaned target file saved to: {output_file_tgt}\")\n","\n","    return output_file_src, output_file_tgt"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1748119081973,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"_9YDcnTbKaHk"},"outputs":[],"source":["import pandas as pd\n","import torch\n","\n","def encode_embeddings_with_concept_column(encoder_model, input_file, output_file):\n","    \"\"\"\n","    Applies an encoder model to a set of embeddings (while preserving the 'Concept' column),\n","    and saves the encoded results in the same tabular format.\n","\n","    Args:\n","        encoder_model: A PyTorch model (e.g., LinearEncoder, MLPEncoder, etc.)\n","        input_file (str): Path to the input TSV file containing 'Concept' and embedding vectors.\n","        output_file (str): Path to save the encoded embeddings.\n","    \"\"\"\n","\n","    # Select device (GPU if available, else CPU)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Move the encoder model to the selected device and set it to evaluation mode\n","    encoder_model = encoder_model.to(device)\n","    encoder_model.eval()\n","\n","    # Load the input TSV file containing concept URIs and embeddings\n","    df = pd.read_csv(input_file, sep='\\t')\n","\n","    # Extract the 'Concept' column to preserve URIs\n","    concepts = df['Concept'].tolist()\n","\n","    # Extract the numerical embedding values (excluding the 'Concept' column)\n","    embedding_values = df.drop(columns=['Concept']).values\n","\n","    # Convert the embedding matrix into a PyTorch tensor and move to the device\n","    embeddings = torch.FloatTensor(embedding_values).to(device)\n","\n","    # Pass the embeddings through the encoder model without computing gradients\n","    with torch.no_grad():\n","        encoded = encoder_model(embeddings).cpu().numpy()\n","\n","    # Reconstruct a new DataFrame with the encoded vectors and corresponding URIs\n","    df_encoded = pd.DataFrame(encoded, columns=[f'dim_{i}' for i in range(encoded.shape[1])])\n","    df_encoded.insert(0, \"Concept\", concepts)  # Re-insert the 'Concept' column at the first position\n","\n","    # Save the encoded embeddings to a TSV file\n","    df_encoded.to_csv(output_file, sep='\\t', index=False)\n","    print(f\"✅ Encoded embeddings saved to: {output_file}\")\n"]},{"cell_type":"markdown","metadata":{"id":"HigIe6n_lQ8X"},"source":["# **FAISS Similarity**"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1748119081981,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"zaX6JH9wj_WR"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import faiss\n","import time\n","\n","def load_embeddings(src_emb_path, tgt_emb_path):\n","    df_src = pd.read_csv(src_emb_path, sep='\\t')\n","    df_tgt = pd.read_csv(tgt_emb_path, sep='\\t')\n","    uris_src = df_src[\"Concept\"].values\n","    uris_tgt = df_tgt[\"Concept\"].values\n","    src_vecs = df_src.drop(columns=[\"Concept\"]).values.astype('float32')\n","    tgt_vecs = df_tgt.drop(columns=[\"Concept\"]).values.astype('float32')\n","    return uris_src, uris_tgt, src_vecs, tgt_vecs\n","\n","def save_results(uris_src, uris_tgt, indices, scores, output_file, top_k):\n","    rows = []\n","    for i, (ind_row, score_row) in enumerate(zip(indices, scores)):\n","        src_uri = uris_src[i]\n","        for j, tgt_idx in enumerate(ind_row):\n","            tgt_uri = uris_tgt[tgt_idx]\n","            score = score_row[j]\n","            rows.append((src_uri, tgt_uri, score))\n","    df_result = pd.DataFrame(rows, columns=[\"SrcEntity\", \"TgtEntity\", \"Score\"])\n","    df_result.to_csv(output_file, sep='\\t', index=False)\n","    print(f\"Top-{top_k} FAISS similarity results saved to: {output_file}\")\n","\n","def topk_faiss_l2(src_emb_path, tgt_emb_path, top_k=15, output_file=\"topk_l2.tsv\"):\n","    print(\"🔹 Using L2 (Euclidean) distance with FAISS\")\n","    start = time.time()\n","\n","    uris_src, uris_tgt, src_vecs, tgt_vecs = load_embeddings(src_emb_path, tgt_emb_path)\n","    dim = src_vecs.shape[1]\n","    index = faiss.IndexFlatL2(dim)\n","    index.add(tgt_vecs)\n","    distances, indices = index.search(src_vecs, top_k)\n","    similarity_scores = 1 / (1 + distances)\n","\n","    save_results(uris_src, uris_tgt, indices, similarity_scores, output_file, top_k)\n","\n","    print(f\"⏱️ Execution time: {time.time() - start:.2f} seconds\")\n"]},{"cell_type":"markdown","metadata":{"id":"vjUYOFO7pdCg"},"source":["# **Mappings Evaluation Functions**"]},{"cell_type":"markdown","metadata":{"id":"B6m04nFw_R00"},"source":["# **Precision, Recall, F1**"]},{"cell_type":"markdown","metadata":{"id":"_GW0Am-TmVMR"},"source":["### Evaluation Strategy and Filtering Justification\n","\n","### Filtering Justification\n","\n","In the `evaluate_predictions` function, two important filtering steps are applied to ensure that the evaluation metrics (such as Precision, Recall, and F1-score) accurately reflect the model's performance:\n","\n","\n","#### 1. Filtering Out Training-Only Entities\n","\n","We remove all predicted mappings involving source or target entities that are present **only in the training set** and not in the test set.\n","\n","This step is critical because:\n","\n","- In some datasets like **Bio-ML**, the same entity can appear in both training and test sets, although with **different correspondences**.\n","- If we don't remove training-only entities, it can lead to **label leakage** and **metric distortion**.\n","\n","#### 2. Filtering on `SrcEntity` present in the test set\n","\n","The second step keeps only the predictions where the `SrcEntity` is included in the test reference set.\n","\n","- This eliminates **non-evaluable false positives**, i.e., predicted mappings for source entities that do not appear in the test set and therefore have no ground-truth correspondences. Including such predictions **unfairly penalizes precision and F1-score**, even though they are technically not verifiable errors.\n","\n","- It focuses the evaluation on entities with defined ground-truth mappings, which is critical for computing metrics such as :\n","\n","$P_{\\text{test}} = \\frac{|\\mathcal{M}_{\\text{out}} \\cap \\mathcal{M}_{\\text{test}}|}{|\\mathcal{M}_{\\text{out}} \\setminus (\\mathcal{M}_{\\text{ref}} \\setminus \\mathcal{M}_{\\text{test}})|}$.\n","\n","---\n"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":146,"status":"ok","timestamp":1748119082130,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"CdP6iYirLJW6"},"outputs":[],"source":["import pandas as pd\n","\n","def evaluate_predictions(\n","    topk_file,\n","    train_file,\n","    test_file,\n","    src_onto,\n","    tgt_onto,\n","    threshold=0.0\n","):\n","    # === Step 1: Load input files ===\n","    df = pd.read_csv(topk_file, sep='\\t', dtype=str)\n","    train_df = pd.read_csv(train_file, sep=\"\\t\", dtype=str)\n","    test_df = pd.read_csv(test_file, sep=\"\\t\", dtype=str)\n","\n","    # === Step 2: Remove URIs only present in training set ===\n","    train_uris = set(train_df['SrcEntity']) | set(train_df['TgtEntity'])\n","    test_uris = set(test_df['SrcEntity']) | set(test_df['TgtEntity'])\n","    uris_to_exclude = train_uris - test_uris\n","    df = df[~(df['SrcEntity'].isin(uris_to_exclude) | df['TgtEntity'].isin(uris_to_exclude))].reset_index(drop=True)\n","\n","    # === Step 3: Keep only source entities from the test set ===\n","    src_entities_test = set(test_df['SrcEntity'])\n","    df = df[df['SrcEntity'].isin(src_entities_test)].reset_index(drop=True)\n","\n","    # === Step 4: Save filtered Top-K predictions ===\n","    output_file1 = topk_file.replace(\".tsv\", \"_filtered.tsv\")\n","    df.to_csv(output_file1, sep='\\t', index=False)\n","\n","    # === Step 5: Convert score column to float\n","    df['Score'] = df['Score'].apply(lambda x: float(x.strip(\"[]\")) if isinstance(x, str) else float(x))\n","    df_sorted = df.sort_values(by='Score', ascending=False).reset_index(drop=True)\n","\n","    # === Step 6: Apply 1-1 constraint (greedy matching)\n","    matched_sources = set()\n","    matched_targets = set()\n","    result = []\n","\n","    for _, row in df_sorted.iterrows():\n","        src, tgt, score = row['SrcEntity'], row['TgtEntity'], row['Score']\n","        if src not in matched_sources and tgt not in matched_targets and score >= threshold:\n","            result.append((src, tgt, score))\n","            matched_sources.add(src)\n","            matched_targets.add(tgt)\n","\n","    matching_results_df = pd.DataFrame(result, columns=['SrcEntity', 'TgtEntity', 'Score'])\n","\n","    # === Step 7: Save Top-1 predictions ===\n","    output_file2 = topk_file.replace(\".tsv\", f\"_predictions.tsv\")\n","    matching_results_df.to_csv(output_file2, sep='\\t', index=False)\n","    print(f\"📁 Top-1 file saved: {output_file2}\")\n","    print(f\"📌 Number of predictions in output: {len(matching_results_df)}\")\n","\n","    # === Step 8: Evaluate predictions ===\n","    preds = EntityMapping.read_table_mappings(output_file2)\n","    refs = ReferenceMapping.read_table_mappings(test_file)\n","    results = AlignmentEvaluator.f1(preds, refs)\n","\n","    preds_set = {p.to_tuple() for p in preds}\n","    refs_set = {r.to_tuple() for r in refs}\n","    correct = len(preds_set & refs_set)\n","\n","    print(f\"🎯 Correct mappings (Top-1): {correct}\")\n","    print(f\"📊 Evaluation (P / R / F1): {results}\")\n","\n","    return output_file2, results, correct\n"]},{"cell_type":"markdown","metadata":{"id":"jPuzmu6f_Y8W"},"source":["# **Precision@k, Recall@k, F1@k**"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":134,"status":"ok","timestamp":1748119082268,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"UwdxR-ZzAgS3"},"outputs":[],"source":["import pandas as pd\n","from collections import defaultdict\n","\n","def evaluate_topk(topk_file, train_file, test_file, k=1, threshold=0.0):\n","    \"\"\"\n","    Evaluate Top-K predictions using Precision, Recall, and F1-score,\n","    after filtering out training-only URIs, keeping only test sources, and applying 1-1 constraint.\n","\n","    Args:\n","        topk_file (str): Path to the top-k prediction file (TSV with SrcEntity, TgtEntity, Score)\n","        train_file (str): Path to the training mappings file (TSV)\n","        test_file (str): Path to the test mappings file (TSV)\n","        k (int): Value of K for top-k evaluation\n","        threshold (float): Minimum score to consider a prediction valid\n","\n","    Returns:\n","        dict: Dictionary containing Precision@K, Recall@K, and F1@K\n","    \"\"\"\n","\n","    # === Step 1: Load input files ===\n","    df = pd.read_csv(topk_file, sep='\\t', dtype=str)\n","    train_df = pd.read_csv(train_file, sep='\\t', dtype=str)\n","    test_df = pd.read_csv(test_file, sep='\\t', dtype=str)\n","\n","    # === Step 2: Remove URIs only present in the training set ===\n","    train_uris = set(train_df['SrcEntity']) | set(train_df['TgtEntity'])\n","    test_uris = set(test_df['SrcEntity']) | set(test_df['TgtEntity'])\n","    uris_to_exclude = train_uris - test_uris\n","    df = df[~(df['SrcEntity'].isin(uris_to_exclude) | df['TgtEntity'].isin(uris_to_exclude))].reset_index(drop=True)\n","\n","    # === Step 3: Keep only source entities from the test set ===\n","    src_entities_test = set(test_df['SrcEntity'])\n","    df = df[df['SrcEntity'].isin(src_entities_test)].reset_index(drop=True)\n","\n","    # === Step 4: Convert score column to float and sort ===\n","    df['Score'] = df['Score'].apply(lambda x: float(x.strip(\"[]\")) if isinstance(x, str) else float(x))\n","    df_sorted = df.sort_values(by='Score', ascending=False).reset_index(drop=True)\n","\n","    # === Step 5: Apply 1-to-1 constraint (greedy strategy with optional threshold)\n","    matched_sources = set()\n","    matched_targets = set()\n","    result = []\n","\n","    for _, row in df_sorted.iterrows():\n","        src, tgt, score = row['SrcEntity'], row['TgtEntity'], row['Score']\n","        if src not in matched_sources and tgt not in matched_targets and score >= threshold:\n","            result.append((src, tgt, score))\n","            matched_sources.add(src)\n","            matched_targets.add(tgt)\n","\n","    # === Step 6: Create and save Top-1 prediction dataframe\n","    matching_results_df = pd.DataFrame(result, columns=['SrcEntity', 'TgtEntity', 'Score'])\n","    output_file = topk_file.replace(\".tsv\", \"_predictions.tsv\")\n","    matching_results_df.to_csv(output_file, sep='\\t', index=False)\n","    print(f\"📁 Top-1 file saved: {output_file}\")\n","    print(f\"📌 Number of predictions in output: {len(matching_results_df)}\")\n","\n","    # === Step 7: Build reference dictionary from test set\n","    ref_dict = defaultdict(set)\n","    for _, row in test_df.iterrows():\n","        ref_dict[row['SrcEntity']].add(row['TgtEntity'])\n","\n","    # === Step 8: Select Top-K from the 1-1 results\n","    matching_results_df['Score'] = matching_results_df['Score'].astype(float)\n","    topk_df = matching_results_df.sort_values(by='Score', ascending=False).groupby('SrcEntity').head(k)\n","\n","    # === Step 9: Compute metrics\n","    total_tp = total_pred = total_ref = 0\n","\n","    for src, group in topk_df.groupby('SrcEntity'):\n","        predicted = set(group['TgtEntity'])\n","        true = ref_dict.get(src, set())\n","        tp = len(predicted & true)\n","        total_tp += tp\n","        total_pred += len(predicted)\n","        total_ref += len(true)\n","\n","    precision = total_tp / total_pred if total_pred else 0.0\n","    recall = total_tp / total_ref if total_ref else 0.0\n","    f1 = 2 * precision * recall / (precision + recall + 1e-8) if precision + recall > 0 else 0.0\n","\n","    return {\n","        f'Precision@{k}': round(precision, 4),\n","        f'Recall@{k}': round(recall, 4),\n","        f'F1@{k}': round(f1, 4)\n","    }\n"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1748119082283,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"WAsAVJEy3o9a"},"outputs":[],"source":["import pandas as pd\n","import torch\n","\n","def encode_embeddings_with_concept_column(encoder_model, input_file, output_file):\n","    \"\"\"\n","    Applies an encoder model to a set of embeddings (while preserving the 'Concept' column),\n","    and saves the encoded results in the same tabular format.\n","\n","    Args:\n","        encoder_model: A PyTorch model (e.g., LinearEncoder, MLPEncoder, etc.)\n","        input_file (str): Path to the input TSV file containing 'Concept' and embedding vectors.\n","        output_file (str): Path to save the encoded embeddings.\n","    \"\"\"\n","\n","    # Select device (GPU if available, else CPU)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Move the encoder model to the selected device and set it to evaluation mode\n","    encoder_model = encoder_model.to(device)\n","    encoder_model.eval()\n","\n","    # Load the input TSV file containing concept URIs and embeddings\n","    df = pd.read_csv(input_file, sep='\\t')\n","\n","    # Extract the 'Concept' column to preserve URIs\n","    concepts = df['Concept'].tolist()\n","\n","    # Extract the numerical embedding values (excluding the 'Concept' column)\n","    embedding_values = df.drop(columns=['Concept']).values\n","\n","    # Convert the embedding matrix into a PyTorch tensor and move to the device\n","    embeddings = torch.FloatTensor(embedding_values).to(device)\n","\n","    # Pass the embeddings through the encoder model without computing gradients\n","    with torch.no_grad():\n","        encoded = encoder_model(embeddings).cpu().numpy()\n","\n","    # Reconstruct a new DataFrame with the encoded vectors and corresponding URIs\n","    df_encoded = pd.DataFrame(encoded, columns=[f'dim_{i}' for i in range(encoded.shape[1])])\n","    df_encoded.insert(0, \"Concept\", concepts)  # Re-insert the 'Concept' column at the first position\n","\n","    # Save the encoded embeddings to a TSV file\n","    df_encoded.to_csv(output_file, sep='\\t', index=False)\n","    print(f\"✅ Encoded embeddings saved to: {output_file}\")\n"]},{"cell_type":"markdown","metadata":{"id":"1HyWMsw1MVhz"},"source":["# **Main Code**\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IC37FlwGDqGM"},"source":["\n","\n","# Reading semantic node embeddings provided by the ENE"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1748119082288,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"FuEfSnw5mod0"},"outputs":[],"source":["# Read the source embeddings from a CSV file into a pandas DataFrame\n","df_embbedings_src = pd.read_csv(src_Emb, index_col=0)\n","\n","# Convert the DataFrame to a NumPy array, which will remove the index and store the data as a raw matrix\n","numpy_array = df_embbedings_src.to_numpy()\n","\n","# Convert the NumPy array into a PyTorch FloatTensor, which is the format required for PyTorch operations\n","x_src = torch.FloatTensor(numpy_array)"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":111,"status":"ok","timestamp":1748119082401,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"STUwqMUXmlG2"},"outputs":[],"source":["# Read the target embeddings from a CSV file into a pandas DataFrame\n","df_embbedings_tgt = pd.read_csv(tgt_Emb, index_col=0)\n","\n","# Convert the DataFrame to a NumPy array, which removes the index and converts the data to a raw matrix\n","numpy_array = df_embbedings_tgt.to_numpy()\n","\n","# Convert the NumPy array into a PyTorch FloatTensor, which is required for PyTorch operations\n","x_tgt = torch.FloatTensor(numpy_array)"]},{"cell_type":"markdown","metadata":{"id":"jZIu9P08DqGN"},"source":["# Reading adjacency Matrix"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1748119082408,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"pH69Up40mycz"},"outputs":[],"source":["# Read the source adjacency matrix from a CSV file into a pandas DataFrame\n","df_ma1 = pd.read_csv(src_Adjacence, index_col=0)\n","\n","# Convert the DataFrame to a list of lists (Python native list format)\n","ma1 = df_ma1.values.tolist()"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1748119082419,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"hYCmAO5Ymzpl"},"outputs":[],"source":["# Read the target adjacency matrix from a CSV file into a pandas DataFrame\n","df_ma2 = pd.read_csv(tgt_Adjacence, index_col=0)\n","\n","# Convert the DataFrame to a list of lists (Python native list format)\n","ma2 = df_ma2.values.tolist()"]},{"cell_type":"markdown","metadata":{"id":"PSyksqh3TrU-"},"source":["# Convert Adjacency matrix (in list format) to an undirected edge index"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1748119082429,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"uVt-Pce5m5ll"},"outputs":[],"source":["# Convert the source adjacency matrix (in list format) to an undirected edge index for PyTorch Geometric\n","edge_src = adjacency_matrix_to_undirected_edge_index(ma1)\n","\n","# Convert the target adjacency matrix (in list format) to an undirected edge index for PyTorch Geometric\n","edge_tgt = adjacency_matrix_to_undirected_edge_index(ma2)"]},{"cell_type":"markdown","metadata":{"id":"H9wMTRdqT4aY"},"source":["# GIT Training"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1748119082458,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"eqiEKCLSMVh3"},"outputs":[],"source":["def train_model_gnn(model, x_src, edge_src, x_tgt, edge_tgt,\n","                    tensor_term1, tensor_term2, tensor_score,\n","                    learning_rate, weight_decay_value, num_epochs, print_interval=10):\n","    \"\"\"\n","    Trains a graph neural network (GNN) model using source and target embeddings and contrastive loss.\n","\n","    Args:\n","        model: The GNN model to be trained.\n","        x_src (torch.Tensor): Source node embeddings.\n","        edge_src (torch.Tensor): Source graph edges.\n","        x_tgt (torch.Tensor): Target node embeddings.\n","        edge_tgt (torch.Tensor): Target graph edges.\n","        tensor_term1 (torch.Tensor): Indices of the source nodes to be compared.\n","        tensor_term2 (torch.Tensor): Indices of the target nodes to be compared.\n","        tensor_score (torch.Tensor): Labels indicating if the pairs are matched (1) or not (0).\n","        learning_rate (float): Learning rate for the optimizer.\n","        weight_decay_value (float): Weight decay (L2 regularization) value for the optimizer.\n","        num_epochs (int): Number of epochs for training.\n","        print_interval (int): Interval at which training progress is printed (every `print_interval` epochs).\n","\n","    Returns:\n","        model: The trained GNN model.\n","    \"\"\"\n","\n","    # Step 1: Set device (GPU or CPU) for computation\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    # Step 2: Move the model and all inputs to the selected device\n","    model.to(device)\n","    x_tgt = x_tgt.to(device)               # Target node embeddings\n","    edge_tgt = edge_tgt.to(device)         # Target graph edges\n","    x_src = x_src.to(device)               # Source node embeddings\n","    edge_src = edge_src.to(device)         # Source graph edges\n","    tensor_term1 = tensor_term1.to(device) # Indices for source nodes\n","    tensor_term2 = tensor_term2.to(device) # Indices for target nodes\n","    tensor_score = tensor_score.to(device) # Ground truth labels\n","\n","    # Step 3: Define optimizer with learning rate and regularization\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay_value)\n","\n","    # Step 4: Initialize list to store training losses\n","    train_losses = []\n","\n","    # Record the start time of training\n","    start_time = time.time()\n","\n","    # Step 5: Training loop\n","    for epoch in range(num_epochs):\n","        # Zero out gradients from the previous iteration\n","        optimizer.zero_grad()\n","\n","        # Forward pass: Compute embeddings for source and target graphs\n","        out1 = model(x_src, edge_src)  # Updated source embeddings\n","        out2 = model(x_tgt, edge_tgt)  # Updated target embeddings\n","\n","        # Extract specific rows of embeddings for terms being compared\n","        src_embeddings = select_rows_by_index(out1, tensor_term1)\n","        tgt_embeddings = select_rows_by_index(out2, tensor_term2)\n","\n","        # Compute contrastive loss based on the embeddings and ground truth labels\n","        loss = contrastive_loss(src_embeddings, tgt_embeddings, tensor_score)\n","\n","        # Backward pass: Compute gradients\n","        loss.backward()\n","\n","        # Update the model's parameters\n","        optimizer.step()\n","\n","        # Append the loss for this iteration to the list\n","        train_losses.append(loss.item())\n","\n","        # Print loss every `print_interval` epochs\n","        if (epoch + 1) % print_interval == 0:\n","            print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item()}\")\n","\n","    # Step 6: Record end time of training\n","    end_time = time.time()\n","\n","    # Step 7: Plot the training loss over time\n","    plt.semilogy(range(1, num_epochs + 1), train_losses, label=\"Training Loss\", marker='o')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.show()\n","\n","    # Print the total training time\n","    training_time = end_time - start_time\n","    print(f\"Training complete! Total training time: {training_time:.2f} seconds\")\n","\n","    # Step 8: Return the trained model\n","    return model"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1748119082476,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"6_tzUG_emtBg"},"outputs":[],"source":["# Initialize the GIT_mod model with the dimensionality of the target embeddings\n","# The first argument is the dimensionality of the target node embeddings (x_tgt.shape[1])\n","# The second argument (1) represents the number of RGIT layers in the model\n","GIT_model = RGIT_mod(x_tgt.shape[1], 1)"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1748119082504,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"wVo-s7UQssSp"},"outputs":[],"source":["# Reading the training pairs from a CSV file into a pandas DataFrame\n","df_embbedings = pd.read_csv(train_file, index_col=0)\n","\n","# Extract the 'SrcEntity' and 'TgtEntity' columns as NumPy arrays and convert them to integers\n","tensor_term1 = df_embbedings['SrcEntity'].values.astype(int)  # Source entity indices\n","tensor_term2 = df_embbedings['TgtEntity'].values.astype(int)  # Target entity indices\n","\n","# Extract the 'Score' column as a NumPy array and convert it to floats\n","tensor_score = df_embbedings['Score'].values.astype(float)  # Scores (labels) indicating if pairs match (1) or not (0)\n","\n","# Convert the NumPy arrays to PyTorch LongTensors (for indices) and FloatTensors (for scores)\n","tensor_term1_o = torch.from_numpy(tensor_term1).type(torch.LongTensor)  # Source entity tensor\n","tensor_term2_o = torch.from_numpy(tensor_term2).type(torch.LongTensor)  # Target entity tensor\n","tensor_score_o = torch.from_numpy(tensor_score).type(torch.FloatTensor)  # Score tensor"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":282021,"status":"ok","timestamp":1748119364531,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"agHlFNesMVh3","outputId":"15a76d5e-ad02-4742-f6c7-896114334526"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [10/1000], Training Loss: 0.2257397472858429\n","Epoch [20/1000], Training Loss: 0.14234431087970734\n","Epoch [30/1000], Training Loss: 0.11174290627241135\n","Epoch [40/1000], Training Loss: 0.0777750089764595\n","Epoch [50/1000], Training Loss: 0.048687174916267395\n","Epoch [60/1000], Training Loss: 0.03185597062110901\n","Epoch [70/1000], Training Loss: 0.023379968479275703\n","Epoch [80/1000], Training Loss: 0.019264496862888336\n","Epoch [90/1000], Training Loss: 0.01714964210987091\n","Epoch [100/1000], Training Loss: 0.015747714787721634\n","Epoch [110/1000], Training Loss: 0.014585339464247227\n","Epoch [120/1000], Training Loss: 0.013516511768102646\n","Epoch [130/1000], Training Loss: 0.012552529573440552\n","Epoch [140/1000], Training Loss: 0.011709781363606453\n","Epoch [150/1000], Training Loss: 0.01099865511059761\n","Epoch [160/1000], Training Loss: 0.010416115634143353\n","Epoch [170/1000], Training Loss: 0.009927086532115936\n","Epoch [180/1000], Training Loss: 0.009505139663815498\n","Epoch [190/1000], Training Loss: 0.009132263250648975\n","Epoch [200/1000], Training Loss: 0.008795687928795815\n","Epoch [210/1000], Training Loss: 0.008485662750899792\n","Epoch [220/1000], Training Loss: 0.008196849375963211\n","Epoch [230/1000], Training Loss: 0.007931515574455261\n","Epoch [240/1000], Training Loss: 0.007689581252634525\n","Epoch [250/1000], Training Loss: 0.00746864452958107\n","Epoch [260/1000], Training Loss: 0.007267489098012447\n","Epoch [270/1000], Training Loss: 0.007084832061082125\n","Epoch [280/1000], Training Loss: 0.006919448263943195\n","Epoch [290/1000], Training Loss: 0.006768995430320501\n","Epoch [300/1000], Training Loss: 0.006630806252360344\n","Epoch [310/1000], Training Loss: 0.006503090728074312\n","Epoch [320/1000], Training Loss: 0.006384924985468388\n","Epoch [330/1000], Training Loss: 0.006275344640016556\n","Epoch [340/1000], Training Loss: 0.006173123139888048\n","Epoch [350/1000], Training Loss: 0.00607746047899127\n","Epoch [360/1000], Training Loss: 0.005987442098557949\n","Epoch [370/1000], Training Loss: 0.005902096629142761\n","Epoch [380/1000], Training Loss: 0.0058205705136060715\n","Epoch [390/1000], Training Loss: 0.005742268171161413\n","Epoch [400/1000], Training Loss: 0.00566644873470068\n","Epoch [410/1000], Training Loss: 0.005592459347099066\n","Epoch [420/1000], Training Loss: 0.005520242732018232\n","Epoch [430/1000], Training Loss: 0.005449360702186823\n","Epoch [440/1000], Training Loss: 0.005379498470574617\n","Epoch [450/1000], Training Loss: 0.005310456734150648\n","Epoch [460/1000], Training Loss: 0.0052420832216739655\n","Epoch [470/1000], Training Loss: 0.005174421705305576\n","Epoch [480/1000], Training Loss: 0.005107327364385128\n","Epoch [490/1000], Training Loss: 0.005040816962718964\n","Epoch [500/1000], Training Loss: 0.004974799696356058\n","Epoch [510/1000], Training Loss: 0.004909205716103315\n","Epoch [520/1000], Training Loss: 0.004844231065362692\n","Epoch [530/1000], Training Loss: 0.004779809154570103\n","Epoch [540/1000], Training Loss: 0.0047159334644675255\n","Epoch [550/1000], Training Loss: 0.004652759525924921\n","Epoch [560/1000], Training Loss: 0.00459041865542531\n","Epoch [570/1000], Training Loss: 0.0045291101559996605\n","Epoch [580/1000], Training Loss: 0.0044684330932796\n","Epoch [590/1000], Training Loss: 0.004408310167491436\n","Epoch [600/1000], Training Loss: 0.004348787944763899\n","Epoch [610/1000], Training Loss: 0.0042898026295006275\n","Epoch [620/1000], Training Loss: 0.004231253173202276\n","Epoch [630/1000], Training Loss: 0.004173262510448694\n","Epoch [640/1000], Training Loss: 0.004115913528949022\n","Epoch [650/1000], Training Loss: 0.004059178754687309\n","Epoch [660/1000], Training Loss: 0.004003017675131559\n","Epoch [670/1000], Training Loss: 0.003947381861507893\n","Epoch [680/1000], Training Loss: 0.0038923199754208326\n","Epoch [690/1000], Training Loss: 0.003837882773950696\n","Epoch [700/1000], Training Loss: 0.0037840474396944046\n","Epoch [710/1000], Training Loss: 0.0037308877799659967\n","Epoch [720/1000], Training Loss: 0.00367837049998343\n","Epoch [730/1000], Training Loss: 0.003626414341852069\n","Epoch [740/1000], Training Loss: 0.0035750586539506912\n","Epoch [750/1000], Training Loss: 0.0035243891179561615\n","Epoch [760/1000], Training Loss: 0.0034743768628686666\n","Epoch [770/1000], Training Loss: 0.0034250523895025253\n","Epoch [780/1000], Training Loss: 0.003376409877091646\n","Epoch [790/1000], Training Loss: 0.003328516613692045\n","Epoch [800/1000], Training Loss: 0.0032813171856105328\n","Epoch [810/1000], Training Loss: 0.003234947333112359\n","Epoch [820/1000], Training Loss: 0.0031894196290522814\n","Epoch [830/1000], Training Loss: 0.0031447140499949455\n","Epoch [840/1000], Training Loss: 0.0031008627265691757\n","Epoch [850/1000], Training Loss: 0.003057947149500251\n","Epoch [860/1000], Training Loss: 0.0030161049216985703\n","Epoch [870/1000], Training Loss: 0.002975267358124256\n","Epoch [880/1000], Training Loss: 0.002935512689873576\n","Epoch [890/1000], Training Loss: 0.0028969082050025463\n","Epoch [900/1000], Training Loss: 0.0028595083858817816\n","Epoch [910/1000], Training Loss: 0.002823202172294259\n","Epoch [920/1000], Training Loss: 0.0027880012057721615\n","Epoch [930/1000], Training Loss: 0.0027539667207747698\n","Epoch [940/1000], Training Loss: 0.0027210800908505917\n","Epoch [950/1000], Training Loss: 0.0026892467867583036\n","Epoch [960/1000], Training Loss: 0.002658379264175892\n","Epoch [970/1000], Training Loss: 0.002628521528095007\n","Epoch [980/1000], Training Loss: 0.002599661238491535\n","Epoch [990/1000], Training Loss: 0.002571740886196494\n","Epoch [1000/1000], Training Loss: 0.0025446799118071795\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Training complete! Total training time: 285.49 seconds\n"]}],"source":["# Train the GNN model using the provided source and target graph embeddings, edges, and training data\n","trained_model = train_model_gnn(\n","    model=GIT_model,                # The GNN model to be trained (initialized earlier)\n","    x_src=x_src,                    # Source node embeddings (tensor for source graph)\n","    edge_src=edge_src,              # Source graph edges (undirected edge index for source graph)\n","    x_tgt=x_tgt,                    # Target node embeddings (tensor for target graph)\n","    edge_tgt=edge_tgt,              # Target graph edges (undirected edge index for target graph)\n","    tensor_term1=tensor_term1_o,    # Indices of source entities for training\n","    tensor_term2=tensor_term2_o,    # Indices of target entities for training\n","    tensor_score=tensor_score_o,    # Scores (labels) indicating if pairs match (1) or not (0)\n","    learning_rate=0.0001,            # Learning rate for the Adam optimizer\n","    weight_decay_value=1e-4,        # Weight decay for L2 regularization to prevent overfitting\n","    num_epochs=1000,                # Number of training epochs\n","    print_interval=10               # Interval at which to print training progress (every 10 epochs)\n",")"]},{"cell_type":"markdown","metadata":{"id":"MYyhvjcTUaae"},"source":["# GIT Application"]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1748119364539,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"BZ_VqP6tq6iD"},"outputs":[],"source":["# Determine if a GPU is available and move the computations to it; otherwise, use the CPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Assuming the model has been trained and hyperparameters (x_src, edge_src, x_tgt, edge_tgt) are set\n","\n","# Move the trained GIT_model to the device (GPU or CPU)\n","GIT_model.to(device)\n","\n","# Move the data tensors to the same device (GPU or CPU)\n","x_tgt = x_tgt.to(device)         # Target node embeddings\n","edge_tgt = edge_tgt.to(device)   # Target graph edges\n","x_src = x_src.to(device)         # Source node embeddings\n","edge_src = edge_src.to(device)   # Source graph edges\n","\n","# Set the model to evaluation mode; this disables dropout and batch normalization\n","GIT_model.eval()\n","\n","# Pass the source and target embeddings through the trained GNN model to update the embeddings\n","with torch.no_grad():  # Disable gradient computation (inference mode)\n","    embeddings_tgt = GIT_model(x_tgt, edge_tgt)  # Get updated embeddings for the target graph\n","    embeddings_src = GIT_model(x_src, edge_src)  # Get updated embeddings for the source graph\n","\n","# Detach the embeddings from the computation graph and move them back to the CPU\n","# This step is useful if you need to use the embeddings for tasks outside PyTorch (e.g., saving to disk)\n","embeddings_tgt = embeddings_tgt.detach().cpu()  # Target graph embeddings\n","embeddings_src = embeddings_src.detach().cpu()  # Source graph embeddings\n","\n","# At this point, embeddings_tgt and embeddings_src contain the updated embeddings, ready for downstream tasks"]},{"cell_type":"markdown","metadata":{"id":"Og5fdoGJrTCG"},"source":["# Selecting embedding pairs to train the Gated Network"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":278,"status":"ok","timestamp":1748119364820,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"J0nTwc-dnjLn"},"outputs":[],"source":["# Read the training pairs from a CSV file into a pandas DataFrame\n","df_embeddings = pd.read_csv(train_file, index_col=0)\n","\n","# Extract columns and convert to NumPy arrays\n","tensor_term1 = df_embeddings['SrcEntity'].values.astype(int)  # Source entity indices\n","tensor_term2 = df_embeddings['TgtEntity'].values.astype(int)  # Target entity indices\n","tensor_score = df_embeddings['Score'].values.astype(float)  # Matching scores\n","\n","# Split data into training and validation sets\n","tensor_term1_train, tensor_term1_val, tensor_term2_train, tensor_term2_val, tensor_score_train, tensor_score_val = train_test_split(\n","    tensor_term1, tensor_term2, tensor_score, test_size=0.3, random_state=42\n",")\n","\n","# Convert split data to PyTorch tensors\n","tensor_term1_train = torch.from_numpy(tensor_term1_train).type(torch.LongTensor)\n","tensor_term2_train = torch.from_numpy(tensor_term2_train).type(torch.LongTensor)\n","tensor_score_train = torch.from_numpy(tensor_score_train).type(torch.FloatTensor)\n","\n","tensor_term1_val = torch.from_numpy(tensor_term1_val).type(torch.LongTensor)\n","tensor_term2_val = torch.from_numpy(tensor_term2_val).type(torch.LongTensor)\n","tensor_score_val = torch.from_numpy(tensor_score_val).type(torch.FloatTensor)\n","\n","# Move the embeddings back to the CPU if not already there\n","x_tgt = x_tgt.cpu()  # Target node embeddings\n","x_src = x_src.cpu()  # Source node embeddings\n","\n","# Select embeddings for the training set\n","X1_train = select_rows_by_index(embeddings_src, tensor_term1_train)\n","X2_train = select_rows_by_index(x_src, tensor_term1_train)\n","X3_train = select_rows_by_index(embeddings_tgt, tensor_term2_train)\n","X4_train = select_rows_by_index(x_tgt, tensor_term2_train)\n","\n","# Select embeddings for the validation set\n","X1_val = select_rows_by_index(embeddings_src, tensor_term1_val)\n","X2_val = select_rows_by_index(x_src, tensor_term1_val)\n","X3_val = select_rows_by_index(embeddings_tgt, tensor_term2_val)\n","X4_val = select_rows_by_index(x_tgt, tensor_term2_val)\n","\n","# Now you have:\n","# - Training tensors: X1_train, X2_train, X3_train, X4_train, tensor_score_train\n","# - Validation tensors: X1_val, X2_val, X3_val, X4_val, tensor_score_val"]},{"cell_type":"markdown","metadata":{"id":"fNdCgaTMExPK"},"source":["# Gated Network Training"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1748119365156,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"},"user_tz":-60},"id":"Gof1eIPIWSVU"},"outputs":[],"source":["from sklearn.metrics import f1_score\n","\n","def train_gated_combination_model(X1_t, X2_t, X3_t, X4_t, tensor_score_o,\n","                                  X1_val, X2_val, X3_val, X4_val, tensor_score_val,\n","                                  epochs=120, batch_size=32, learning_rate=0.001, weight_decay=1e-5):\n","    \"\"\"\n","    Trains the GatedCombination model with training and validation data, using ReduceLROnPlateau scheduler.\n","    Also calculates and displays F1-score during training and validation.\n","    \"\"\"\n","\n","    # Create datasets and DataLoaders\n","    train_dataset = TensorDataset(X1_t, X2_t, X3_t, X4_t, tensor_score_o)\n","    val_dataset = TensorDataset(X1_val, X2_val, X3_val, X4_val, tensor_score_val)\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model = GatedCombinationWithFaiss(X1_t.shape[1]).to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","\n","    # Use ReduceLROnPlateau scheduler\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n","\n","    train_losses, val_losses = [], []\n","\n","    start_time = time.time()\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        total_train_loss = 0.0\n","        y_true_train, y_pred_train = [], []\n","\n","        for batch_X1, batch_X2, batch_X3, batch_X4, batch_y in train_loader:\n","            batch_X1, batch_X2, batch_X3, batch_X4, batch_y = (\n","                batch_X1.to(device),\n","                batch_X2.to(device),\n","                batch_X3.to(device),\n","                batch_X4.to(device),\n","                batch_y.to(device),\n","            )\n","            optimizer.zero_grad()\n","\n","            # Forward pass\n","            outputs = model(batch_X1, batch_X2, batch_X3, batch_X4)\n","\n","            # Compute loss\n","            loss = F.binary_cross_entropy(outputs, batch_y.unsqueeze(1).float())\n","            loss.backward()\n","            optimizer.step()\n","            total_train_loss += loss.item()\n","\n","            # Store true labels and predictions for F1-score\n","            y_true_train.extend(batch_y.cpu().numpy())\n","            y_pred_train.extend((outputs > 0.5).float().cpu().numpy())\n","\n","        train_loss = total_train_loss / len(train_loader)\n","        train_losses.append(train_loss)\n","\n","        # Calculate F1-score for training\n","        train_f1 = f1_score(y_true_train, y_pred_train)\n","\n","        # Validation phase\n","        model.eval()\n","        total_val_loss = 0.0\n","        y_true_val, y_pred_val = [], []\n","\n","        with torch.no_grad():\n","            for batch_X1, batch_X2, batch_X3, batch_X4, batch_y in val_loader:\n","                batch_X1, batch_X2, batch_X3, batch_X4, batch_y = (\n","                    batch_X1.to(device),\n","                    batch_X2.to(device),\n","                    batch_X3.to(device),\n","                    batch_X4.to(device),\n","                    batch_y.to(device),\n","                )\n","                outputs = model(batch_X1, batch_X2, batch_X3, batch_X4)\n","\n","                # Compute loss\n","                val_loss = F.binary_cross_entropy(outputs, batch_y.unsqueeze(1).float())\n","                total_val_loss += val_loss.item()\n","\n","                # Store true labels and predictions for F1-score\n","                y_true_val.extend(batch_y.cpu().numpy())\n","                y_pred_val.extend((outputs > 0.5).float().cpu().numpy())\n","\n","        avg_val_loss = total_val_loss / len(val_loader)\n","        val_losses.append(avg_val_loss)\n","\n","        # Calculate F1-score for validation\n","        val_f1 = f1_score(y_true_val, y_pred_val)\n","\n","        # Step the scheduler with validation loss\n","        scheduler.step(avg_val_loss)\n","\n","        # Print training and validation metrics\n","        print(f\"Epoch [{epoch + 1}/{epochs}] Training Loss: {train_loss:.4f}, F1 Score: {train_f1:.4f} | \"\n","              f\"Validation Loss: {avg_val_loss:.4f}, F1 Score: {val_f1:.4f}\")\n","\n","    end_time = time.time()\n","\n","    # Plotting training and validation loss\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(train_losses, label=\"Training Loss\", marker='o')\n","    plt.plot(val_losses, label=\"Validation Loss\", marker='x')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.show()\n","\n","    print(f\"Training complete! Total time: {end_time - start_time:.2f} seconds\")\n","    return model\n","\n"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"QrpFp6aDbtSW","executionInfo":{"status":"ok","timestamp":1748119847139,"user_tz":-60,"elapsed":481980,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"}},"outputId":"b6ed57cb-8e53-49c0-d272-a6f843efa504"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/100] Training Loss: 0.4260, F1 Score: 0.1013 | Validation Loss: 0.2123, F1 Score: 0.0000\n","Epoch [2/100] Training Loss: 0.1459, F1 Score: 0.0000 | Validation Loss: 0.1059, F1 Score: 0.0000\n","Epoch [3/100] Training Loss: 0.0890, F1 Score: 0.0000 | Validation Loss: 0.0797, F1 Score: 0.0000\n","Epoch [4/100] Training Loss: 0.0741, F1 Score: 0.0000 | Validation Loss: 0.0724, F1 Score: 0.0000\n","Epoch [5/100] Training Loss: 0.0693, F1 Score: 0.0000 | Validation Loss: 0.0693, F1 Score: 0.0000\n","Epoch [6/100] Training Loss: 0.0661, F1 Score: 0.0000 | Validation Loss: 0.0660, F1 Score: 0.0000\n","Epoch [7/100] Training Loss: 0.0627, F1 Score: 0.0000 | Validation Loss: 0.0626, F1 Score: 0.0000\n","Epoch [8/100] Training Loss: 0.0594, F1 Score: 0.0000 | Validation Loss: 0.0594, F1 Score: 0.0000\n","Epoch [9/100] Training Loss: 0.0564, F1 Score: 0.0000 | Validation Loss: 0.0566, F1 Score: 0.0000\n","Epoch [10/100] Training Loss: 0.0538, F1 Score: 0.0000 | Validation Loss: 0.0542, F1 Score: 0.0000\n","Epoch [11/100] Training Loss: 0.0516, F1 Score: 0.0000 | Validation Loss: 0.0521, F1 Score: 0.0000\n","Epoch [12/100] Training Loss: 0.0493, F1 Score: 0.0000 | Validation Loss: 0.0503, F1 Score: 0.0000\n","Epoch [13/100] Training Loss: 0.0478, F1 Score: 0.0000 | Validation Loss: 0.0487, F1 Score: 0.0000\n","Epoch [14/100] Training Loss: 0.0466, F1 Score: 0.0102 | Validation Loss: 0.0475, F1 Score: 0.0454\n","Epoch [15/100] Training Loss: 0.0452, F1 Score: 0.1368 | Validation Loss: 0.0465, F1 Score: 0.2163\n","Epoch [16/100] Training Loss: 0.0444, F1 Score: 0.2480 | Validation Loss: 0.0456, F1 Score: 0.2695\n","Epoch [17/100] Training Loss: 0.0440, F1 Score: 0.3202 | Validation Loss: 0.0450, F1 Score: 0.3227\n","Epoch [18/100] Training Loss: 0.0429, F1 Score: 0.3642 | Validation Loss: 0.0443, F1 Score: 0.3682\n","Epoch [19/100] Training Loss: 0.0425, F1 Score: 0.4074 | Validation Loss: 0.0438, F1 Score: 0.3908\n","Epoch [20/100] Training Loss: 0.0421, F1 Score: 0.4208 | Validation Loss: 0.0434, F1 Score: 0.4146\n","Epoch [21/100] Training Loss: 0.0418, F1 Score: 0.4456 | Validation Loss: 0.0431, F1 Score: 0.4146\n","Epoch [22/100] Training Loss: 0.0411, F1 Score: 0.4582 | Validation Loss: 0.0427, F1 Score: 0.4274\n","Epoch [23/100] Training Loss: 0.0412, F1 Score: 0.4783 | Validation Loss: 0.0425, F1 Score: 0.4274\n","Epoch [24/100] Training Loss: 0.0411, F1 Score: 0.4748 | Validation Loss: 0.0423, F1 Score: 0.4504\n","Epoch [25/100] Training Loss: 0.0411, F1 Score: 0.4841 | Validation Loss: 0.0421, F1 Score: 0.4600\n","Epoch [26/100] Training Loss: 0.0401, F1 Score: 0.4915 | Validation Loss: 0.0419, F1 Score: 0.4845\n","Epoch [27/100] Training Loss: 0.0403, F1 Score: 0.4996 | Validation Loss: 0.0418, F1 Score: 0.4984\n","Epoch [28/100] Training Loss: 0.0402, F1 Score: 0.5045 | Validation Loss: 0.0417, F1 Score: 0.5008\n","Epoch [29/100] Training Loss: 0.0400, F1 Score: 0.5139 | Validation Loss: 0.0415, F1 Score: 0.5080\n","Epoch [30/100] Training Loss: 0.0403, F1 Score: 0.5100 | Validation Loss: 0.0415, F1 Score: 0.5008\n","Epoch [31/100] Training Loss: 0.0403, F1 Score: 0.5027 | Validation Loss: 0.0414, F1 Score: 0.5080\n","Epoch [32/100] Training Loss: 0.0400, F1 Score: 0.5176 | Validation Loss: 0.0413, F1 Score: 0.5072\n","Epoch [33/100] Training Loss: 0.0396, F1 Score: 0.5246 | Validation Loss: 0.0413, F1 Score: 0.5096\n","Epoch [34/100] Training Loss: 0.0396, F1 Score: 0.5221 | Validation Loss: 0.0412, F1 Score: 0.5183\n","Epoch [35/100] Training Loss: 0.0400, F1 Score: 0.5238 | Validation Loss: 0.0412, F1 Score: 0.5096\n","Epoch [36/100] Training Loss: 0.0398, F1 Score: 0.5194 | Validation Loss: 0.0411, F1 Score: 0.5096\n","Epoch [37/100] Training Loss: 0.0399, F1 Score: 0.5235 | Validation Loss: 0.0411, F1 Score: 0.5096\n","Epoch [38/100] Training Loss: 0.0396, F1 Score: 0.5206 | Validation Loss: 0.0411, F1 Score: 0.5253\n","Epoch [39/100] Training Loss: 0.0397, F1 Score: 0.5266 | Validation Loss: 0.0410, F1 Score: 0.5253\n","Epoch [40/100] Training Loss: 0.0392, F1 Score: 0.5311 | Validation Loss: 0.0410, F1 Score: 0.5245\n","Epoch [41/100] Training Loss: 0.0400, F1 Score: 0.5202 | Validation Loss: 0.0410, F1 Score: 0.5245\n","Epoch [42/100] Training Loss: 0.0398, F1 Score: 0.5256 | Validation Loss: 0.0410, F1 Score: 0.5253\n","Epoch [43/100] Training Loss: 0.0395, F1 Score: 0.5300 | Validation Loss: 0.0410, F1 Score: 0.5291\n","Epoch [44/100] Training Loss: 0.0392, F1 Score: 0.5374 | Validation Loss: 0.0409, F1 Score: 0.5338\n","Epoch [45/100] Training Loss: 0.0393, F1 Score: 0.5405 | Validation Loss: 0.0409, F1 Score: 0.5245\n","Epoch [46/100] Training Loss: 0.0394, F1 Score: 0.5339 | Validation Loss: 0.0409, F1 Score: 0.5268\n","Epoch [47/100] Training Loss: 0.0393, F1 Score: 0.5390 | Validation Loss: 0.0409, F1 Score: 0.5291\n","Epoch [48/100] Training Loss: 0.0393, F1 Score: 0.5372 | Validation Loss: 0.0409, F1 Score: 0.5338\n","Epoch [49/100] Training Loss: 0.0396, F1 Score: 0.5325 | Validation Loss: 0.0409, F1 Score: 0.5268\n","Epoch [50/100] Training Loss: 0.0390, F1 Score: 0.5442 | Validation Loss: 0.0408, F1 Score: 0.5338\n","Epoch [51/100] Training Loss: 0.0395, F1 Score: 0.5389 | Validation Loss: 0.0408, F1 Score: 0.5245\n","Epoch [52/100] Training Loss: 0.0394, F1 Score: 0.5355 | Validation Loss: 0.0408, F1 Score: 0.5314\n","Epoch [53/100] Training Loss: 0.0396, F1 Score: 0.5311 | Validation Loss: 0.0408, F1 Score: 0.5245\n","Epoch [54/100] Training Loss: 0.0390, F1 Score: 0.5402 | Validation Loss: 0.0408, F1 Score: 0.5291\n","Epoch [55/100] Training Loss: 0.0395, F1 Score: 0.5348 | Validation Loss: 0.0408, F1 Score: 0.5245\n","Epoch [56/100] Training Loss: 0.0393, F1 Score: 0.5362 | Validation Loss: 0.0408, F1 Score: 0.5361\n","Epoch [57/100] Training Loss: 0.0395, F1 Score: 0.5342 | Validation Loss: 0.0408, F1 Score: 0.5361\n","Epoch [58/100] Training Loss: 0.0397, F1 Score: 0.5286 | Validation Loss: 0.0408, F1 Score: 0.5338\n","Epoch [59/100] Training Loss: 0.0392, F1 Score: 0.5399 | Validation Loss: 0.0408, F1 Score: 0.5338\n","Epoch [60/100] Training Loss: 0.0393, F1 Score: 0.5352 | Validation Loss: 0.0408, F1 Score: 0.5338\n","Epoch [61/100] Training Loss: 0.0396, F1 Score: 0.5318 | Validation Loss: 0.0408, F1 Score: 0.5338\n","Epoch [62/100] Training Loss: 0.0400, F1 Score: 0.5321 | Validation Loss: 0.0409, F1 Score: 0.5245\n","Epoch [63/100] Training Loss: 0.0396, F1 Score: 0.5341 | Validation Loss: 0.0409, F1 Score: 0.5245\n","Epoch [64/100] Training Loss: 0.0393, F1 Score: 0.5353 | Validation Loss: 0.0409, F1 Score: 0.5338\n","Epoch [65/100] Training Loss: 0.0393, F1 Score: 0.5401 | Validation Loss: 0.0408, F1 Score: 0.5338\n","Epoch [66/100] Training Loss: 0.0395, F1 Score: 0.5305 | Validation Loss: 0.0408, F1 Score: 0.5361\n","Epoch [67/100] Training Loss: 0.0394, F1 Score: 0.5317 | Validation Loss: 0.0408, F1 Score: 0.5338\n","Epoch [68/100] Training Loss: 0.0392, F1 Score: 0.5365 | Validation Loss: 0.0408, F1 Score: 0.5338\n","Epoch [69/100] Training Loss: 0.0393, F1 Score: 0.5407 | Validation Loss: 0.0408, F1 Score: 0.5338\n","Epoch [70/100] Training Loss: 0.0394, F1 Score: 0.5393 | Validation Loss: 0.0408, F1 Score: 0.5338\n","Epoch [71/100] Training Loss: 0.0391, F1 Score: 0.5390 | Validation Loss: 0.0408, F1 Score: 0.5338\n","Epoch [72/100] Training Loss: 0.0396, F1 Score: 0.5365 | Validation Loss: 0.0408, F1 Score: 0.5338\n","Epoch [73/100] Training Loss: 0.0394, F1 Score: 0.5408 | Validation Loss: 0.0408, F1 Score: 0.5338\n","Epoch [74/100] Training Loss: 0.0391, F1 Score: 0.5392 | Validation Loss: 0.0408, F1 Score: 0.5338\n","Epoch [75/100] Training Loss: 0.0395, F1 Score: 0.5341 | Validation Loss: 0.0408, F1 Score: 0.5338\n","Epoch [76/100] Training Loss: 0.0395, F1 Score: 0.5326 | Validation Loss: 0.0408, F1 Score: 0.5338\n","Epoch [77/100] Training Loss: 0.0397, F1 Score: 0.5335 | Validation Loss: 0.0408, F1 Score: 0.5338\n","Epoch [78/100] Training Loss: 0.0399, F1 Score: 0.5351 | Validation Loss: 0.0408, F1 Score: 0.5314\n","Epoch [79/100] Training Loss: 0.0389, F1 Score: 0.5432 | Validation Loss: 0.0408, F1 Score: 0.5314\n","Epoch [80/100] Training Loss: 0.0390, F1 Score: 0.5399 | Validation Loss: 0.0408, F1 Score: 0.5314\n","Epoch [81/100] Training Loss: 0.0393, F1 Score: 0.5381 | Validation Loss: 0.0408, F1 Score: 0.5314\n","Epoch [82/100] Training Loss: 0.0390, F1 Score: 0.5399 | Validation Loss: 0.0408, F1 Score: 0.5314\n","Epoch [83/100] Training Loss: 0.0395, F1 Score: 0.5367 | Validation Loss: 0.0408, F1 Score: 0.5314\n","Epoch [84/100] Training Loss: 0.0399, F1 Score: 0.5274 | Validation Loss: 0.0408, F1 Score: 0.5314\n","Epoch [85/100] Training Loss: 0.0395, F1 Score: 0.5367 | Validation Loss: 0.0408, F1 Score: 0.5314\n","Epoch [86/100] Training Loss: 0.0390, F1 Score: 0.5407 | Validation Loss: 0.0408, F1 Score: 0.5314\n","Epoch [87/100] Training Loss: 0.0391, F1 Score: 0.5414 | Validation Loss: 0.0408, F1 Score: 0.5314\n","Epoch [88/100] Training Loss: 0.0392, F1 Score: 0.5369 | Validation Loss: 0.0408, F1 Score: 0.5314\n","Epoch [89/100] Training Loss: 0.0389, F1 Score: 0.5421 | Validation Loss: 0.0408, F1 Score: 0.5314\n","Epoch [90/100] Training Loss: 0.0390, F1 Score: 0.5403 | Validation Loss: 0.0408, F1 Score: 0.5314\n","Epoch [91/100] Training Loss: 0.0390, F1 Score: 0.5424 | Validation Loss: 0.0408, F1 Score: 0.5314\n","Epoch [92/100] Training Loss: 0.0390, F1 Score: 0.5418 | Validation Loss: 0.0408, F1 Score: 0.5314\n","Epoch [93/100] Training Loss: 0.0397, F1 Score: 0.5367 | Validation Loss: 0.0408, F1 Score: 0.5314\n","Epoch [94/100] Training Loss: 0.0392, F1 Score: 0.5432 | Validation Loss: 0.0408, F1 Score: 0.5314\n","Epoch [95/100] Training Loss: 0.0394, F1 Score: 0.5410 | Validation Loss: 0.0408, F1 Score: 0.5314\n","Epoch [96/100] Training Loss: 0.0398, F1 Score: 0.5349 | Validation Loss: 0.0408, F1 Score: 0.5314\n","Epoch [97/100] Training Loss: 0.0397, F1 Score: 0.5403 | Validation Loss: 0.0408, F1 Score: 0.5314\n","Epoch [98/100] Training Loss: 0.0391, F1 Score: 0.5420 | Validation Loss: 0.0408, F1 Score: 0.5314\n","Epoch [99/100] Training Loss: 0.0394, F1 Score: 0.5367 | Validation Loss: 0.0408, F1 Score: 0.5314\n","Epoch [100/100] Training Loss: 0.0392, F1 Score: 0.5436 | Validation Loss: 0.0408, F1 Score: 0.5314\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x500 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Training complete! Total time: 481.75 seconds\n"]}],"source":["# Train the GatedCombination model using training and validation data\n","trained_model = train_gated_combination_model(\n","    X1_train,          # Updated source embeddings (after applying the GNN model)\n","    X2_train,          # Original source embeddings (before applying the GNN model)\n","    X3_train,          # Updated target embeddings (after applying the GNN model)\n","    X4_train,          # Original target embeddings (before applying the GNN model)\n","    tensor_score_train, # Ground truth labels for the training set (1 for matched pairs, 0 for unmatched pairs)\n","\n","    X1_val,            # Updated source embeddings for the validation set\n","    X2_val,            # Original source embeddings for the validation set\n","    X3_val,            # Updated target embeddings for the validation set\n","    X4_val,            # Original target embeddings for the validation set\n","    tensor_score_val,  # Ground truth labels for the validation set (1 for matched pairs, 0 for unmatched pairs)\n","\n","    epochs=100,        # Number of epochs (iterations over the entire training dataset)\n","    batch_size=32,     # Number of training samples processed in one forward/backward pass\n","    learning_rate=0.001, # Learning rate for the optimizer (controls step size during optimization)\n","    weight_decay=1e-4 # Weight decay (L2 regularization) to prevent overfitting\n",")"]},{"cell_type":"markdown","metadata":{"id":"uAmLuMtGW9c2"},"source":["# **Second Round Modifications**"]},{"cell_type":"markdown","metadata":{"id":"IVDqN5AxFu9m"},"source":["# **Generate Embeddings**"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"ITZZcElm8qRN","executionInfo":{"status":"ok","timestamp":1748119847690,"user_tz":-60,"elapsed":548,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"}}},"outputs":[],"source":["# Build an indexed dictionary for the source ontology classes\n","# src_class is the file path to the JSON file containing the source ontology classes\n","indexed_dict_src = build_indexed_dict(src_class)\n","\n","# Build an indexed dictionary for the target ontology classes\n","# tgt_class is the file path to the JSON file containing the target ontology classes\n","indexed_dict_tgt = build_indexed_dict(tgt_class)"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"GbSbmPlRDOs3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748119861719,"user_tz":-60,"elapsed":14019,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"}},"outputId":"4f6c5467-4598-46a5-dc3a-ec873781b477"},"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Gated embeddings saved:\n","- Source: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Data/ncit_final_embeddings_All_Mini.tsv\n","- Target: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Data/doid_final_embeddings_All_Mini.tsv\n","⏱️ Execution time: 13.97 seconds\n"]}],"source":["# Define output file paths for final embeddings of source and target ontologies\n","output_file_src = f\"{data_dir}/{src_ent}_final_embeddings_All_Mini.tsv\"\n","output_file_tgt = f\"{data_dir}/{tgt_ent}_final_embeddings_All_Mini.tsv\"\n","\n","# Save the final gated embeddings for all concepts in source and target ontologies\n","save_gated_embeddings(\n","    gated_model=trained_model,          # The trained GatedCombination model\n","    embeddings_src=embeddings_src,      # GNN-transformed embeddings for source entities\n","    x_src=x_src,                        # Initial semantic embeddings for source entities\n","    embeddings_tgt=embeddings_tgt,      # GNN-transformed embeddings for target entities\n","    x_tgt=x_tgt,                        # Initial semantic embeddings for target entities\n","    indexed_dict_src=indexed_dict_src,  # Index-to-URI mapping for source ontology\n","    indexed_dict_tgt=indexed_dict_tgt,  # Index-to-URI mapping for target ontology\n","    output_file_src=output_file_src,    # Destination file path for source embeddings\n","    output_file_tgt=output_file_tgt     # Destination file path for target embeddings\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"BIDvbZj2GIGo"},"source":["# **Filter No Used Concepts**\n","\n","\n"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"6Gl_wUG9KADo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748119872134,"user_tz":-60,"elapsed":10423,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"}},"outputId":"50d40a94-e562-4196-d65d-18bb8f097b9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["🔍 Initial source file: 15992 rows\n","🔍 Initial target file: 8480 rows\n","✅ Source after removing ignored classes: 7065 rows\n","✅ Target after removing ignored classes: 8463 rows\n","📁 Cleaned source file saved to: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Data/ncit_final_embeddings_All_Mini_cleaned.tsv\n","📁 Cleaned target file saved to: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Data/doid_final_embeddings_All_Mini_cleaned.tsv\n"]}],"source":["# Call the function to filter out ignored concepts (e.g., owl:Thing, deprecated, etc.)\n","# from the source and target ontology embeddings.\n","\n","# Input:\n","# - src_emb_path: Path to the TSV file containing embeddings for the source ontology\n","# - tgt_emb_path: Path to the TSV file containing embeddings for the target ontology\n","# - src_onto / tgt_onto: DeepOnto ontology objects used to identify ignored concepts\n","\n","# Output:\n","# - src_file: Path to the cleaned source embeddings (with ignored concepts removed)\n","# - tgt_file: Path to the cleaned target embeddings (with ignored concepts removed)\n","\n","src_file, tgt_file = filter_ignored_class(\n","    src_emb_path=f\"{data_dir}/{src_ent}_final_embeddings_All_Mini.tsv\",\n","    tgt_emb_path=f\"{data_dir}/{tgt_ent}_final_embeddings_All_Mini.tsv\",\n","    src_onto=src_onto,\n","    tgt_onto=tgt_onto\n","\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"EpUklR4xnVMH"},"source":["# **Mappings Generation**"]},{"cell_type":"markdown","metadata":{"id":"ljGEyKNOerBT"},"source":["# **Using faiss l2**"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"xOSRYREwerBi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748119881074,"user_tz":-60,"elapsed":8937,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"}},"outputId":"3483d518-0d97-4067-87e0-e2816bbaeae8"},"outputs":[{"output_type":"stream","name":"stdout","text":["🔹 Using L2 (Euclidean) distance with FAISS\n","Top-10 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Results/ncit2doid_top_10_mappings_All_Mini.tsv\n","⏱️ Execution time: 8.95 seconds\n"]}],"source":["# Compute the top-10 most similar mappings using l2 distance\n","# between ResMLP-encoded embeddings of the source and target ontologies.\n","# The input embeddings were previously encoded using the ResMLPEncoder,\n","# and the similarity score is computed as the inverse of the l2 distance.\n","# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.\n","topk_faiss_l2(\n","    src_emb_path=f\"{data_dir}/{src_ent}_final_embeddings_All_Mini_cleaned.tsv\",\n","    tgt_emb_path=f\"{data_dir}/{tgt_ent}_final_embeddings_All_Mini_cleaned.tsv\",\n","    top_k=10,\n","    output_file=f\"{results_dir}/{task}_top_10_mappings_All_Mini.tsv\"\n",")"]},{"cell_type":"markdown","metadata":{"id":"MD-mvVjaerBh"},"source":["# **Evaluation**"]},{"cell_type":"markdown","metadata":{"id":"r8GRfT_pR1kD"},"source":["# **Global Metrics: Precision, Recall and F1 score**"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"9WZKJM46erBi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748119883175,"user_tz":-60,"elapsed":2102,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"}},"outputId":"4a25cf7b-73bc-45a1-a329-2dc82f3d9e80"},"outputs":[{"output_type":"stream","name":"stdout","text":["📁 Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Results/ncit2doid_top_10_mappings_All_Mini_predictions.tsv\n","📌 Number of predictions in output: 3175\n","🎯 Correct mappings (Top-1): 2664\n","📊 Evaluation (P / R / F1): {'P': 0.839, 'R': 0.812, 'F1': 0.825}\n"]}],"source":["# Run the evaluation on the predicted mappings using a filtering and evaluation function.\n","\n","output_file, metrics, correct = evaluate_predictions(\n","    topk_file=f\"{results_dir}/{task}_top_10_mappings_All_Mini.tsv\",\n","    # Path to the TSV file containing predicted mappings with scores (before filtering).\n","\n","    train_file=f\"{dataset_dir}/refs_equiv/train.tsv\",\n","    # Path to the training reference file (used to exclude mappings involving train-only entities).\n","\n","    test_file=f\"{dataset_dir}/refs_equiv/test.tsv\",\n","    # Path to the test reference file (used as the gold standard for evaluation).\n","\n","    src_onto=src_onto,\n","    # The source ontology object, used to detect ignored classes or perform additional filtering.\n","\n","    tgt_onto=tgt_onto,\n","    # The target ontology object, used similarly for filtering ignored or irrelevant classes.\n",")\n","\n","# This function returns:\n","# - `output_file`: the path to the filtered and evaluated output file.\n","# - `metrics`: a tuple containing (Precision, Recall, F1-score).\n","# - `correct`: the number of correctly predicted mappings found in the gold standard.\n"]},{"cell_type":"markdown","metadata":{"id":"KE3WArY1SAWO"},"source":["# **Metrics@1**"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"h0y9PGOjerBh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748119885516,"user_tz":-60,"elapsed":2337,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"}},"outputId":"1fbf2e44-4ba5-4c56-d042-d046ad1f6a1e"},"outputs":[{"output_type":"stream","name":"stdout","text":["🔹 Using L2 (Euclidean) distance with FAISS\n","Top-1 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Results/ncit2doid_top_1_mappings_All_Mini.tsv\n","⏱️ Execution time: 2.32 seconds\n"]}],"source":["# Compute the top-1 most similar mappings using l2 distance\n","# and the similarity score is computed as the inverse of the l2 distance.\n","# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.\n","topk_faiss_l2(\n","    src_emb_path=f\"{data_dir}/{src_ent}_final_embeddings_All_Mini_cleaned.tsv\",\n","    tgt_emb_path=f\"{data_dir}/{tgt_ent}_final_embeddings_All_Mini_cleaned.tsv\",\n","    top_k=1,\n","    output_file=f\"{results_dir}/{task}_top_1_mappings_All_Mini.tsv\"\n",")"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"wk-B3ayYerBh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748119886079,"user_tz":-60,"elapsed":561,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"}},"outputId":"44a5e24d-e7d6-4eac-da09-07ea99e7f226"},"outputs":[{"output_type":"stream","name":"stdout","text":["📁 Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Results/ncit2doid_top_1_mappings_All_Mini_predictions.tsv\n","📌 Number of predictions in output: 2648\n","{'Precision@1': 0.8999, 'Recall@1': 0.8902, 'F1@1': 0.895}\n"]}],"source":["# === Evaluate Top-1 Mappings ===\n","\n","results = evaluate_topk(\n","    topk_file=f\"{results_dir}/{task}_top_1_mappings_All_Mini.tsv\",\n","    # Path to the file containing the predicted mappings with scores.\n","    # This file may include unfiltered predictions (e.g., over all candidates).\n","\n","    train_file=f\"{dataset_dir}/refs_equiv/train.tsv\",\n","    # Path to the training reference mappings file.\n","    # Used to remove mappings that involve entities appearing only in training.\n","\n","    test_file=f\"{dataset_dir}/refs_equiv/test.tsv\",\n","    # Path to the test reference mappings file.\n","    # Ground-truth correspondences are extracted from this file for evaluation.\n","\n","    k=1  # Evaluate top-1 predictions per source entity.\n",")\n","\n","# === Display evaluation results ===\n","print(results)\n","# Outputs a dictionary with Precision@1, Recall@1, and F1@1\n"]},{"cell_type":"markdown","metadata":{"id":"Mf6vZML-KewM"},"source":["# **Local MRR and Hit@k**"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"7xXm15EQKeE_","executionInfo":{"status":"ok","timestamp":1748119895235,"user_tz":-60,"elapsed":9154,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"}}},"outputs":[],"source":["import pandas as pd\n","\n","# === Step 1: Load input files ===\n","\n","# Define paths to cleaned embedding files\n","src_emb_path = f\"{data_dir}/{src_ent}_final_embeddings_All_Mini_cleaned.tsv\"\n","tgt_emb_path = f\"{data_dir}/{tgt_ent}_final_embeddings_All_Mini_cleaned.tsv\"\n","\n","# Load candidate mappings (SrcEntity, TgtEntity) and source/target embeddings\n","df_cands = pd.read_csv(cands_path)\n","src_emb_df = pd.read_csv(src_emb_path, sep=\"\\t\")\n","tgt_emb_df = pd.read_csv(tgt_emb_path, sep=\"\\t\")\n","\n","# === Step 2: Extract unique source and target URIs from the candidate pairs ===\n","\n","# Keep only distinct source and target entities (URIs) for which embeddings are needed\n","unique_src_df = pd.DataFrame(df_cands[\"SrcEntity\"].unique(), columns=[\"Concept\"])\n","unique_tgt_df = pd.DataFrame(df_cands[\"TgtEntity\"].unique(), columns=[\"Concept\"])\n","\n","# === Step 3: Join embeddings for each concept based on the \"Concept\" URI ===\n","\n","# Merge source entities with their corresponding embeddings (if available)\n","merged_src_df = pd.merge(unique_src_df, src_emb_df, on=\"Concept\", how=\"left\")\n","\n","# Merge target entities with their corresponding embeddings (if available)\n","merged_tgt_df = pd.merge(unique_tgt_df, tgt_emb_df, on=\"Concept\", how=\"left\")\n","\n","# === Step 4: Save the merged results to TSV files ===\n","\n","# Save the source concepts and their embeddings to file\n","merged_src_df.to_csv(f\"{data_dir}/{src_ent}_cands_with_embeddings_All_Mini.tsv\", sep=\"\\t\", index=False)\n","\n","# Save the target concepts and their embeddings to file\n","merged_tgt_df.to_csv(f\"{data_dir}/{tgt_ent}_cands_with_embeddings_All_Mini.tsv\", sep=\"\\t\", index=False)"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"_BgQMQzperBl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748119901216,"user_tz":-60,"elapsed":5976,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"}},"outputId":"9e2c45c1-0e91-4cae-bfaf-4ed12c3b17dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["🔹 Using L2 (Euclidean) distance with FAISS\n","Top-200 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Results/ncit2doid_top_200_mappings_mrr_hit_All_Mini.tsv\n","⏱️ Execution time: 5.96 seconds\n"]}],"source":["topk_faiss_l2(\n","    # Path to the source entity embeddings (already filtered and linearly encoded)\n","    src_emb_path=f\"{data_dir}/{src_ent}_cands_with_embeddings_All_Mini.tsv\",\n","\n","    # Path to the target entity embeddings (already filtered and linearly encoded)\n","    tgt_emb_path=f\"{data_dir}/{tgt_ent}_cands_with_embeddings_All_Mini.tsv\",\n","\n","    # Number of top matches to retrieve per source entity (Top-K candidates)\n","    top_k=200,\n","\n","    # Path to save the resulting Top-K mappings sorted by FAISS L2 distance (converted to similarity)\n","    output_file=f\"{results_dir}/{task}_top_200_mappings_mrr_hit_All_Mini.tsv\"\n",")\n"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"DpzkN2-verBl","executionInfo":{"status":"ok","timestamp":1748119929562,"user_tz":-60,"elapsed":28347,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"}}},"outputs":[],"source":["test_cands = f\"{dataset_dir}/refs_equiv/test.cands.tsv\"\n","# Compute MRR and Hits@k metrics\n","# This function evaluates the predicted rankings against the reference mappings\n","results = compute_mrr_and_hits(\n","    reference_file=test_cands,             # Reference file with true ranks\n","    predicted_file=f\"{results_dir}/{task}_top_200_mappings_mrr_hit_All_Mini.tsv\",             # File containing predicted rankings\n","    output_file=formatted_predictions_path,    # File path to save formatted predictions\n","    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10\n",")"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"quXigRGeerBl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748119932267,"user_tz":-60,"elapsed":2702,"user":{"displayName":"BioGITOM Ontology Matching","userId":"17792409086127109994"}},"outputId":"5cbe9705-365d-492f-df40-92dc2a0fbfec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Ranking Evaluation Results at K=1, 5, and 10:\n","{'MRR': 0.84121904332949, 'Hits@1': 0.7731707317073171, 'Hits@5': 0.9286585365853659, 'Hits@10': 0.9560975609756097}\n"]}],"source":["# Evaluate ranking performance using standard metrics like MRR and Hits@K\n","# 'formatted_predictions_path' should point to a TSV file with columns: SrcEntity, TgtEntity, TgtCandidates\n","# This function computes how well the true targets are ranked among the candidates\n","results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])\n","\n","# Print the evaluation results for Hits@1, Hits@5, and Hits@10\n","print(\"Ranking Evaluation Results at K=1, 5, and 10:\")\n","print(results)"]},{"cell_type":"markdown","metadata":{"id":"yFqKl-p1aVPF"},"source":[]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":["zqEXsgPGMVhw","zxCn5ztKVztw","QpwWQ2ndKGOA"],"gpuType":"V28","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}