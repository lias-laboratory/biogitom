{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkAX88H3RNRW"
      },
      "source": [
        "# **Package Installation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bSuJvX5_qNhr",
        "outputId": "8537c1a5-5d9b-4174-ad8c-ee8934a47508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.6.0\n",
            "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision==0.21.0\n",
            "  Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting filelock (from torch==2.6.0)\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting typing-extensions>=4.10.0 (from torch==2.6.0)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting networkx (from torch==2.6.0)\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch==2.6.0)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec (from torch==2.6.0)\n",
            "  Downloading fsspec-2025.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.6.0)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.2.0 (from torch==2.6.0)\n",
            "  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting sympy==1.13.1 (from torch==2.6.0)\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting numpy (from torchvision==0.21.0)\n",
            "  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.21.0)\n",
            "  Downloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch==2.6.0)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.6.0)\n",
            "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m483.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m114.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m625.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Downloading fsspec-2025.5.0-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.3.0\n",
            "    Uninstalling mpmath-1.3.0:\n",
            "      Successfully uninstalled mpmath-1.3.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.13.2\n",
            "    Uninstalling typing_extensions-4.13.2:\n",
            "      Successfully uninstalled typing_extensions-4.13.2\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.2.1\n",
            "    Uninstalling pillow-11.2.1:\n",
            "      Successfully uninstalled pillow-11.2.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.4.2\n",
            "    Uninstalling networkx-3.4.2:\n",
            "      Successfully uninstalled networkx-3.4.2\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.18.0\n",
            "    Uninstalling filelock-3.18.0:\n",
            "      Successfully uninstalled filelock-3.18.0\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cpu\n",
            "    Uninstalling torch-2.6.0+cpu:\n",
            "      Successfully uninstalled torch-2.6.0+cpu\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cpu\n",
            "    Uninstalling torchvision-0.21.0+cpu:\n",
            "      Successfully uninstalled torchvision-0.21.0+cpu\n",
            "Successfully installed MarkupSafe-3.0.2 filelock-3.18.0 fsspec-2025.5.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.6 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pillow-11.2.1 sympy-1.13.1 torch-2.6.0 torchvision-0.21.0 triton-3.2.0 typing-extensions-4.13.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "284e289a9b2b4998b5075a64194a0fb7"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Reinstall a specific version of PyTorch (v2.6.0) and torchvision (v0.21.0)\n",
        "# The \"--force-reinstall\" flag ensures that the packages are reinstalled even if the correct version is already present.\n",
        "# This is useful to resolve environment issues or when dependencies need to be reset.\n",
        "!pip install torch==2.6.0 torchvision==0.21.0 --force-reinstall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItSvFeEAfLBF",
        "outputId": "f48a0785-c98d-4fc1-9e92-d6eae6ac7c1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.2.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.2.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Collecting torch-geometric==2.4.0\n",
            "  Using cached torch_geometric-2.4.0-py3-none-any.whl.metadata (63 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (2.2.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (1.15.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (3.1.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (2.32.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (3.2.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (1.6.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric==2.4.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.4.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.4.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.4.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.4.0) (2025.4.26)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->torch-geometric==2.4.0) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->torch-geometric==2.4.0) (3.6.0)\n",
            "Using cached torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "Installing collected packages: torch-geometric\n",
            "  Attempting uninstall: torch-geometric\n",
            "    Found existing installation: torch-geometric 2.7.0\n",
            "    Uninstalling torch-geometric-2.7.0:\n",
            "      Successfully uninstalled torch-geometric-2.7.0\n",
            "Successfully installed torch-geometric-2.4.0\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.11/dist-packages (2.1.2+pt20cpu)\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.11/dist-packages (0.6.18+pt20cpu)\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.11/dist-packages (1.6.3+pt20cpu)\n",
            "Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.11/dist-packages (1.2.2+pt20cpu)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (2.2.6)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deeponto\n",
            "  Using cached deeponto-0.9.3-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting JPype1 (from deeponto)\n",
            "  Using cached jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting yacs (from deeponto)\n",
            "  Using cached yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from deeponto) (2.6.0)\n",
            "Collecting anytree (from deeponto)\n",
            "  Using cached anytree-2.13.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from deeponto) (8.2.0)\n",
            "Collecting dill (from deeponto)\n",
            "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from deeponto) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deeponto) (2.2.6)\n",
            "Requirement already satisfied: scikit_learn in /usr/local/lib/python3.11/dist-packages (from deeponto) (1.6.1)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.11/dist-packages (from deeponto) (4.51.3)\n",
            "Collecting datasets (from deeponto)\n",
            "  Using cached datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (from deeponto) (3.8.5)\n",
            "Collecting pprintpp (from deeponto)\n",
            "  Using cached pprintpp-0.4.0-py2.py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from deeponto) (3.4.2)\n",
            "Collecting lxml (from deeponto)\n",
            "  Using cached lxml-5.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting textdistance (from deeponto)\n",
            "  Using cached textdistance-4.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (from deeponto) (7.7.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from deeponto) (6.17.1)\n",
            "Collecting enlighten (from deeponto)\n",
            "  Using cached enlighten-1.14.1-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Collecting rdflib (from deeponto)\n",
            "  Using cached rdflib-7.1.4-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from deeponto) (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (20.0.0)\n",
            "Collecting dill (from deeponto)\n",
            "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (3.5.0)\n",
            "Collecting multiprocess<0.70.17 (from datasets->deeponto)\n",
            "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto)\n",
            "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (6.0.2)\n",
            "Collecting blessed>=1.17.7 (from enlighten->deeponto)\n",
            "  Using cached blessed-1.21.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting prefixed>=0.3.2 (from enlighten->deeponto)\n",
            "  Using cached prefixed-0.9.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (1.8.0)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (6.4.2)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (5.7.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->deeponto) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->deeponto) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->deeponto) (3.0.15)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->deeponto) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->deeponto) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->deeponto) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->deeponto) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->deeponto) (2025.2)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib->deeponto) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn->deeponto) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn->deeponto) (3.6.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (0.15.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (4.13.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->deeponto) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]->deeponto) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]->deeponto) (0.5.3)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]->deeponto) (1.6.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from blessed>=1.17.7->enlighten->deeponto) (0.2.13)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (3.11.15)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel->deeponto)\n",
            "  Using cached jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (5.2.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (4.9.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel->deeponto) (5.7.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->deeponto) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->deeponto) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->deeponto) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->deeponto) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->deeponto) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->deeponto) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->deeponto) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->deeponto) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->deeponto) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->deeponto) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->deeponto) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy->deeponto) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy->deeponto) (14.0.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->deeponto) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->deeponto) (7.1.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->deeponto) (6.5.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy->deeponto) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (1.20.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->deeponto) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel->deeponto) (4.3.8)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->deeponto) (1.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (23.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->deeponto) (0.7.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->deeponto) (3.0.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->deeponto) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->deeponto) (0.1.2)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (4.23.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.24.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (2.7)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (2.22)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.3.1)\n",
            "Using cached deeponto-0.9.3-py3-none-any.whl (89.7 MB)\n",
            "Using cached anytree-2.13.0-py3-none-any.whl (45 kB)\n",
            "Using cached datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Using cached enlighten-1.14.1-py2.py3-none-any.whl (42 kB)\n",
            "Using cached jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n",
            "Using cached lxml-5.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.9 MB)\n",
            "Using cached pprintpp-0.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Using cached rdflib-7.1.4-py3-none-any.whl (565 kB)\n",
            "Using cached textdistance-4.6.3-py3-none-any.whl (31 kB)\n",
            "Using cached yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Using cached blessed-1.21.0-py2.py3-none-any.whl (84 kB)\n",
            "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "Using cached prefixed-0.9.0-py2.py3-none-any.whl (13 kB)\n",
            "Using cached jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "Installing collected packages: prefixed, pprintpp, yacs, textdistance, rdflib, lxml, JPype1, jedi, fsspec, dill, blessed, anytree, multiprocess, enlighten, datasets, deeponto\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.5.0\n",
            "    Uninstalling fsspec-2025.5.0:\n",
            "      Successfully uninstalled fsspec-2025.5.0\n",
            "Successfully installed JPype1-1.5.2 anytree-2.13.0 blessed-1.21.0 datasets-3.6.0 deeponto-0.9.3 dill-0.3.8 enlighten-1.14.1 fsspec-2025.3.0 jedi-0.19.2 lxml-5.4.0 multiprocess-0.70.16 pprintpp-0.4.0 prefixed-0.9.0 rdflib-7.1.4 textdistance-4.6.3 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "# === Base Libraries ===\n",
        "!pip install numpy --upgrade\n",
        "!pip install pandas\n",
        "!pip install optuna\n",
        "\n",
        "# === FAISS (for Approximate Nearest Neighbor Search) ===\n",
        "!pip install faiss-cpu        # CPU version (recommended unless using GPU)\n",
        "# !pip install faiss-gpu      # Uncomment if running on CUDA-enabled GPU\n",
        "\n",
        "# === PyTorch Geometric and dependencies ===\n",
        "!pip install torch-geometric==2.4.0\n",
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
        "# Optional: latest dev version from GitHub\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "\n",
        "# === DeepOnto (Ontology Matching Toolkit) ===\n",
        "!pip install deeponto\n",
        "# Optionally install custom version from a GitHub repository\n",
        "# !pip install git+https://github.com/<username>/deeponto.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nFonRjT5fMCv"
      },
      "outputs": [],
      "source": [
        "# Import pandas for working with tabular data (e.g., CSV, TSV files)\n",
        "import pandas as pd\n",
        "\n",
        "# Import numpy for numerical operations and efficient array handling\n",
        "import numpy as np\n",
        "\n",
        "# Import json for reading and writing JSON-formatted files (useful for config or ontology structures)\n",
        "import json\n",
        "\n",
        "# Import pickle for serializing and deserializing Python objects (e.g., saving models or processed data)\n",
        "import pickle\n",
        "\n",
        "# Import warnings to control or suppress warning messages during runtime\n",
        "import warnings\n",
        "\n",
        "# Import gc (garbage collector) for managing memory manually when dealing with large datasets\n",
        "import gc\n",
        "\n",
        "# Ignore all warning messages to keep the output clean\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uchfZJP2fZwe"
      },
      "outputs": [],
      "source": [
        "# Import PyTorch core library for tensor operations and model definition\n",
        "import torch\n",
        "\n",
        "# Import commonly used PyTorch components\n",
        "from torch import Tensor, optim  # Tensor type and optimization algorithms (e.g., SGD, Adam)\n",
        "\n",
        "# Import PyTorch's neural network module (base class for defining models)\n",
        "import torch.nn as nn\n",
        "\n",
        "# Import PyTorch's functional API for operations like activations and loss functions\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Import DataLoader utilities for batching and loading datasets during training\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# === PyTorch Geometric (PyG) modules for graph-based learning ===\n",
        "\n",
        "# Basic graph data structure from PyG\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# PyG-specific DataLoader for batching graphs\n",
        "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
        "\n",
        "# Import graph convolution layers and pooling functions from PyG\n",
        "from torch_geometric.nn import (\n",
        "    GCNConv,             # Graph Convolutional Network layer\n",
        "    GINConv,             # Graph Isomorphism Network convolution\n",
        "    global_mean_pool,    # Global mean pooling over node embeddings\n",
        "    global_add_pool,     # Global sum pooling over node embeddings\n",
        "    MessagePassing       # Base class for defining custom GNN layers\n",
        ")\n",
        "\n",
        "# Explicitly re-import MessagePassing (optional if already above)\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "\n",
        "# Graph utility functions from PyG\n",
        "from torch_geometric.utils import (\n",
        "    to_undirected,       # Converts a directed graph to undirected\n",
        "    softmax              # Softmax over edges (e.g., for attention)\n",
        ")\n",
        "\n",
        "# Initialization utilities for GNN layers\n",
        "from torch_geometric.nn.inits import (\n",
        "    reset,               # Reset parameters\n",
        "    glorot,              # Glorot (Xavier) weight initialization\n",
        "    zeros                # Zero initialization\n",
        ")\n",
        "\n",
        "# Typing utilities from PyG for adjacency and tensor specifications\n",
        "from torch_geometric.typing import (\n",
        "    Adj, OptTensor, PairTensor, SparseTensor\n",
        ")\n",
        "\n",
        "# Dense linear transformation layer from PyG (alternative to torch.nn.Linear)\n",
        "from torch_geometric.nn.dense.linear import Linear\n",
        "\n",
        "# Additional PyTorch neural network components\n",
        "from torch.nn import (\n",
        "    Linear,             # Fully connected (dense) layer\n",
        "    PReLU,              # Parametric ReLU activation\n",
        "    Sequential,         # Layer container for building sequential models\n",
        "    BatchNorm1d,        # Batch normalization for 1D inputs\n",
        "    Dropout             # Dropout regularization\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ziMBSWE8ff1N"
      },
      "outputs": [],
      "source": [
        "# Import matplotlib for creating visualizations (e.g., loss curves, evaluation metrics, embedding projections)\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JeAvp6PNfiLh"
      },
      "outputs": [],
      "source": [
        "# Import function to split data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import encoder to convert categorical labels into integer values (useful for classification tasks)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Import evaluation metrics for classification and regression tasks\n",
        "from sklearn.metrics import (\n",
        "    f1_score,            # Harmonic mean of precision and recall; useful for imbalanced classification\n",
        "    precision_score,     # Measures the proportion of true positives among all predicted positives\n",
        "    accuracy_score,      # Measures overall correctness of predictions (classification)\n",
        "    mean_squared_error,  # Measures average squared difference between predicted and actual values (regression)\n",
        "    mean_absolute_error  # Measures average absolute difference between predicted and actual values (regression)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jm1rMZvmfl2M",
        "outputId": "4622e169-1435-4252-df22-e5f4f712cd3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the maximum memory located to JVM [8g]: 8g\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import the Ontology class for loading and manipulating OWL ontologies\n",
        "from deeponto.onto import Ontology\n",
        "\n",
        "# Import all components related to OAEI (Ontology Alignment Evaluation Initiative) benchmarking\n",
        "from deeponto.align.oaei import *\n",
        "\n",
        "# Import data structures for representing mappings between ontology entities\n",
        "from deeponto.align.mapping import EntityMapping, ReferenceMapping\n",
        "# - EntityMapping: represents a predicted alignment (one or more mappings)\n",
        "# - ReferenceMapping: represents the gold standard/reference alignments\n",
        "\n",
        "# Import the evaluator to compute Precision, Recall, and F1-score for alignments\n",
        "from deeponto.align.evaluation import AlignmentEvaluator\n",
        "\n",
        "# Utility function to read TSV/CSV tables as mapping or data frames\n",
        "from deeponto.utils import read_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JYhwr3Q_ft2N"
      },
      "outputs": [],
      "source": [
        "# Import Optuna, a hyperparameter optimization framework for automating model tuning using strategies like Bayesian optimization\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UmSCo5Olfzuz"
      },
      "outputs": [],
      "source": [
        "# Import the math module for mathematical functions (e.g., sqrt, log, exp)\n",
        "import math\n",
        "\n",
        "# Import the time module for measuring execution time of code blocks or functions\n",
        "import time\n",
        "\n",
        "# Import typing annotations for function signatures and code clarity\n",
        "from typing import Optional, Tuple, Union, Callable\n",
        "# - Optional[T]: denotes a value that could be of type T or None\n",
        "# - Tuple: fixed-size ordered collection of elements\n",
        "# - Union: allows multiple possible types (e.g., Union[int, str])\n",
        "# - Callable: represents a function or method type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9WNn0OMQW2CS"
      },
      "outputs": [],
      "source": [
        "# Import Python's built-in random module for generating pseudo-random numbers\n",
        "import random\n",
        "\n",
        "# Set the seed for PyTorch's random number generator to ensure reproducibility\n",
        "import torch\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Set the seed for NumPy's random number generator to ensure reproducibility\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "\n",
        "# Set the seed for Python's built-in random module to ensure reproducibility\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-abbBHOoRdWl"
      },
      "source": [
        "# **Paths Definition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVgl_Bb42naS",
        "outputId": "d40cbebe-2b9a-42d2-8238-39c9faaff34f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Importing the 'drive' module from Google Colab to interact with Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount the user's Google Drive to the Colab environment\n",
        "# After running this, a link will appear to authorize access, and Google Drive will be mounted at '/content/gdrive'\n",
        "drive.mount('/content/gdrive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "36ttssQ3W7cx"
      },
      "outputs": [],
      "source": [
        "# Define the source ontology name\n",
        "src_ent = \"ncit\"\n",
        "\n",
        "# Define the target ontology name\n",
        "tgt_ent = \"doid\"\n",
        "\n",
        "# Define the task name for this ontology matching process\n",
        "task = \"ncit2doid\"\n",
        "\n",
        "# Define the similarity threshold for validating matches\n",
        "thres = 0.50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SJpvkdwVSQye"
      },
      "outputs": [],
      "source": [
        "dir = \"/content/gdrive/My Drive/BioGITOM-VLDB/\"\n",
        "\n",
        "# Define the directory for the dataset containing source and target ontologies\n",
        "dataset_dir = f\"{dir}/Datasets/{task}\"\n",
        "\n",
        "# Define the data directory for storing embeddings, adjacency matrices, and related files\n",
        "data_dir = f\"{dir}/{task}/Data\"\n",
        "\n",
        "# Define the directory for storing the results\n",
        "results_dir = f\"{dir}/{task}/Results\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eFDNSFef23er"
      },
      "outputs": [],
      "source": [
        "# Load the Source ontology using the Ontology class from DeepOnto\n",
        "# This initializes the source ontology by loading its .owl file.\n",
        "src_onto = Ontology(f\"{dataset_dir}/{src_ent}.owl\")\n",
        "\n",
        "# Load the Target ontology using the Ontology class from DeepOnto\n",
        "# This initializes the target ontology by loading its .owl file.\n",
        "tgt_onto = Ontology(f\"{dataset_dir}/{tgt_ent}.owl\")\n",
        "\n",
        "# Define the file path for the Source embeddings CSV file\n",
        "# Embeddings for the source ontology entities are stored in this file.\n",
        "src_Emb = f\"{data_dir}/{src_ent}_bert-base-nli-mean-tokens_emb.csv\"\n",
        "\n",
        "# Define the file path for the Target embeddings CSV file\n",
        "# Embeddings for the target ontology entities are stored in this file.\n",
        "tgt_Emb = f\"{data_dir}/{tgt_ent}_bert-base-nli-mean-tokens_emb.csv\"\n",
        "\n",
        "# Define the file path for the Source adjacency matrix\n",
        "# This file represents the relationships (edges) between entities in the source ontology.\n",
        "src_Adjacence = f\"{data_dir}/{src_ent}_adjacence.csv\"\n",
        "\n",
        "# Define the file path for the Target adjacency matrix\n",
        "# This file represents the relationships (edges) between entities in the target ontology.\n",
        "tgt_Adjacence = f\"{data_dir}/{tgt_ent}_adjacence.csv\"\n",
        "\n",
        "# Define the file path for the JSON file containing the Source ontology class labels\n",
        "# This file maps the source ontology entities to their labels or names.\n",
        "src_class = f\"{data_dir}/{src_ent}_classes.json\"\n",
        "\n",
        "# Define the file path for the JSON file containing the Target ontology class labels\n",
        "# This file maps the target ontology entities to their labels or names.\n",
        "tgt_class = f\"{data_dir}/{tgt_ent}_classes.json\"\n",
        "\n",
        "# Define the file path for the train data\n",
        "train_file = f\"{data_dir}/{task}_train.csv\"\n",
        "\n",
        "# Define the file path for the test data\n",
        "# The test file contains reference mappings (ground truth) between the source and target ontologies.\n",
        "test_file = f\"{dataset_dir}/refs_equiv/test.tsv\"\n",
        "\n",
        "# Define the file path for the candidate mappings used during testing\n",
        "# This file includes the candidate pairs (source and target entities) for ranking based metrics.\n",
        "test_cands = f\"{dataset_dir}/refs_equiv/test.cands.tsv\"\n",
        "cands_path = f\"{data_dir}/{task}_cands.csv\"\n",
        "\n",
        "# Define the path where the prediction results will be saved in TSV format\n",
        "# This file will store the final predictions (mappings) between source and target entities.\n",
        "prediction_path = f\"{results_dir}/{task}_matching_results_bert-base-nli-mean-tokens.tsv\"\n",
        "\n",
        "# Define the path where all prediction results will be saved in TSV format\n",
        "# This file will store detailed prediction results, including all candidate scores.\n",
        "all_predictions_path = f\"{results_dir}/{task}_all_predictions_bert-base-nli-mean-tokens.tsv\"\n",
        "\n",
        "# Define the path where formatted ranking predictions will be saved in TSV format\n",
        "# This file will contain predictions formatted for evaluation using ranking-based metrics.\n",
        "formatted_predictions_path = f\"{results_dir}/{task}_formatted_predictions_bert-base-nli-mean-tokens.tsv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqEXsgPGMVhw"
      },
      "source": [
        "# **GIT Architecture**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "A_d6XCsUMVhx"
      },
      "outputs": [],
      "source": [
        "# RGIT class definition which inherits from PyTorch Geometric's MessagePassing class\n",
        "class RGIT(MessagePassing):\n",
        "\n",
        "    _alpha: OptTensor  # Define _alpha as an optional tensor for storing attention weights\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        nn: Callable,  # Neural network to be used in the final layer of the GNN\n",
        "        in_channels: Union[int, Tuple[int, int]],  # Input dimension, can be a single or pair of integers\n",
        "        out_channels: int,  # Output dimension of the GNN\n",
        "        eps: float = 0.,  # GIN parameter: epsilon for GIN aggregation\n",
        "        train_eps: bool = False,  # GIN parameter: whether epsilon should be learnable\n",
        "        heads: int = 1,  # Transformer parameter: number of attention heads\n",
        "        dropout: float = 0.,  # Dropout rate for attention weights\n",
        "        edge_dim: Optional[int] = None,  # Dimension for edge attributes (optional)\n",
        "        bias: bool = True,  # Whether to use bias in linear layers\n",
        "        root_weight: bool = True,  # GIN parameter: whether to apply root weight in aggregation\n",
        "        **kwargs,  # Additional arguments passed to the parent class\n",
        "    ):\n",
        "        # Set the aggregation type to 'add' and initialize the parent class with node_dim=0\n",
        "        kwargs.setdefault('aggr', 'add')\n",
        "        super().__init__(node_dim=0, **kwargs)\n",
        "\n",
        "        # Initialize input/output dimensions, neural network, and GIN/transformer parameters\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.nn = nn  # Neural network used by the GNN\n",
        "        self.initial_eps = eps  # Initial value of epsilon for GIN\n",
        "\n",
        "        # Set epsilon to be learnable or fixed\n",
        "        if train_eps:\n",
        "            self.eps = torch.nn.Parameter(torch.empty(1))  # Learnable epsilon\n",
        "        else:\n",
        "            self.register_buffer('eps', torch.empty(1))  # Non-learnable epsilon (fixed)\n",
        "\n",
        "        # Initialize transformer-related parameters\n",
        "        self.heads = heads\n",
        "        self.dropout = dropout\n",
        "        self.edge_dim = edge_dim\n",
        "        self._alpha = None  # Placeholder for attention weights\n",
        "\n",
        "        # Handle case where in_channels is a single integer or a tuple\n",
        "        if isinstance(in_channels, int):\n",
        "            in_channels = (in_channels, in_channels)\n",
        "\n",
        "        # Define the linear layers for key, query, and value for the transformer mechanism\n",
        "        self.lin_key = Linear(in_channels[0], heads * out_channels)\n",
        "        self.lin_query = Linear(in_channels[1], heads * out_channels)\n",
        "        self.lin_value = Linear(in_channels[0], heads * out_channels)\n",
        "\n",
        "        # Define linear transformation for edge embeddings if provided\n",
        "        if edge_dim is not None:\n",
        "            self.lin_edge = Linear(edge_dim, heads * out_channels, bias=False)\n",
        "        else:\n",
        "            self.lin_edge = self.register_parameter('lin_edge', None)\n",
        "\n",
        "        # Reset all parameters to their initial values\n",
        "        self.reset_parameters()\n",
        "\n",
        "    # Function to reset model parameters\n",
        "    def reset_parameters(self):\n",
        "        super().reset_parameters()  # Call parent class reset method\n",
        "        self.lin_key.reset_parameters()  # Reset key linear layer\n",
        "        self.lin_query.reset_parameters()  # Reset query linear layer\n",
        "        self.lin_value.reset_parameters()  # Reset value linear layer\n",
        "        if self.edge_dim:\n",
        "            self.lin_edge.reset_parameters()  # Reset edge linear layer if used\n",
        "        reset(self.nn)  # Reset the neural network provided\n",
        "        self.eps.data.fill_(self.initial_eps)  # Initialize epsilon with the starting value\n",
        "\n",
        "    # Forward function defining how the input data flows through the model\n",
        "    def forward(self, x: Union[Tensor, PairTensor], edge_index: Adj,\n",
        "                edge_attr: OptTensor = None, return_attention_weights=None):\n",
        "        # Unpack number of heads and output channels\n",
        "        H, C = self.heads, self.out_channels\n",
        "\n",
        "        # If x is a tensor, treat it as a pair of tensors (source and target embeddings)\n",
        "        if isinstance(x, Tensor):\n",
        "            x: PairTensor = (x, x)\n",
        "\n",
        "        # Extract source node embeddings\n",
        "        x_t = x[0]\n",
        "\n",
        "        # Apply linear transformations and reshape query, key, and value for multi-head attention\n",
        "        query = self.lin_query(x[1]).view(-1, H, C)\n",
        "        key = self.lin_key(x[0]).view(-1, H, C)\n",
        "        value = self.lin_value(x[0]).view(-1, H, C)\n",
        "\n",
        "        # Propagate messages through the graph using the propagate function\n",
        "        out = self.propagate(edge_index, query=query, key=key, value=value,\n",
        "                             edge_attr=edge_attr, size=None)\n",
        "\n",
        "        # Retrieve attention weights and reset them\n",
        "        alpha = self._alpha\n",
        "        self._alpha = None  # Reset _alpha after use\n",
        "        out = out.mean(dim=1)  # Take the mean over all attention heads\n",
        "\n",
        "        # Apply GIN aggregation by adding epsilon-scaled original node embeddings\n",
        "        out = out + (1 + self.eps) * x_t\n",
        "        return self.nn(out)  # Pass through the neural network\n",
        "\n",
        "    # Message passing function which calculates attention and combines messages\n",
        "    def message(self, query_i: Tensor, key_j: Tensor, value_j: Tensor,\n",
        "                edge_attr: OptTensor, index: Tensor, ptr: OptTensor,\n",
        "                size_i: Optional[int]) -> Tensor:\n",
        "        # If edge attributes are used, apply linear transformation and add them to the key\n",
        "        if self.lin_edge is not None:\n",
        "            assert edge_attr is not None\n",
        "            edge_attr = self.lin_edge(edge_attr).view(-1, self.heads, self.out_channels)\n",
        "            key_j = key_j + edge_attr\n",
        "\n",
        "        # Calculate attention (alpha) using the dot product between query and key\n",
        "        alpha = (query_i * key_j).sum(dim=-1) / math.sqrt(self.out_channels)\n",
        "        alpha = softmax(alpha, index, ptr, size_i)  # Apply softmax to normalize attention\n",
        "        self._alpha = alpha  # Store attention weights\n",
        "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)  # Apply dropout\n",
        "\n",
        "        # Calculate the output message by applying attention to the value\n",
        "        out = value_j\n",
        "        if edge_attr is not None:\n",
        "            out = out + edge_attr  # Add edge embeddings to the output if present\n",
        "        out = out * alpha.view(-1, self.heads, 1)  # Scale by attention weights\n",
        "        return out\n",
        "\n",
        "    # String representation function for debugging or printing\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
        "                f'{self.out_channels}, heads={self.heads})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qwFv6RgHmGCf"
      },
      "outputs": [],
      "source": [
        "# Define the RGIT_mod class, a multi-layer GNN that uses both RGIT and linear layers\n",
        "class RGIT_mod(torch.nn.Module):\n",
        "    \"\"\"Multi-layer RGIT with optional linear layers\"\"\"\n",
        "\n",
        "    # Initialize the model with hidden dimension, number of RGIT layers, and number of linear layers\n",
        "    def __init__(self, dim_h, num_layers, num_linear_layers=1):\n",
        "        super(RGIT_mod, self).__init__()\n",
        "        self.num_layers = num_layers  # Number of RGIT layers\n",
        "        self.num_linear_layers = num_linear_layers  # Number of linear layers\n",
        "        self.linears = torch.nn.ModuleList()  # List to store linear layers\n",
        "        self.rgit_layers = torch.nn.ModuleList()  # List to store RGIT layers\n",
        "\n",
        "        # Create a list of Linear and PReLU layers (for encoding entity names)\n",
        "        for _ in range(num_linear_layers):\n",
        "            self.linears.append(Linear(dim_h, dim_h))  # Linear transformation layer\n",
        "            self.linears.append(PReLU(num_parameters=dim_h))  # Parametric ReLU activation function\n",
        "\n",
        "        # Create a list of RGIT layers\n",
        "        for _ in range(num_layers):\n",
        "            self.rgit_layers.append(RGIT(  # Each RGIT layer contains a small MLP with Linear and PReLU\n",
        "                Sequential(Linear(dim_h, dim_h), PReLU(num_parameters=dim_h),\n",
        "                           Linear(dim_h, dim_h), PReLU(num_parameters=dim_h)), dim_h, dim_h))\n",
        "\n",
        "    # Forward pass through the model\n",
        "    def forward(self, x, edge_index):\n",
        "        # Apply the linear layers first to the input\n",
        "        for layer in self.linears:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Then apply the RGIT layers for message passing\n",
        "        for layer in self.rgit_layers:\n",
        "            x = layer(x, edge_index)\n",
        "\n",
        "        return x  # Return the final node embeddings after all layers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxCn5ztKVztw"
      },
      "source": [
        "# **Gated Network Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7MKQUv7o7zay"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "class GatedCombinationWithFaiss(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Linear layers to compute gating values for the source and target embeddings\n",
        "        self.gate_A_fc = nn.Linear(input_dim, input_dim)\n",
        "        self.gate_B_fc = nn.Linear(input_dim, input_dim)\n",
        "\n",
        "        # Final linear layer to map similarity score to prediction (sigmoid output)\n",
        "        self.fc = nn.Linear(1, 1)\n",
        "\n",
        "    def faiss_l2(self, a, b):\n",
        "        \"\"\"\n",
        "        Compute L2 distances using FAISS (non-differentiable).\n",
        "        This function converts tensors to NumPy, builds a FAISS index, and performs a search.\n",
        "        Only use this during inference or evaluation — not for training.\n",
        "\n",
        "        Args:\n",
        "            a (Tensor): Query vectors (batch_size x dim)\n",
        "            b (Tensor): Database vectors (batch_size x dim)\n",
        "\n",
        "        Returns:\n",
        "            Tensor: L2 distances between aligned rows (one-to-one)\n",
        "        \"\"\"\n",
        "        # Detach tensors from the computation graph and move to CPU\n",
        "        a_np = a.detach().cpu().numpy().astype(np.float32)\n",
        "        b_np = b.detach().cpu().numpy().astype(np.float32)\n",
        "\n",
        "        # Create a FAISS index for L2 distance\n",
        "        index = faiss.IndexFlatL2(a_np.shape[1])\n",
        "        index.add(b_np)\n",
        "\n",
        "        # Perform 1-NN search\n",
        "        distances, _ = index.search(a_np, 1)  # shape: (batch_size, 1)\n",
        "\n",
        "        # Convert back to PyTorch tensor on the original device\n",
        "        return torch.tensor(distances[:, 0], dtype=torch.float32, device=a.device)\n",
        "\n",
        "    def forward(self, x1, x2, x3, x4, return_embeddings=False):\n",
        "        \"\"\"\n",
        "        Forward pass through the gated combination model.\n",
        "        Combines original and transformed embeddings using learned gates.\n",
        "\n",
        "        Args:\n",
        "            x1, x2: original and GNN-transformed embeddings for source entities\n",
        "            x3, x4: original and GNN-transformed embeddings for target entities\n",
        "            return_embeddings (bool): if True, return gated embeddings instead of prediction\n",
        "\n",
        "        Returns:\n",
        "            Tensor: similarity score (if return_embeddings=False)\n",
        "            OR\n",
        "            Tuple[Tensor, Tensor]: gated source and target embeddings (if return_embeddings=True)\n",
        "        \"\"\"\n",
        "\n",
        "        # Compute gate for source embeddings\n",
        "        gate_values1 = torch.sigmoid(self.gate_A_fc(x1))\n",
        "        a = x1 * gate_values1 + x2 * (1 - gate_values1)\n",
        "\n",
        "        # Compute gate for target embeddings\n",
        "        gate_values2 = torch.sigmoid(self.gate_B_fc(x3))\n",
        "        b = x3 * gate_values2 + x4 * (1 - gate_values2)\n",
        "\n",
        "        if return_embeddings:\n",
        "            return a, b\n",
        "\n",
        "        # Compute non-differentiable distance with FAISS (1-to-1)\n",
        "        distance = self.faiss_l2(a, b)\n",
        "\n",
        "        # Pass through a sigmoid layer for binary classification output\n",
        "        out = torch.sigmoid(self.fc(distance.unsqueeze(1)))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Encoder Definition**"
      ],
      "metadata": {
        "id": "QpwWQ2ndKGOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Transformer-based Encoder ===\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, nhead=4, num_layers=1):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        # Define a single Transformer encoder layer\n",
        "        # d_model: input/output embedding dimension\n",
        "        # nhead: number of attention heads\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=nhead)\n",
        "\n",
        "        # Stack multiple Transformer encoder layers\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape [batch_size, embedding_dim]\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Encoded output of shape [batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "\n",
        "        # Step 1: Add a sequence length dimension (required format: [seq_len, batch_size, embedding_dim])\n",
        "        x = x.unsqueeze(1)             # Shape: [batch_size, 1, embedding_dim]\n",
        "        x = x.transpose(0, 1)          # Shape: [1, batch_size, embedding_dim] (PyTorch expects seq_len first)\n",
        "\n",
        "        # Step 2: Pass through the Transformer encoder\n",
        "        x = self.transformer_encoder(x)\n",
        "\n",
        "        # Step 3: Remove the sequence dimension to return to shape [batch_size, embedding_dim]\n",
        "        x = x.transpose(0, 1).squeeze(1)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "zYDHQY8fJ6YA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLJ5j9FNMVhy"
      },
      "source": [
        "# **Utility functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "k0L86DgUQjMU"
      },
      "outputs": [],
      "source": [
        "def adjacency_matrix_to_undirected_edge_index(adjacency_matrix):\n",
        "    \"\"\"\n",
        "    Converts an adjacency matrix into an undirected edge index for use in graph-based neural networks.\n",
        "\n",
        "    Args:\n",
        "        adjacency_matrix: A 2D list or array representing the adjacency matrix of a graph.\n",
        "\n",
        "    Returns:\n",
        "        edge_index_undirected: A PyTorch tensor representing the undirected edges.\n",
        "    \"\"\"\n",
        "    # Convert each element in the adjacency matrix to an integer (from boolean or float)\n",
        "    adjacency_matrix = [[int(element) for element in sublist] for sublist in adjacency_matrix]\n",
        "\n",
        "    # Convert the adjacency matrix into a PyTorch LongTensor (used for indexing)\n",
        "    edge_index = torch.tensor(adjacency_matrix, dtype=torch.long)\n",
        "\n",
        "    # Transpose the edge_index tensor so that rows represent edges in the form [source, target]\n",
        "    edge_index = edge_index.t().contiguous()\n",
        "\n",
        "    # Convert the directed edge_index into an undirected edge_index, meaning both directions are added (i.e., (i, j) and (j, i))\n",
        "    edge_index_undirected = to_undirected(edge_index)\n",
        "\n",
        "    return edge_index_undirected  # Return the undirected edge index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YvmOxkLcpf9w"
      },
      "outputs": [],
      "source": [
        "def build_indexed_dict(file_path):\n",
        "    \"\"\"\n",
        "    Builds a dictionary with numeric indexes for each key from a JSON file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the JSON file.\n",
        "\n",
        "    Returns:\n",
        "        indexed_dict (dict): A new dictionary where each key from the JSON file is assigned a numeric index.\n",
        "    \"\"\"\n",
        "    # Load the JSON file into a Python dictionary\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Create a new dictionary with numeric indexes as keys and the original JSON keys as values\n",
        "    indexed_dict = {index: key for index, key in enumerate(data.keys())}\n",
        "\n",
        "    return indexed_dict  # Return the newly created dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "QgFINoPGl9Wg"
      },
      "outputs": [],
      "source": [
        "def select_rows_by_index(embedding_vector, index_vector):\n",
        "    \"\"\"\n",
        "    Select rows from an embedding vector using an index vector.\n",
        "\n",
        "    Args:\n",
        "        embedding_vector (torch.Tensor): 2D tensor representing the embedding vector with shape [num_rows, embedding_size].\n",
        "        index_vector (torch.Tensor): 1D tensor representing the index vector.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: New tensor with selected rows from the embedding vector.\n",
        "    \"\"\"\n",
        "    # Use torch.index_select to select the desired rows\n",
        "    new_tensor = torch.index_select(embedding_vector, 0, index_vector)\n",
        "\n",
        "    return new_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "a12L7vEmmCJq"
      },
      "outputs": [],
      "source": [
        "def contrastive_loss(source_embeddings, target_embeddings, labels, margin=1.0):\n",
        "    \"\"\"\n",
        "    Computes the contrastive loss, a type of loss function used to train models in tasks like matching or similarity learning.\n",
        "\n",
        "    Args:\n",
        "        source_embeddings (torch.Tensor): Embeddings of the source graphs, shape [batch_size, embedding_size].\n",
        "        target_embeddings (torch.Tensor): Embeddings of the target graphs, shape [batch_size, embedding_size].\n",
        "        labels (torch.Tensor): Binary labels indicating if the pairs are matched (1) or not (0), shape [batch_size].\n",
        "        margin (float): Margin value for the contrastive loss. Defaults to 1.0.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The contrastive loss value.\n",
        "    \"\"\"\n",
        "    # Calculate the pairwise Euclidean distance between source and target embeddings\n",
        "    distances = F.pairwise_distance(source_embeddings, target_embeddings)\n",
        "\n",
        "    # Compute the contrastive loss:\n",
        "    # - For matched pairs (label == 1), the loss is the squared distance between embeddings.\n",
        "    # - For non-matched pairs (label == 0), the loss is based on how far apart the embeddings are,\n",
        "    #   but penalizes them only if the distance is less than the margin.\n",
        "    loss = torch.mean(\n",
        "        labels * 0.4 * distances.pow(2) +  # For positive pairs, minimize the distance (squared)\n",
        "        (1 - labels) * 0.4 * torch.max(torch.zeros_like(distances), margin - distances).pow(2)  # For negative pairs, maximize the distance (up to the margin)\n",
        "    )\n",
        "\n",
        "    return loss  # Return the computed contrastive loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_ggVYlTiO_WA"
      },
      "outputs": [],
      "source": [
        "def compute_mrr_and_hits(reference_file, predicted_file, output_file, k_values=[1, 5, 10]):\n",
        "    \"\"\"\n",
        "    Compute Mean Reciprocal Rank (MRR) and Hits@k metrics for ontology matching results.\n",
        "\n",
        "    Args:\n",
        "        reference_file (str): Path to the reference test candidate file (usually 'test.cands.tsv').\n",
        "        predicted_file (str): Path to the prediction results (with columns: SrcEntity, TgtEntity, Score).\n",
        "        output_file (str): Path to save ranked candidate predictions with scores.\n",
        "        k_values (list): List of integers specifying which Hits@k metrics to compute.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with MRR and Hits@k scores.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load reference candidate mappings: each row = (SrcEntity, CorrectTgtEntity, [CandidateTgtEntities])\n",
        "    test_candidate_mappings = read_table(reference_file).values.tolist()\n",
        "\n",
        "    # Load predictions and ensure Score is float\n",
        "    predicted_data = pd.read_csv(predicted_file, sep=\"\\t\")\n",
        "    predicted_data[\"Score\"] = predicted_data[\"Score\"].apply(\n",
        "        lambda x: float(x.strip(\"[]\")) if isinstance(x, str) else float(x)\n",
        "    )\n",
        "\n",
        "    # Create a dictionary mapping (SrcEntity, TgtEntity) -> predicted score\n",
        "    score_lookup = {\n",
        "        (row[\"SrcEntity\"], row[\"TgtEntity\"]): row[\"Score\"]\n",
        "        for _, row in predicted_data.iterrows()\n",
        "    }\n",
        "\n",
        "    ranking_results = []\n",
        "\n",
        "    # Rank the candidates for each source entity\n",
        "    for src_ref_class, tgt_ref_class, tgt_cands in test_candidate_mappings:\n",
        "        # Safely parse the candidate list (tgt_cands is a stringified list)\n",
        "        try:\n",
        "            tgt_cands = eval(tgt_cands)\n",
        "        except Exception:\n",
        "            tgt_cands = []\n",
        "\n",
        "        # Score each candidate (use a large negative default if not found)\n",
        "        scored_cands = [\n",
        "            (tgt_cand, score_lookup.get((src_ref_class, tgt_cand), -1e9))\n",
        "            for tgt_cand in tgt_cands\n",
        "        ]\n",
        "\n",
        "        # Sort candidates by score descending\n",
        "        scored_cands = sorted(scored_cands, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Store the ranking result\n",
        "        ranking_results.append((src_ref_class, tgt_ref_class, scored_cands))\n",
        "\n",
        "    # Save ranked predictions for inspection/debugging\n",
        "    pd.DataFrame(ranking_results, columns=[\"SrcEntity\", \"TgtEntity\", \"TgtCandidates\"]).to_csv(\n",
        "        output_file, sep=\"\\t\", index=False\n",
        "    )\n",
        "\n",
        "    # === Evaluation: compute MRR and Hits@k ===\n",
        "    total_entities = len(ranking_results)\n",
        "    reciprocal_ranks = []\n",
        "    hits_at_k = {k: 0 for k in k_values}\n",
        "\n",
        "    for src_entity, tgt_ref_class, tgt_cands in ranking_results:\n",
        "        ranked_candidates = [cand[0] for cand in tgt_cands]  # candidate URIs only\n",
        "        if tgt_ref_class in ranked_candidates:\n",
        "            rank = ranked_candidates.index(tgt_ref_class) + 1\n",
        "            reciprocal_ranks.append(1 / rank)\n",
        "            for k in k_values:\n",
        "                if rank <= k:\n",
        "                    hits_at_k[k] += 1\n",
        "        else:\n",
        "            reciprocal_ranks.append(0)  # No correct match in candidate list\n",
        "\n",
        "    # Compute final metrics\n",
        "    mrr = sum(reciprocal_ranks) / total_entities\n",
        "    hits_at_k = {k: hits / total_entities for k, hits in hits_at_k.items()}\n",
        "\n",
        "    return {\"MRR\": mrr, \"Hits@k\": hits_at_k}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "zmzBcuHZDOs3"
      },
      "outputs": [],
      "source": [
        "def save_gated_embeddings(gated_model, embeddings_src, x_src, embeddings_tgt, x_tgt,\n",
        "                          indexed_dict_src, indexed_dict_tgt,\n",
        "                          output_file_src, output_file_tgt):\n",
        "    \"\"\"\n",
        "    Compute and save the final entity embeddings generated by the GatedCombination model\n",
        "    for both source and target ontologies. Outputs include entity URIs and their final vectors.\n",
        "    Measures and prints the execution time of the entire operation.\n",
        "\n",
        "    Args:\n",
        "        gated_model (nn.Module): The trained GatedCombination model.\n",
        "        embeddings_src (Tensor): Structural embeddings for the source ontology.\n",
        "        x_src (Tensor): Semantic embeddings for the source ontology.\n",
        "        embeddings_tgt (Tensor): Structural embeddings for the target ontology.\n",
        "        x_tgt (Tensor): Semantic embeddings for the target ontology.\n",
        "        indexed_dict_src (dict): Index-to-URI mapping for the source ontology.\n",
        "        indexed_dict_tgt (dict): Index-to-URI mapping for the target ontology.\n",
        "        output_file_src (str): Path to save source embeddings (TSV).\n",
        "        output_file_tgt (str): Path to save target embeddings (TSV).\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    import torch\n",
        "    import time\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Use GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    gated_model = gated_model.to(device)\n",
        "    gated_model.eval()\n",
        "\n",
        "    # Move inputs to the same device\n",
        "    embeddings_src = embeddings_src.to(device)\n",
        "    x_src = x_src.to(device)\n",
        "    embeddings_tgt = embeddings_tgt.to(device)\n",
        "    x_tgt = x_tgt.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # === Source ontology ===\n",
        "        gate_src = torch.sigmoid(gated_model.gate_A_fc(embeddings_src))\n",
        "        final_src = embeddings_src * gate_src + x_src * (1 - gate_src)\n",
        "        final_src = final_src.cpu().numpy()\n",
        "\n",
        "        # === Target ontology ===\n",
        "        gate_tgt = torch.sigmoid(gated_model.gate_B_fc(embeddings_tgt))\n",
        "        final_tgt = embeddings_tgt * gate_tgt + x_tgt * (1 - gate_tgt)\n",
        "        final_tgt = final_tgt.cpu().numpy()\n",
        "\n",
        "    # Create DataFrames with Concept URI and embedding values\n",
        "    df_src = pd.DataFrame(final_src)\n",
        "    df_src.insert(0, \"Concept\", [indexed_dict_src[i] for i in range(len(df_src))])\n",
        "\n",
        "    df_tgt = pd.DataFrame(final_tgt)\n",
        "    df_tgt.insert(0, \"Concept\", [indexed_dict_tgt[i] for i in range(len(df_tgt))])\n",
        "\n",
        "    # Save embeddings to file\n",
        "    df_src.to_csv(output_file_src, sep='\\t', index=False)\n",
        "    df_tgt.to_csv(output_file_tgt, sep='\\t', index=False)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"✅ Gated embeddings saved:\\n- Source: {output_file_src}\\n- Target: {output_file_tgt}\")\n",
        "    print(f\"⏱️ Execution time: {elapsed_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_KZdtF46GHL4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def filter_ignored_class(src_emb_path, tgt_emb_path, src_onto, tgt_onto):\n",
        "    \"\"\"\n",
        "    Filters the source and target embedding files by removing concepts considered \"ignored classes\"\n",
        "    (e.g., owl:Thing, deprecated entities, etc.) based on both source and target ontologies.\n",
        "\n",
        "    Args:\n",
        "        src_emb_path (str): Path to the TSV file containing source embeddings with 'Concept' column.\n",
        "        tgt_emb_path (str): Path to the TSV file containing target embeddings with 'Concept' column.\n",
        "        src_onto (Ontology): Source ontology object loaded with DeepOnto.\n",
        "        tgt_onto (Ontology): Target ontology object loaded with DeepOnto.\n",
        "\n",
        "    Returns:\n",
        "        (str, str): Paths to the cleaned source and target embedding files.\n",
        "    \"\"\"\n",
        "\n",
        "    # === Load the embedding files ===\n",
        "    df_src = pd.read_csv(src_emb_path, sep='\\t', dtype=str)\n",
        "    print(f\"🔍 Initial source file: {len(df_src)} rows\")\n",
        "\n",
        "    df_tgt = pd.read_csv(tgt_emb_path, sep='\\t', dtype=str)\n",
        "    print(f\"🔍 Initial target file: {len(df_tgt)} rows\")\n",
        "\n",
        "    # === Step 1: Retrieve ignored classes from both ontologies ===\n",
        "    ignored_class_index = get_ignored_class_index(src_onto)  # e.g., owl:Thing, non-usable classes\n",
        "    ignored_class_index.update(get_ignored_class_index(tgt_onto))  # Merge with target ontology's ignored classes\n",
        "    ignored_uris = set(str(uri).strip() for uri in ignored_class_index)\n",
        "\n",
        "    # === Step 2: Remove rows where the 'Concept' column matches ignored URIs ===\n",
        "    df_src_cleaned = df_src[~df_src['Concept'].isin(ignored_uris)].reset_index(drop=True)\n",
        "    df_tgt_cleaned = df_tgt[~df_tgt['Concept'].isin(ignored_uris)].reset_index(drop=True)\n",
        "\n",
        "    print(f\"✅ Source after removing ignored classes: {len(df_src_cleaned)} rows\")\n",
        "    print(f\"✅ Target after removing ignored classes: {len(df_tgt_cleaned)} rows\")\n",
        "\n",
        "    # === Step 3: Save the cleaned embedding files ===\n",
        "    output_file_src = src_emb_path.replace(\".tsv\", \"_cleaned.tsv\")\n",
        "    output_file_tgt = tgt_emb_path.replace(\".tsv\", \"_cleaned.tsv\")\n",
        "\n",
        "    df_src_cleaned.to_csv(output_file_src, sep='\\t', index=False)\n",
        "    df_tgt_cleaned.to_csv(output_file_tgt, sep='\\t', index=False)\n",
        "\n",
        "    print(f\"📁 Cleaned source file saved to: {output_file_src}\")\n",
        "    print(f\"📁 Cleaned target file saved to: {output_file_tgt}\")\n",
        "\n",
        "    return output_file_src, output_file_tgt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "_9YDcnTbKaHk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "def encode_embeddings_with_concept_column(encoder_model, input_file, output_file):\n",
        "    \"\"\"\n",
        "    Applies an encoder model to a set of embeddings (while preserving the 'Concept' column),\n",
        "    and saves the encoded results in the same tabular format.\n",
        "\n",
        "    Args:\n",
        "        encoder_model: A PyTorch model (e.g., LinearEncoder, MLPEncoder, etc.)\n",
        "        input_file (str): Path to the input TSV file containing 'Concept' and embedding vectors.\n",
        "        output_file (str): Path to save the encoded embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    # Select device (GPU if available, else CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Move the encoder model to the selected device and set it to evaluation mode\n",
        "    encoder_model = encoder_model.to(device)\n",
        "    encoder_model.eval()\n",
        "\n",
        "    # Load the input TSV file containing concept URIs and embeddings\n",
        "    df = pd.read_csv(input_file, sep='\\t')\n",
        "\n",
        "    # Extract the 'Concept' column to preserve URIs\n",
        "    concepts = df['Concept'].tolist()\n",
        "\n",
        "    # Extract the numerical embedding values (excluding the 'Concept' column)\n",
        "    embedding_values = df.drop(columns=['Concept']).values\n",
        "\n",
        "    # Convert the embedding matrix into a PyTorch tensor and move to the device\n",
        "    embeddings = torch.FloatTensor(embedding_values).to(device)\n",
        "\n",
        "    # Pass the embeddings through the encoder model without computing gradients\n",
        "    with torch.no_grad():\n",
        "        encoded = encoder_model(embeddings).cpu().numpy()\n",
        "\n",
        "    # Reconstruct a new DataFrame with the encoded vectors and corresponding URIs\n",
        "    df_encoded = pd.DataFrame(encoded, columns=[f'dim_{i}' for i in range(encoded.shape[1])])\n",
        "    df_encoded.insert(0, \"Concept\", concepts)  # Re-insert the 'Concept' column at the first position\n",
        "\n",
        "    # Save the encoded embeddings to a TSV file\n",
        "    df_encoded.to_csv(output_file, sep='\\t', index=False)\n",
        "    print(f\"✅ Encoded embeddings saved to: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HigIe6n_lQ8X"
      },
      "source": [
        "# **FAISS Similarity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "zaX6JH9wj_WR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "import time\n",
        "\n",
        "def load_embeddings(src_emb_path, tgt_emb_path):\n",
        "    df_src = pd.read_csv(src_emb_path, sep='\\t')\n",
        "    df_tgt = pd.read_csv(tgt_emb_path, sep='\\t')\n",
        "    uris_src = df_src[\"Concept\"].values\n",
        "    uris_tgt = df_tgt[\"Concept\"].values\n",
        "    src_vecs = df_src.drop(columns=[\"Concept\"]).values.astype('float32')\n",
        "    tgt_vecs = df_tgt.drop(columns=[\"Concept\"]).values.astype('float32')\n",
        "    return uris_src, uris_tgt, src_vecs, tgt_vecs\n",
        "\n",
        "def save_results(uris_src, uris_tgt, indices, scores, output_file, top_k):\n",
        "    rows = []\n",
        "    for i, (ind_row, score_row) in enumerate(zip(indices, scores)):\n",
        "        src_uri = uris_src[i]\n",
        "        for j, tgt_idx in enumerate(ind_row):\n",
        "            tgt_uri = uris_tgt[tgt_idx]\n",
        "            score = score_row[j]\n",
        "            rows.append((src_uri, tgt_uri, score))\n",
        "    df_result = pd.DataFrame(rows, columns=[\"SrcEntity\", \"TgtEntity\", \"Score\"])\n",
        "    df_result.to_csv(output_file, sep='\\t', index=False)\n",
        "    print(f\"Top-{top_k} FAISS similarity results saved to: {output_file}\")\n",
        "\n",
        "def topk_faiss_l2(src_emb_path, tgt_emb_path, top_k=15, output_file=\"topk_l2.tsv\"):\n",
        "    print(\"🔹 Using L2 (Euclidean) distance with FAISS\")\n",
        "    start = time.time()\n",
        "\n",
        "    uris_src, uris_tgt, src_vecs, tgt_vecs = load_embeddings(src_emb_path, tgt_emb_path)\n",
        "    dim = src_vecs.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(tgt_vecs)\n",
        "    distances, indices = index.search(src_vecs, top_k)\n",
        "    similarity_scores = 1 / (1 + distances)\n",
        "\n",
        "    save_results(uris_src, uris_tgt, indices, similarity_scores, output_file, top_k)\n",
        "\n",
        "    print(f\"⏱️ Execution time: {time.time() - start:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjUYOFO7pdCg"
      },
      "source": [
        "# **Mappings Evaluation Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6m04nFw_R00"
      },
      "source": [
        "# **Precision, Recall, F1**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Strategy and Filtering Justification\n",
        "\n",
        "### Filtering Justification\n",
        "\n",
        "In the `evaluate_predictions` function, two important filtering steps are applied to ensure that the evaluation metrics (such as Precision, Recall, and F1-score) accurately reflect the model's performance:\n",
        "\n",
        "\n",
        "#### 1. Filtering Out Training-Only Entities\n",
        "\n",
        "We remove all predicted mappings involving source or target entities that are present **only in the training set** and not in the test set.\n",
        "\n",
        "This step is critical because:\n",
        "\n",
        "- In some datasets like **Bio-ML**, the same entity can appear in both training and test sets, although with **different correspondences**.\n",
        "- If we don't remove training-only entities, it can lead to **label leakage** and **metric distortion**.\n",
        "\n",
        "#### 2. Filtering on `SrcEntity` present in the test set\n",
        "\n",
        "The second step keeps only the predictions where the `SrcEntity` is included in the test reference set.\n",
        "\n",
        "- This eliminates **non-evaluable false positives**, i.e., predicted mappings for source entities that do not appear in the test set and therefore have no ground-truth correspondences. Including such predictions **unfairly penalizes precision and F1-score**, even though they are technically not verifiable errors.\n",
        "\n",
        "- It focuses the evaluation on entities with defined ground-truth mappings, which is critical for computing metrics such as :\n",
        "\n",
        "$P_{\\text{test}} = \\frac{|\\mathcal{M}_{\\text{out}} \\cap \\mathcal{M}_{\\text{test}}|}{|\\mathcal{M}_{\\text{out}} \\setminus (\\mathcal{M}_{\\text{ref}} \\setminus \\mathcal{M}_{\\text{test}})|}$.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "_GW0Am-TmVMR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "CdP6iYirLJW6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def evaluate_predictions(\n",
        "    topk_file,\n",
        "    train_file,\n",
        "    test_file,\n",
        "    src_onto,\n",
        "    tgt_onto,\n",
        "    threshold=0.0\n",
        "):\n",
        "    # === Step 1: Load input files ===\n",
        "    df = pd.read_csv(topk_file, sep='\\t', dtype=str)\n",
        "    train_df = pd.read_csv(train_file, sep=\"\\t\", dtype=str)\n",
        "    test_df = pd.read_csv(test_file, sep=\"\\t\", dtype=str)\n",
        "\n",
        "    # === Step 2: Remove URIs only present in training set ===\n",
        "    train_uris = set(train_df['SrcEntity']) | set(train_df['TgtEntity'])\n",
        "    test_uris = set(test_df['SrcEntity']) | set(test_df['TgtEntity'])\n",
        "    uris_to_exclude = train_uris - test_uris\n",
        "    df = df[~(df['SrcEntity'].isin(uris_to_exclude) | df['TgtEntity'].isin(uris_to_exclude))].reset_index(drop=True)\n",
        "\n",
        "    # === Step 3: Keep only source entities from the test set ===\n",
        "    src_entities_test = set(test_df['SrcEntity'])\n",
        "    df = df[df['SrcEntity'].isin(src_entities_test)].reset_index(drop=True)\n",
        "\n",
        "    # === Step 4: Save filtered Top-K predictions ===\n",
        "    output_file1 = topk_file.replace(\".tsv\", \"_filtered.tsv\")\n",
        "    df.to_csv(output_file1, sep='\\t', index=False)\n",
        "\n",
        "    # === Step 5: Convert score column to float\n",
        "    df['Score'] = df['Score'].apply(lambda x: float(x.strip(\"[]\")) if isinstance(x, str) else float(x))\n",
        "    df_sorted = df.sort_values(by='Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # === Step 6: Apply 1-1 constraint (greedy matching)\n",
        "    matched_sources = set()\n",
        "    matched_targets = set()\n",
        "    result = []\n",
        "\n",
        "    for _, row in df_sorted.iterrows():\n",
        "        src, tgt, score = row['SrcEntity'], row['TgtEntity'], row['Score']\n",
        "        if src not in matched_sources and tgt not in matched_targets and score >= threshold:\n",
        "            result.append((src, tgt, score))\n",
        "            matched_sources.add(src)\n",
        "            matched_targets.add(tgt)\n",
        "\n",
        "    matching_results_df = pd.DataFrame(result, columns=['SrcEntity', 'TgtEntity', 'Score'])\n",
        "\n",
        "    # === Step 7: Save Top-1 predictions ===\n",
        "    output_file2 = topk_file.replace(\".tsv\", f\"_predictions.tsv\")\n",
        "    matching_results_df.to_csv(output_file2, sep='\\t', index=False)\n",
        "    print(f\"📁 Top-1 file saved: {output_file2}\")\n",
        "    print(f\"📌 Number of predictions in output: {len(matching_results_df)}\")\n",
        "\n",
        "    # === Step 8: Evaluate predictions ===\n",
        "    preds = EntityMapping.read_table_mappings(output_file2)\n",
        "    refs = ReferenceMapping.read_table_mappings(test_file)\n",
        "    results = AlignmentEvaluator.f1(preds, refs)\n",
        "\n",
        "    preds_set = {p.to_tuple() for p in preds}\n",
        "    refs_set = {r.to_tuple() for r in refs}\n",
        "    correct = len(preds_set & refs_set)\n",
        "\n",
        "    print(f\"🎯 Correct mappings (Top-1): {correct}\")\n",
        "    print(f\"📊 Evaluation (P / R / F1): {results}\")\n",
        "\n",
        "    return output_file2, results, correct\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPuzmu6f_Y8W"
      },
      "source": [
        "# **Precision@k, Recall@k, F1@k**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "UwdxR-ZzAgS3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "def evaluate_topk(topk_file, train_file, test_file, k=1, threshold=0.0):\n",
        "    \"\"\"\n",
        "    Evaluate Top-K predictions using Precision, Recall, and F1-score,\n",
        "    after filtering out training-only URIs, keeping only test sources, and applying 1-1 constraint.\n",
        "\n",
        "    Args:\n",
        "        topk_file (str): Path to the top-k prediction file (TSV with SrcEntity, TgtEntity, Score)\n",
        "        train_file (str): Path to the training mappings file (TSV)\n",
        "        test_file (str): Path to the test mappings file (TSV)\n",
        "        k (int): Value of K for top-k evaluation\n",
        "        threshold (float): Minimum score to consider a prediction valid\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing Precision@K, Recall@K, and F1@K\n",
        "    \"\"\"\n",
        "\n",
        "    # === Step 1: Load input files ===\n",
        "    df = pd.read_csv(topk_file, sep='\\t', dtype=str)\n",
        "    train_df = pd.read_csv(train_file, sep='\\t', dtype=str)\n",
        "    test_df = pd.read_csv(test_file, sep='\\t', dtype=str)\n",
        "\n",
        "    # === Step 2: Remove URIs only present in the training set ===\n",
        "    train_uris = set(train_df['SrcEntity']) | set(train_df['TgtEntity'])\n",
        "    test_uris = set(test_df['SrcEntity']) | set(test_df['TgtEntity'])\n",
        "    uris_to_exclude = train_uris - test_uris\n",
        "    df = df[~(df['SrcEntity'].isin(uris_to_exclude) | df['TgtEntity'].isin(uris_to_exclude))].reset_index(drop=True)\n",
        "\n",
        "    # === Step 3: Keep only source entities from the test set ===\n",
        "    src_entities_test = set(test_df['SrcEntity'])\n",
        "    df = df[df['SrcEntity'].isin(src_entities_test)].reset_index(drop=True)\n",
        "\n",
        "    # === Step 4: Convert score column to float and sort ===\n",
        "    df['Score'] = df['Score'].apply(lambda x: float(x.strip(\"[]\")) if isinstance(x, str) else float(x))\n",
        "    df_sorted = df.sort_values(by='Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # === Step 5: Apply 1-to-1 constraint (greedy strategy with optional threshold)\n",
        "    matched_sources = set()\n",
        "    matched_targets = set()\n",
        "    result = []\n",
        "\n",
        "    for _, row in df_sorted.iterrows():\n",
        "        src, tgt, score = row['SrcEntity'], row['TgtEntity'], row['Score']\n",
        "        if src not in matched_sources and tgt not in matched_targets and score >= threshold:\n",
        "            result.append((src, tgt, score))\n",
        "            matched_sources.add(src)\n",
        "            matched_targets.add(tgt)\n",
        "\n",
        "    # === Step 6: Create and save Top-1 prediction dataframe\n",
        "    matching_results_df = pd.DataFrame(result, columns=['SrcEntity', 'TgtEntity', 'Score'])\n",
        "    output_file = topk_file.replace(\".tsv\", \"_predictions.tsv\")\n",
        "    matching_results_df.to_csv(output_file, sep='\\t', index=False)\n",
        "    print(f\"📁 Top-1 file saved: {output_file}\")\n",
        "    print(f\"📌 Number of predictions in output: {len(matching_results_df)}\")\n",
        "\n",
        "    # === Step 7: Build reference dictionary from test set\n",
        "    ref_dict = defaultdict(set)\n",
        "    for _, row in test_df.iterrows():\n",
        "        ref_dict[row['SrcEntity']].add(row['TgtEntity'])\n",
        "\n",
        "    # === Step 8: Select Top-K from the 1-1 results\n",
        "    matching_results_df['Score'] = matching_results_df['Score'].astype(float)\n",
        "    topk_df = matching_results_df.sort_values(by='Score', ascending=False).groupby('SrcEntity').head(k)\n",
        "\n",
        "    # === Step 9: Compute metrics\n",
        "    total_tp = total_pred = total_ref = 0\n",
        "\n",
        "    for src, group in topk_df.groupby('SrcEntity'):\n",
        "        predicted = set(group['TgtEntity'])\n",
        "        true = ref_dict.get(src, set())\n",
        "        tp = len(predicted & true)\n",
        "        total_tp += tp\n",
        "        total_pred += len(predicted)\n",
        "        total_ref += len(true)\n",
        "\n",
        "    precision = total_tp / total_pred if total_pred else 0.0\n",
        "    recall = total_tp / total_ref if total_ref else 0.0\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-8) if precision + recall > 0 else 0.0\n",
        "\n",
        "    return {\n",
        "        f'Precision@{k}': round(precision, 4),\n",
        "        f'Recall@{k}': round(recall, 4),\n",
        "        f'F1@{k}': round(f1, 4)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "WAsAVJEy3o9a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "def encode_embeddings_with_concept_column(encoder_model, input_file, output_file):\n",
        "    \"\"\"\n",
        "    Applies an encoder model to a set of embeddings (while preserving the 'Concept' column),\n",
        "    and saves the encoded results in the same tabular format.\n",
        "\n",
        "    Args:\n",
        "        encoder_model: A PyTorch model (e.g., LinearEncoder, MLPEncoder, etc.)\n",
        "        input_file (str): Path to the input TSV file containing 'Concept' and embedding vectors.\n",
        "        output_file (str): Path to save the encoded embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    # Select device (GPU if available, else CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Move the encoder model to the selected device and set it to evaluation mode\n",
        "    encoder_model = encoder_model.to(device)\n",
        "    encoder_model.eval()\n",
        "\n",
        "    # Load the input TSV file containing concept URIs and embeddings\n",
        "    df = pd.read_csv(input_file, sep='\\t')\n",
        "\n",
        "    # Extract the 'Concept' column to preserve URIs\n",
        "    concepts = df['Concept'].tolist()\n",
        "\n",
        "    # Extract the numerical embedding values (excluding the 'Concept' column)\n",
        "    embedding_values = df.drop(columns=['Concept']).values\n",
        "\n",
        "    # Convert the embedding matrix into a PyTorch tensor and move to the device\n",
        "    embeddings = torch.FloatTensor(embedding_values).to(device)\n",
        "\n",
        "    # Pass the embeddings through the encoder model without computing gradients\n",
        "    with torch.no_grad():\n",
        "        encoded = encoder_model(embeddings).cpu().numpy()\n",
        "\n",
        "    # Reconstruct a new DataFrame with the encoded vectors and corresponding URIs\n",
        "    df_encoded = pd.DataFrame(encoded, columns=[f'dim_{i}' for i in range(encoded.shape[1])])\n",
        "    df_encoded.insert(0, \"Concept\", concepts)  # Re-insert the 'Concept' column at the first position\n",
        "\n",
        "    # Save the encoded embeddings to a TSV file\n",
        "    df_encoded.to_csv(output_file, sep='\\t', index=False)\n",
        "    print(f\"✅ Encoded embeddings saved to: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HyWMsw1MVhz"
      },
      "source": [
        "# **Main Code**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC37FlwGDqGM"
      },
      "source": [
        "\n",
        "\n",
        "# Reading semantic node embeddings provided by the ENE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "FuEfSnw5mod0"
      },
      "outputs": [],
      "source": [
        "# Read the source embeddings from a CSV file into a pandas DataFrame\n",
        "df_embbedings_src = pd.read_csv(src_Emb, index_col=0)\n",
        "\n",
        "# Convert the DataFrame to a NumPy array, which will remove the index and store the data as a raw matrix\n",
        "numpy_array = df_embbedings_src.to_numpy()\n",
        "\n",
        "# Convert the NumPy array into a PyTorch FloatTensor, which is the format required for PyTorch operations\n",
        "x_src = torch.FloatTensor(numpy_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "STUwqMUXmlG2"
      },
      "outputs": [],
      "source": [
        "# Read the target embeddings from a CSV file into a pandas DataFrame\n",
        "df_embbedings_tgt = pd.read_csv(tgt_Emb, index_col=0)\n",
        "\n",
        "# Convert the DataFrame to a NumPy array, which removes the index and converts the data to a raw matrix\n",
        "numpy_array = df_embbedings_tgt.to_numpy()\n",
        "\n",
        "# Convert the NumPy array into a PyTorch FloatTensor, which is required for PyTorch operations\n",
        "x_tgt = torch.FloatTensor(numpy_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZIu9P08DqGN"
      },
      "source": [
        "# Reading adjacency Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "pH69Up40mycz"
      },
      "outputs": [],
      "source": [
        "# Read the source adjacency matrix from a CSV file into a pandas DataFrame\n",
        "df_ma1 = pd.read_csv(src_Adjacence, index_col=0)\n",
        "\n",
        "# Convert the DataFrame to a list of lists (Python native list format)\n",
        "ma1 = df_ma1.values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "hYCmAO5Ymzpl"
      },
      "outputs": [],
      "source": [
        "# Read the target adjacency matrix from a CSV file into a pandas DataFrame\n",
        "df_ma2 = pd.read_csv(tgt_Adjacence, index_col=0)\n",
        "\n",
        "# Convert the DataFrame to a list of lists (Python native list format)\n",
        "ma2 = df_ma2.values.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSyksqh3TrU-"
      },
      "source": [
        "# Convert Adjacency matrix (in list format) to an undirected edge index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "uVt-Pce5m5ll"
      },
      "outputs": [],
      "source": [
        "# Convert the source adjacency matrix (in list format) to an undirected edge index for PyTorch Geometric\n",
        "edge_src = adjacency_matrix_to_undirected_edge_index(ma1)\n",
        "\n",
        "# Convert the target adjacency matrix (in list format) to an undirected edge index for PyTorch Geometric\n",
        "edge_tgt = adjacency_matrix_to_undirected_edge_index(ma2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9wMTRdqT4aY"
      },
      "source": [
        "# GIT Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "eqiEKCLSMVh3"
      },
      "outputs": [],
      "source": [
        "def train_model_gnn(model, x_src, edge_src, x_tgt, edge_tgt,\n",
        "                    tensor_term1, tensor_term2, tensor_score,\n",
        "                    learning_rate, weight_decay_value, num_epochs, print_interval=10):\n",
        "    \"\"\"\n",
        "    Trains a graph neural network (GNN) model using source and target embeddings and contrastive loss.\n",
        "\n",
        "    Args:\n",
        "        model: The GNN model to be trained.\n",
        "        x_src (torch.Tensor): Source node embeddings.\n",
        "        edge_src (torch.Tensor): Source graph edges.\n",
        "        x_tgt (torch.Tensor): Target node embeddings.\n",
        "        edge_tgt (torch.Tensor): Target graph edges.\n",
        "        tensor_term1 (torch.Tensor): Indices of the source nodes to be compared.\n",
        "        tensor_term2 (torch.Tensor): Indices of the target nodes to be compared.\n",
        "        tensor_score (torch.Tensor): Labels indicating if the pairs are matched (1) or not (0).\n",
        "        learning_rate (float): Learning rate for the optimizer.\n",
        "        weight_decay_value (float): Weight decay (L2 regularization) value for the optimizer.\n",
        "        num_epochs (int): Number of epochs for training.\n",
        "        print_interval (int): Interval at which training progress is printed (every `print_interval` epochs).\n",
        "\n",
        "    Returns:\n",
        "        model: The trained GNN model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Set device (GPU or CPU) for computation\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Step 2: Move the model and all inputs to the selected device\n",
        "    model.to(device)\n",
        "    x_tgt = x_tgt.to(device)               # Target node embeddings\n",
        "    edge_tgt = edge_tgt.to(device)         # Target graph edges\n",
        "    x_src = x_src.to(device)               # Source node embeddings\n",
        "    edge_src = edge_src.to(device)         # Source graph edges\n",
        "    tensor_term1 = tensor_term1.to(device) # Indices for source nodes\n",
        "    tensor_term2 = tensor_term2.to(device) # Indices for target nodes\n",
        "    tensor_score = tensor_score.to(device) # Ground truth labels\n",
        "\n",
        "    # Step 3: Define optimizer with learning rate and regularization\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay_value)\n",
        "\n",
        "    # Step 4: Initialize list to store training losses\n",
        "    train_losses = []\n",
        "\n",
        "    # Record the start time of training\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Step 5: Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        # Zero out gradients from the previous iteration\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: Compute embeddings for source and target graphs\n",
        "        out1 = model(x_src, edge_src)  # Updated source embeddings\n",
        "        out2 = model(x_tgt, edge_tgt)  # Updated target embeddings\n",
        "\n",
        "        # Extract specific rows of embeddings for terms being compared\n",
        "        src_embeddings = select_rows_by_index(out1, tensor_term1)\n",
        "        tgt_embeddings = select_rows_by_index(out2, tensor_term2)\n",
        "\n",
        "        # Compute contrastive loss based on the embeddings and ground truth labels\n",
        "        loss = contrastive_loss(src_embeddings, tgt_embeddings, tensor_score)\n",
        "\n",
        "        # Backward pass: Compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the model's parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Append the loss for this iteration to the list\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        # Print loss every `print_interval` epochs\n",
        "        if (epoch + 1) % print_interval == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item()}\")\n",
        "\n",
        "    # Step 6: Record end time of training\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Step 7: Plot the training loss over time\n",
        "    plt.semilogy(range(1, num_epochs + 1), train_losses, label=\"Training Loss\", marker='o')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Print the total training time\n",
        "    training_time = end_time - start_time\n",
        "    print(f\"Training complete! Total training time: {training_time:.2f} seconds\")\n",
        "\n",
        "    # Step 8: Return the trained model\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "6_tzUG_emtBg"
      },
      "outputs": [],
      "source": [
        "# Initialize the GIT_mod model with the dimensionality of the target embeddings\n",
        "# The first argument is the dimensionality of the target node embeddings (x_tgt.shape[1])\n",
        "# The second argument (1) represents the number of RGIT layers in the model\n",
        "GIT_model = RGIT_mod(x_tgt.shape[1], 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "wVo-s7UQssSp"
      },
      "outputs": [],
      "source": [
        "# Reading the training pairs from a CSV file into a pandas DataFrame\n",
        "df_embbedings = pd.read_csv(train_file, index_col=0)\n",
        "\n",
        "# Extract the 'SrcEntity' and 'TgtEntity' columns as NumPy arrays and convert them to integers\n",
        "tensor_term1 = df_embbedings['SrcEntity'].values.astype(int)  # Source entity indices\n",
        "tensor_term2 = df_embbedings['TgtEntity'].values.astype(int)  # Target entity indices\n",
        "\n",
        "# Extract the 'Score' column as a NumPy array and convert it to floats\n",
        "tensor_score = df_embbedings['Score'].values.astype(float)  # Scores (labels) indicating if pairs match (1) or not (0)\n",
        "\n",
        "# Convert the NumPy arrays to PyTorch LongTensors (for indices) and FloatTensors (for scores)\n",
        "tensor_term1_o = torch.from_numpy(tensor_term1).type(torch.LongTensor)  # Source entity tensor\n",
        "tensor_term2_o = torch.from_numpy(tensor_term2).type(torch.LongTensor)  # Target entity tensor\n",
        "tensor_score_o = torch.from_numpy(tensor_score).type(torch.FloatTensor)  # Score tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "agHlFNesMVh3",
        "outputId": "2a8527ff-deaf-42ca-cad4-1979bf1f8b50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Training Loss: 0.0032047706190496683\n",
            "Epoch [20/1000], Training Loss: 0.002626564586535096\n",
            "Epoch [30/1000], Training Loss: 0.0022905750665813684\n",
            "Epoch [40/1000], Training Loss: 0.0020647665951400995\n",
            "Epoch [50/1000], Training Loss: 0.0018999767489731312\n",
            "Epoch [60/1000], Training Loss: 0.00177058856934309\n",
            "Epoch [70/1000], Training Loss: 0.0016651402693241835\n",
            "Epoch [80/1000], Training Loss: 0.001576801179908216\n",
            "Epoch [90/1000], Training Loss: 0.0015017781406641006\n",
            "Epoch [100/1000], Training Loss: 0.0014364885864779353\n",
            "Epoch [110/1000], Training Loss: 0.0013793713878840208\n",
            "Epoch [120/1000], Training Loss: 0.0013285937020555139\n",
            "Epoch [130/1000], Training Loss: 0.001282808487303555\n",
            "Epoch [140/1000], Training Loss: 0.0012413120130077004\n",
            "Epoch [150/1000], Training Loss: 0.0012033110251650214\n",
            "Epoch [160/1000], Training Loss: 0.0011681661708280444\n",
            "Epoch [170/1000], Training Loss: 0.0011352134170010686\n",
            "Epoch [180/1000], Training Loss: 0.0011042420519515872\n",
            "Epoch [190/1000], Training Loss: 0.0010748117929324508\n",
            "Epoch [200/1000], Training Loss: 0.0010471055284142494\n",
            "Epoch [210/1000], Training Loss: 0.0010206598090007901\n",
            "Epoch [220/1000], Training Loss: 0.000995331327430904\n",
            "Epoch [230/1000], Training Loss: 0.000970994122326374\n",
            "Epoch [240/1000], Training Loss: 0.0009474453399889171\n",
            "Epoch [250/1000], Training Loss: 0.0009249716531485319\n",
            "Epoch [260/1000], Training Loss: 0.0009032507659867406\n",
            "Epoch [270/1000], Training Loss: 0.0008824295946396887\n",
            "Epoch [280/1000], Training Loss: 0.0008624215261079371\n",
            "Epoch [290/1000], Training Loss: 0.0008431112510152161\n",
            "Epoch [300/1000], Training Loss: 0.0008245177450589836\n",
            "Epoch [310/1000], Training Loss: 0.0008065596339292824\n",
            "Epoch [320/1000], Training Loss: 0.0007893663132563233\n",
            "Epoch [330/1000], Training Loss: 0.0007728265482001007\n",
            "Epoch [340/1000], Training Loss: 0.0007570071611553431\n",
            "Epoch [350/1000], Training Loss: 0.000742665899451822\n",
            "Epoch [360/1000], Training Loss: 0.0007280596764758229\n",
            "Epoch [370/1000], Training Loss: 0.0007131835445761681\n",
            "Epoch [380/1000], Training Loss: 0.000700197706464678\n",
            "Epoch [390/1000], Training Loss: 0.0006878127460367978\n",
            "Epoch [400/1000], Training Loss: 0.0006753797642886639\n",
            "Epoch [410/1000], Training Loss: 0.0006628403789363801\n",
            "Epoch [420/1000], Training Loss: 0.0006523121846839786\n",
            "Epoch [430/1000], Training Loss: 0.0006419999990612268\n",
            "Epoch [440/1000], Training Loss: 0.0006332097109407187\n",
            "Epoch [450/1000], Training Loss: 0.0006218786584213376\n",
            "Epoch [460/1000], Training Loss: 0.0006109353853389621\n",
            "Epoch [470/1000], Training Loss: 0.0006005642935633659\n",
            "Epoch [480/1000], Training Loss: 0.0005915915826335549\n",
            "Epoch [490/1000], Training Loss: 0.0005845640553161502\n",
            "Epoch [500/1000], Training Loss: 0.0005749117699451745\n",
            "Epoch [510/1000], Training Loss: 0.0005661904579028487\n",
            "Epoch [520/1000], Training Loss: 0.0005604663165286183\n",
            "Epoch [530/1000], Training Loss: 0.0005498763057403266\n",
            "Epoch [540/1000], Training Loss: 0.0005543265142478049\n",
            "Epoch [550/1000], Training Loss: 0.000537174753844738\n",
            "Epoch [560/1000], Training Loss: 0.0005301296478137374\n",
            "Epoch [570/1000], Training Loss: 0.000523528375197202\n",
            "Epoch [580/1000], Training Loss: 0.0005177634302526712\n",
            "Epoch [590/1000], Training Loss: 0.0005123543669469655\n",
            "Epoch [600/1000], Training Loss: 0.0005044255522079766\n",
            "Epoch [610/1000], Training Loss: 0.0004977193311788142\n",
            "Epoch [620/1000], Training Loss: 0.0004943913081660867\n",
            "Epoch [630/1000], Training Loss: 0.0004892066936008632\n",
            "Epoch [640/1000], Training Loss: 0.00048041032277978957\n",
            "Epoch [650/1000], Training Loss: 0.0004751495725940913\n",
            "Epoch [660/1000], Training Loss: 0.00047200711560435593\n",
            "Epoch [670/1000], Training Loss: 0.00046306842705234885\n",
            "Epoch [680/1000], Training Loss: 0.0004582548572216183\n",
            "Epoch [690/1000], Training Loss: 0.0004554541374091059\n",
            "Epoch [700/1000], Training Loss: 0.0004483989323489368\n",
            "Epoch [710/1000], Training Loss: 0.000443916826043278\n",
            "Epoch [720/1000], Training Loss: 0.000438939401647076\n",
            "Epoch [730/1000], Training Loss: 0.0004334177647251636\n",
            "Epoch [740/1000], Training Loss: 0.00044151596375741065\n",
            "Epoch [750/1000], Training Loss: 0.00042795014451257885\n",
            "Epoch [760/1000], Training Loss: 0.00042282999493181705\n",
            "Epoch [770/1000], Training Loss: 0.0004179390671197325\n",
            "Epoch [780/1000], Training Loss: 0.0004165011050645262\n",
            "Epoch [790/1000], Training Loss: 0.00041202668217010796\n",
            "Epoch [800/1000], Training Loss: 0.00040953056304715574\n",
            "Epoch [810/1000], Training Loss: 0.0004090755828656256\n",
            "Epoch [820/1000], Training Loss: 0.00040308787720277905\n",
            "Epoch [830/1000], Training Loss: 0.00039904032018966973\n",
            "Epoch [840/1000], Training Loss: 0.00039577396819368005\n",
            "Epoch [850/1000], Training Loss: 0.000397372612496838\n",
            "Epoch [860/1000], Training Loss: 0.0003911383973900229\n",
            "Epoch [870/1000], Training Loss: 0.00038957857759669423\n",
            "Epoch [880/1000], Training Loss: 0.0003860814031213522\n",
            "Epoch [890/1000], Training Loss: 0.0003865321050398052\n",
            "Epoch [900/1000], Training Loss: 0.0003822872240561992\n",
            "Epoch [910/1000], Training Loss: 0.000377724994905293\n",
            "Epoch [920/1000], Training Loss: 0.0003805452724918723\n",
            "Epoch [930/1000], Training Loss: 0.00037529735709540546\n",
            "Epoch [940/1000], Training Loss: 0.00037159054772928357\n",
            "Epoch [950/1000], Training Loss: 0.00036702060606330633\n",
            "Epoch [960/1000], Training Loss: 0.0003647022822406143\n",
            "Epoch [970/1000], Training Loss: 0.0003642396186478436\n",
            "Epoch [980/1000], Training Loss: 0.00036028516478836536\n",
            "Epoch [990/1000], Training Loss: 0.00035532150650396943\n",
            "Epoch [1000/1000], Training Loss: 0.00035755106364376843\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete! Total training time: 609.52 seconds\n"
          ]
        }
      ],
      "source": [
        "# Train the GNN model using the provided source and target graph embeddings, edges, and training data\n",
        "trained_model = train_model_gnn(\n",
        "    model=GIT_model,                # The GNN model to be trained (initialized earlier)\n",
        "    x_src=x_src,                    # Source node embeddings (tensor for source graph)\n",
        "    edge_src=edge_src,              # Source graph edges (undirected edge index for source graph)\n",
        "    x_tgt=x_tgt,                    # Target node embeddings (tensor for target graph)\n",
        "    edge_tgt=edge_tgt,              # Target graph edges (undirected edge index for target graph)\n",
        "    tensor_term1=tensor_term1_o,    # Indices of source entities for training\n",
        "    tensor_term2=tensor_term2_o,    # Indices of target entities for training\n",
        "    tensor_score=tensor_score_o,    # Scores (labels) indicating if pairs match (1) or not (0)\n",
        "    learning_rate=0.0001,            # Learning rate for the Adam optimizer\n",
        "    weight_decay_value=1e-4,        # Weight decay for L2 regularization to prevent overfitting\n",
        "    num_epochs=1000,                # Number of training epochs\n",
        "    print_interval=10               # Interval at which to print training progress (every 10 epochs)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYyhvjcTUaae"
      },
      "source": [
        "# GIT Application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "BZ_VqP6tq6iD"
      },
      "outputs": [],
      "source": [
        "# Determine if a GPU is available and move the computations to it; otherwise, use the CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Assuming the model has been trained and hyperparameters (x_src, edge_src, x_tgt, edge_tgt) are set\n",
        "\n",
        "# Move the trained GIT_model to the device (GPU or CPU)\n",
        "GIT_model.to(device)\n",
        "\n",
        "# Move the data tensors to the same device (GPU or CPU)\n",
        "x_tgt = x_tgt.to(device)         # Target node embeddings\n",
        "edge_tgt = edge_tgt.to(device)   # Target graph edges\n",
        "x_src = x_src.to(device)         # Source node embeddings\n",
        "edge_src = edge_src.to(device)   # Source graph edges\n",
        "\n",
        "# Set the model to evaluation mode; this disables dropout and batch normalization\n",
        "GIT_model.eval()\n",
        "\n",
        "# Pass the source and target embeddings through the trained GNN model to update the embeddings\n",
        "with torch.no_grad():  # Disable gradient computation (inference mode)\n",
        "    embeddings_tgt = GIT_model(x_tgt, edge_tgt)  # Get updated embeddings for the target graph\n",
        "    embeddings_src = GIT_model(x_src, edge_src)  # Get updated embeddings for the source graph\n",
        "\n",
        "# Detach the embeddings from the computation graph and move them back to the CPU\n",
        "# This step is useful if you need to use the embeddings for tasks outside PyTorch (e.g., saving to disk)\n",
        "embeddings_tgt = embeddings_tgt.detach().cpu()  # Target graph embeddings\n",
        "embeddings_src = embeddings_src.detach().cpu()  # Source graph embeddings\n",
        "\n",
        "# At this point, embeddings_tgt and embeddings_src contain the updated embeddings, ready for downstream tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Og5fdoGJrTCG"
      },
      "source": [
        "# Selecting embedding pairs to train the Gated Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "J0nTwc-dnjLn"
      },
      "outputs": [],
      "source": [
        "# Read the training pairs from a CSV file into a pandas DataFrame\n",
        "df_embeddings = pd.read_csv(train_file, index_col=0)\n",
        "\n",
        "# Extract columns and convert to NumPy arrays\n",
        "tensor_term1 = df_embeddings['SrcEntity'].values.astype(int)  # Source entity indices\n",
        "tensor_term2 = df_embeddings['TgtEntity'].values.astype(int)  # Target entity indices\n",
        "tensor_score = df_embeddings['Score'].values.astype(float)  # Matching scores\n",
        "\n",
        "# Split data into training and validation sets\n",
        "tensor_term1_train, tensor_term1_val, tensor_term2_train, tensor_term2_val, tensor_score_train, tensor_score_val = train_test_split(\n",
        "    tensor_term1, tensor_term2, tensor_score, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Convert split data to PyTorch tensors\n",
        "tensor_term1_train = torch.from_numpy(tensor_term1_train).type(torch.LongTensor)\n",
        "tensor_term2_train = torch.from_numpy(tensor_term2_train).type(torch.LongTensor)\n",
        "tensor_score_train = torch.from_numpy(tensor_score_train).type(torch.FloatTensor)\n",
        "\n",
        "tensor_term1_val = torch.from_numpy(tensor_term1_val).type(torch.LongTensor)\n",
        "tensor_term2_val = torch.from_numpy(tensor_term2_val).type(torch.LongTensor)\n",
        "tensor_score_val = torch.from_numpy(tensor_score_val).type(torch.FloatTensor)\n",
        "\n",
        "# Move the embeddings back to the CPU if not already there\n",
        "x_tgt = x_tgt.cpu()  # Target node embeddings\n",
        "x_src = x_src.cpu()  # Source node embeddings\n",
        "\n",
        "# Select embeddings for the training set\n",
        "X1_train = select_rows_by_index(embeddings_src, tensor_term1_train)\n",
        "X2_train = select_rows_by_index(x_src, tensor_term1_train)\n",
        "X3_train = select_rows_by_index(embeddings_tgt, tensor_term2_train)\n",
        "X4_train = select_rows_by_index(x_tgt, tensor_term2_train)\n",
        "\n",
        "# Select embeddings for the validation set\n",
        "X1_val = select_rows_by_index(embeddings_src, tensor_term1_val)\n",
        "X2_val = select_rows_by_index(x_src, tensor_term1_val)\n",
        "X3_val = select_rows_by_index(embeddings_tgt, tensor_term2_val)\n",
        "X4_val = select_rows_by_index(x_tgt, tensor_term2_val)\n",
        "\n",
        "# Now you have:\n",
        "# - Training tensors: X1_train, X2_train, X3_train, X4_train, tensor_score_train\n",
        "# - Validation tensors: X1_val, X2_val, X3_val, X4_val, tensor_score_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNdCgaTMExPK"
      },
      "source": [
        "# Gated Network Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Gof1eIPIWSVU"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def train_gated_combination_model(X1_t, X2_t, X3_t, X4_t, tensor_score_o,\n",
        "                                  X1_val, X2_val, X3_val, X4_val, tensor_score_val,\n",
        "                                  epochs=120, batch_size=32, learning_rate=0.001, weight_decay=1e-5):\n",
        "    \"\"\"\n",
        "    Trains the GatedCombination model with training and validation data, using ReduceLROnPlateau scheduler.\n",
        "    Also calculates and displays F1-score during training and validation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create datasets and DataLoaders\n",
        "    train_dataset = TensorDataset(X1_t, X2_t, X3_t, X4_t, tensor_score_o)\n",
        "    val_dataset = TensorDataset(X1_val, X2_val, X3_val, X4_val, tensor_score_val)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = GatedCombinationWithFaiss(X1_t.shape[1]).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # Use ReduceLROnPlateau scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0.0\n",
        "        y_true_train, y_pred_train = [], []\n",
        "\n",
        "        for batch_X1, batch_X2, batch_X3, batch_X4, batch_y in train_loader:\n",
        "            batch_X1, batch_X2, batch_X3, batch_X4, batch_y = (\n",
        "                batch_X1.to(device),\n",
        "                batch_X2.to(device),\n",
        "                batch_X3.to(device),\n",
        "                batch_X4.to(device),\n",
        "                batch_y.to(device),\n",
        "            )\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(batch_X1, batch_X2, batch_X3, batch_X4)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = F.binary_cross_entropy(outputs, batch_y.unsqueeze(1).float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            # Store true labels and predictions for F1-score\n",
        "            y_true_train.extend(batch_y.cpu().numpy())\n",
        "            y_pred_train.extend((outputs > 0.5).float().cpu().numpy())\n",
        "\n",
        "        train_loss = total_train_loss / len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Calculate F1-score for training\n",
        "        train_f1 = f1_score(y_true_train, y_pred_train)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        total_val_loss = 0.0\n",
        "        y_true_val, y_pred_val = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_X1, batch_X2, batch_X3, batch_X4, batch_y in val_loader:\n",
        "                batch_X1, batch_X2, batch_X3, batch_X4, batch_y = (\n",
        "                    batch_X1.to(device),\n",
        "                    batch_X2.to(device),\n",
        "                    batch_X3.to(device),\n",
        "                    batch_X4.to(device),\n",
        "                    batch_y.to(device),\n",
        "                )\n",
        "                outputs = model(batch_X1, batch_X2, batch_X3, batch_X4)\n",
        "\n",
        "                # Compute loss\n",
        "                val_loss = F.binary_cross_entropy(outputs, batch_y.unsqueeze(1).float())\n",
        "                total_val_loss += val_loss.item()\n",
        "\n",
        "                # Store true labels and predictions for F1-score\n",
        "                y_true_val.extend(batch_y.cpu().numpy())\n",
        "                y_pred_val.extend((outputs > 0.5).float().cpu().numpy())\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Calculate F1-score for validation\n",
        "        val_f1 = f1_score(y_true_val, y_pred_val)\n",
        "\n",
        "        # Step the scheduler with validation loss\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        # Print training and validation metrics\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}] Training Loss: {train_loss:.4f}, F1 Score: {train_f1:.4f} | \"\n",
        "              f\"Validation Loss: {avg_val_loss:.4f}, F1 Score: {val_f1:.4f}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Plotting training and validation loss\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_losses, label=\"Training Loss\", marker='o')\n",
        "    plt.plot(val_losses, label=\"Validation Loss\", marker='x')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Training complete! Total time: {end_time - start_time:.2f} seconds\")\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "QrpFp6aDbtSW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "27f7c08f-ddd7-4136-a67e-b2d2c1e96d50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100] Training Loss: 0.0754, F1 Score: 0.2300 | Validation Loss: 0.0621, F1 Score: 0.3333\n",
            "Epoch [2/100] Training Loss: 0.0592, F1 Score: 0.3298 | Validation Loss: 0.0616, F1 Score: 0.3481\n",
            "Epoch [3/100] Training Loss: 0.0588, F1 Score: 0.3601 | Validation Loss: 0.0612, F1 Score: 0.3542\n",
            "Epoch [4/100] Training Loss: 0.0585, F1 Score: 0.3658 | Validation Loss: 0.0609, F1 Score: 0.3577\n",
            "Epoch [5/100] Training Loss: 0.0588, F1 Score: 0.3753 | Validation Loss: 0.0608, F1 Score: 0.3577\n",
            "Epoch [6/100] Training Loss: 0.0583, F1 Score: 0.3858 | Validation Loss: 0.0607, F1 Score: 0.3871\n",
            "Epoch [7/100] Training Loss: 0.0582, F1 Score: 0.3882 | Validation Loss: 0.0611, F1 Score: 0.3813\n",
            "Epoch [8/100] Training Loss: 0.0577, F1 Score: 0.3944 | Validation Loss: 0.0606, F1 Score: 0.4014\n",
            "Epoch [9/100] Training Loss: 0.0580, F1 Score: 0.3966 | Validation Loss: 0.0608, F1 Score: 0.3943\n",
            "Epoch [10/100] Training Loss: 0.0583, F1 Score: 0.4000 | Validation Loss: 0.0606, F1 Score: 0.4042\n",
            "Epoch [11/100] Training Loss: 0.0583, F1 Score: 0.4058 | Validation Loss: 0.0607, F1 Score: 0.4014\n",
            "Epoch [12/100] Training Loss: 0.0584, F1 Score: 0.4082 | Validation Loss: 0.0605, F1 Score: 0.4167\n",
            "Epoch [13/100] Training Loss: 0.0583, F1 Score: 0.4082 | Validation Loss: 0.0607, F1 Score: 0.4111\n",
            "Epoch [14/100] Training Loss: 0.0580, F1 Score: 0.4127 | Validation Loss: 0.0606, F1 Score: 0.4159\n",
            "Epoch [15/100] Training Loss: 0.0580, F1 Score: 0.4115 | Validation Loss: 0.0606, F1 Score: 0.4167\n",
            "Epoch [16/100] Training Loss: 0.0581, F1 Score: 0.4114 | Validation Loss: 0.0607, F1 Score: 0.4167\n",
            "Epoch [17/100] Training Loss: 0.0579, F1 Score: 0.4209 | Validation Loss: 0.0607, F1 Score: 0.4159\n",
            "Epoch [18/100] Training Loss: 0.0586, F1 Score: 0.4122 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [19/100] Training Loss: 0.0586, F1 Score: 0.4120 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [20/100] Training Loss: 0.0581, F1 Score: 0.4150 | Validation Loss: 0.0606, F1 Score: 0.4159\n",
            "Epoch [21/100] Training Loss: 0.0583, F1 Score: 0.4158 | Validation Loss: 0.0608, F1 Score: 0.4167\n",
            "Epoch [22/100] Training Loss: 0.0579, F1 Score: 0.4181 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [23/100] Training Loss: 0.0584, F1 Score: 0.4095 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [24/100] Training Loss: 0.0583, F1 Score: 0.4180 | Validation Loss: 0.0606, F1 Score: 0.4293\n",
            "Epoch [25/100] Training Loss: 0.0573, F1 Score: 0.4174 | Validation Loss: 0.0605, F1 Score: 0.4254\n",
            "Epoch [26/100] Training Loss: 0.0586, F1 Score: 0.4140 | Validation Loss: 0.0607, F1 Score: 0.4159\n",
            "Epoch [27/100] Training Loss: 0.0585, F1 Score: 0.4179 | Validation Loss: 0.0606, F1 Score: 0.4247\n",
            "Epoch [28/100] Training Loss: 0.0580, F1 Score: 0.4174 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [29/100] Training Loss: 0.0584, F1 Score: 0.4134 | Validation Loss: 0.0606, F1 Score: 0.4187\n",
            "Epoch [30/100] Training Loss: 0.0574, F1 Score: 0.4187 | Validation Loss: 0.0609, F1 Score: 0.4159\n",
            "Epoch [31/100] Training Loss: 0.0587, F1 Score: 0.4131 | Validation Loss: 0.0606, F1 Score: 0.4207\n",
            "Epoch [32/100] Training Loss: 0.0584, F1 Score: 0.4195 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [33/100] Training Loss: 0.0582, F1 Score: 0.4137 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [34/100] Training Loss: 0.0584, F1 Score: 0.4182 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [35/100] Training Loss: 0.0580, F1 Score: 0.4145 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [36/100] Training Loss: 0.0586, F1 Score: 0.4161 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [37/100] Training Loss: 0.0581, F1 Score: 0.4201 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [38/100] Training Loss: 0.0583, F1 Score: 0.4185 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [39/100] Training Loss: 0.0583, F1 Score: 0.4152 | Validation Loss: 0.0606, F1 Score: 0.4207\n",
            "Epoch [40/100] Training Loss: 0.0585, F1 Score: 0.4149 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [41/100] Training Loss: 0.0579, F1 Score: 0.4199 | Validation Loss: 0.0606, F1 Score: 0.4207\n",
            "Epoch [42/100] Training Loss: 0.0579, F1 Score: 0.4137 | Validation Loss: 0.0606, F1 Score: 0.4207\n",
            "Epoch [43/100] Training Loss: 0.0572, F1 Score: 0.4175 | Validation Loss: 0.0606, F1 Score: 0.4207\n",
            "Epoch [44/100] Training Loss: 0.0581, F1 Score: 0.4162 | Validation Loss: 0.0606, F1 Score: 0.4207\n",
            "Epoch [45/100] Training Loss: 0.0579, F1 Score: 0.4218 | Validation Loss: 0.0606, F1 Score: 0.4207\n",
            "Epoch [46/100] Training Loss: 0.0584, F1 Score: 0.4122 | Validation Loss: 0.0606, F1 Score: 0.4207\n",
            "Epoch [47/100] Training Loss: 0.0587, F1 Score: 0.4162 | Validation Loss: 0.0606, F1 Score: 0.4207\n",
            "Epoch [48/100] Training Loss: 0.0580, F1 Score: 0.4177 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [49/100] Training Loss: 0.0577, F1 Score: 0.4225 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [50/100] Training Loss: 0.0581, F1 Score: 0.4195 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [51/100] Training Loss: 0.0579, F1 Score: 0.4221 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [52/100] Training Loss: 0.0579, F1 Score: 0.4177 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [53/100] Training Loss: 0.0586, F1 Score: 0.4170 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [54/100] Training Loss: 0.0575, F1 Score: 0.4183 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [55/100] Training Loss: 0.0577, F1 Score: 0.4192 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [56/100] Training Loss: 0.0579, F1 Score: 0.4192 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [57/100] Training Loss: 0.0578, F1 Score: 0.4149 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [58/100] Training Loss: 0.0584, F1 Score: 0.4180 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [59/100] Training Loss: 0.0581, F1 Score: 0.4186 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [60/100] Training Loss: 0.0581, F1 Score: 0.4167 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [61/100] Training Loss: 0.0579, F1 Score: 0.4183 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [62/100] Training Loss: 0.0585, F1 Score: 0.4192 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [63/100] Training Loss: 0.0577, F1 Score: 0.4158 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [64/100] Training Loss: 0.0585, F1 Score: 0.4174 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [65/100] Training Loss: 0.0579, F1 Score: 0.4189 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [66/100] Training Loss: 0.0579, F1 Score: 0.4186 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [67/100] Training Loss: 0.0580, F1 Score: 0.4221 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [68/100] Training Loss: 0.0582, F1 Score: 0.4151 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [69/100] Training Loss: 0.0581, F1 Score: 0.4214 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [70/100] Training Loss: 0.0585, F1 Score: 0.4199 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [71/100] Training Loss: 0.0579, F1 Score: 0.4226 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [72/100] Training Loss: 0.0581, F1 Score: 0.4180 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [73/100] Training Loss: 0.0579, F1 Score: 0.4158 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [74/100] Training Loss: 0.0579, F1 Score: 0.4221 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [75/100] Training Loss: 0.0579, F1 Score: 0.4223 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [76/100] Training Loss: 0.0581, F1 Score: 0.4192 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [77/100] Training Loss: 0.0586, F1 Score: 0.4183 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [78/100] Training Loss: 0.0573, F1 Score: 0.4240 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [79/100] Training Loss: 0.0579, F1 Score: 0.4224 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [80/100] Training Loss: 0.0579, F1 Score: 0.4243 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [81/100] Training Loss: 0.0578, F1 Score: 0.4221 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [82/100] Training Loss: 0.0584, F1 Score: 0.4202 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [83/100] Training Loss: 0.0585, F1 Score: 0.4174 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [84/100] Training Loss: 0.0579, F1 Score: 0.4205 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [85/100] Training Loss: 0.0574, F1 Score: 0.4180 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [86/100] Training Loss: 0.0574, F1 Score: 0.4224 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [87/100] Training Loss: 0.0574, F1 Score: 0.4199 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [88/100] Training Loss: 0.0581, F1 Score: 0.4176 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [89/100] Training Loss: 0.0578, F1 Score: 0.4243 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [90/100] Training Loss: 0.0577, F1 Score: 0.4202 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [91/100] Training Loss: 0.0575, F1 Score: 0.4192 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [92/100] Training Loss: 0.0582, F1 Score: 0.4174 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [93/100] Training Loss: 0.0582, F1 Score: 0.4211 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [94/100] Training Loss: 0.0580, F1 Score: 0.4186 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [95/100] Training Loss: 0.0580, F1 Score: 0.4192 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [96/100] Training Loss: 0.0583, F1 Score: 0.4164 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [97/100] Training Loss: 0.0576, F1 Score: 0.4205 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [98/100] Training Loss: 0.0581, F1 Score: 0.4195 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [99/100] Training Loss: 0.0580, F1 Score: 0.4227 | Validation Loss: 0.0605, F1 Score: 0.4207\n",
            "Epoch [100/100] Training Loss: 0.0576, F1 Score: 0.4211 | Validation Loss: 0.0605, F1 Score: 0.4207\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete! Total time: 541.42 seconds\n"
          ]
        }
      ],
      "source": [
        "# Train the GatedCombination model using training and validation data\n",
        "trained_model = train_gated_combination_model(\n",
        "    X1_train,          # Updated source embeddings (after applying the GNN model)\n",
        "    X2_train,          # Original source embeddings (before applying the GNN model)\n",
        "    X3_train,          # Updated target embeddings (after applying the GNN model)\n",
        "    X4_train,          # Original target embeddings (before applying the GNN model)\n",
        "    tensor_score_train, # Ground truth labels for the training set (1 for matched pairs, 0 for unmatched pairs)\n",
        "\n",
        "    X1_val,            # Updated source embeddings for the validation set\n",
        "    X2_val,            # Original source embeddings for the validation set\n",
        "    X3_val,            # Updated target embeddings for the validation set\n",
        "    X4_val,            # Original target embeddings for the validation set\n",
        "    tensor_score_val,  # Ground truth labels for the validation set (1 for matched pairs, 0 for unmatched pairs)\n",
        "\n",
        "    epochs=100,        # Number of epochs (iterations over the entire training dataset)\n",
        "    batch_size=32,     # Number of training samples processed in one forward/backward pass\n",
        "    learning_rate=0.001, # Learning rate for the optimizer (controls step size during optimization)\n",
        "    weight_decay=1e-4 # Weight decay (L2 regularization) to prevent overfitting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAmLuMtGW9c2"
      },
      "source": [
        "# **Second Round Modifications**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVDqN5AxFu9m"
      },
      "source": [
        "# **Generate Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ITZZcElm8qRN"
      },
      "outputs": [],
      "source": [
        "# Build an indexed dictionary for the source ontology classes\n",
        "# src_class is the file path to the JSON file containing the source ontology classes\n",
        "indexed_dict_src = build_indexed_dict(src_class)\n",
        "\n",
        "# Build an indexed dictionary for the target ontology classes\n",
        "# tgt_class is the file path to the JSON file containing the target ontology classes\n",
        "indexed_dict_tgt = build_indexed_dict(tgt_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "GbSbmPlRDOs3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eec459c-e5bb-4ed8-8a07-0a6bcae476b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Gated embeddings saved:\n",
            "- Source: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Data/ncit_final_embeddings_bert-base-nli-mean-tokens.tsv\n",
            "- Target: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Data/doid_final_embeddings_bert-base-nli-mean-tokens.tsv\n",
            "⏱️ Execution time: 21.63 seconds\n"
          ]
        }
      ],
      "source": [
        "# Define output file paths for final embeddings of source and target ontologies\n",
        "output_file_src = f\"{data_dir}/{src_ent}_final_embeddings_bert-base-nli-mean-tokens.tsv\"\n",
        "output_file_tgt = f\"{data_dir}/{tgt_ent}_final_embeddings_bert-base-nli-mean-tokens.tsv\"\n",
        "\n",
        "# Save the final gated embeddings for all concepts in source and target ontologies\n",
        "save_gated_embeddings(\n",
        "    gated_model=trained_model,          # The trained GatedCombination model\n",
        "    embeddings_src=embeddings_src,      # GNN-transformed embeddings for source entities\n",
        "    x_src=x_src,                        # Initial semantic embeddings for source entities\n",
        "    embeddings_tgt=embeddings_tgt,      # GNN-transformed embeddings for target entities\n",
        "    x_tgt=x_tgt,                        # Initial semantic embeddings for target entities\n",
        "    indexed_dict_src=indexed_dict_src,  # Index-to-URI mapping for source ontology\n",
        "    indexed_dict_tgt=indexed_dict_tgt,  # Index-to-URI mapping for target ontology\n",
        "    output_file_src=output_file_src,    # Destination file path for source embeddings\n",
        "    output_file_tgt=output_file_tgt     # Destination file path for target embeddings\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIDvbZj2GIGo"
      },
      "source": [
        "# **Filter No Used Concepts**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "6Gl_wUG9KADo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbc98185-d7cd-4718-d328-3d94219cb2aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Initial source file: 15992 rows\n",
            "🔍 Initial target file: 8480 rows\n",
            "✅ Source after removing ignored classes: 7065 rows\n",
            "✅ Target after removing ignored classes: 8463 rows\n",
            "📁 Cleaned source file saved to: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Data/ncit_final_embeddings_bert-base-nli-mean-tokens_cleaned.tsv\n",
            "📁 Cleaned target file saved to: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Data/doid_final_embeddings_bert-base-nli-mean-tokens_cleaned.tsv\n"
          ]
        }
      ],
      "source": [
        "# Call the function to filter out ignored concepts (e.g., owl:Thing, deprecated, etc.)\n",
        "# from the source and target ontology embeddings.\n",
        "\n",
        "# Input:\n",
        "# - src_emb_path: Path to the TSV file containing embeddings for the source ontology\n",
        "# - tgt_emb_path: Path to the TSV file containing embeddings for the target ontology\n",
        "# - src_onto / tgt_onto: DeepOnto ontology objects used to identify ignored concepts\n",
        "\n",
        "# Output:\n",
        "# - src_file: Path to the cleaned source embeddings (with ignored concepts removed)\n",
        "# - tgt_file: Path to the cleaned target embeddings (with ignored concepts removed)\n",
        "\n",
        "src_file, tgt_file = filter_ignored_class(\n",
        "    src_emb_path=f\"{data_dir}/{src_ent}_final_embeddings_bert-base-nli-mean-tokens.tsv\",\n",
        "    tgt_emb_path=f\"{data_dir}/{tgt_ent}_final_embeddings_bert-base-nli-mean-tokens.tsv\",\n",
        "    src_onto=src_onto,\n",
        "    tgt_onto=tgt_onto\n",
        "\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpUklR4xnVMH"
      },
      "source": [
        "# **Mappings Generation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljGEyKNOerBT"
      },
      "source": [
        "# **Using faiss l2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "xOSRYREwerBi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ac913c7-7e9f-42c3-ef74-0d9757f3a1e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Using L2 (Euclidean) distance with FAISS\n",
            "Top-10 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Results/ncit2doid_top_10_mappings_bert-base-nli-mean-tokens.tsv\n",
            "⏱️ Execution time: 8.46 seconds\n"
          ]
        }
      ],
      "source": [
        "# Compute the top-10 most similar mappings using l2 distance\n",
        "# between ResMLP-encoded embeddings of the source and target ontologies.\n",
        "# The input embeddings were previously encoded using the ResMLPEncoder,\n",
        "# and the similarity score is computed as the inverse of the l2 distance.\n",
        "# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.\n",
        "topk_faiss_l2(\n",
        "    src_emb_path=f\"{data_dir}/{src_ent}_final_embeddings_bert-base-nli-mean-tokens_cleaned.tsv\",\n",
        "    tgt_emb_path=f\"{data_dir}/{tgt_ent}_final_embeddings_bert-base-nli-mean-tokens_cleaned.tsv\",\n",
        "    top_k=10,\n",
        "    output_file=f\"{results_dir}/{task}_top_10_mappings_bert-base-nli-mean-tokens.tsv\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD-mvVjaerBh"
      },
      "source": [
        "# **Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Global Metrics: Precision, Recall and F1 score**"
      ],
      "metadata": {
        "id": "r8GRfT_pR1kD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "9WZKJM46erBi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a2441de-5f6f-433c-ac61-94858365032e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📁 Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Results/ncit2doid_top_10_mappings_bert-base-nli-mean-tokens_predictions.tsv\n",
            "📌 Number of predictions in output: 2963\n",
            "🎯 Correct mappings (Top-1): 2113\n",
            "📊 Evaluation (P / R / F1): {'P': 0.713, 'R': 0.644, 'F1': 0.677}\n"
          ]
        }
      ],
      "source": [
        "# Run the evaluation on the predicted mappings using a filtering and evaluation function.\n",
        "\n",
        "output_file, metrics, correct = evaluate_predictions(\n",
        "    topk_file=f\"{results_dir}/{task}_top_10_mappings_bert-base-nli-mean-tokens.tsv\",\n",
        "    # Path to the TSV file containing predicted mappings with scores (before filtering).\n",
        "\n",
        "    train_file=f\"{dataset_dir}/refs_equiv/train.tsv\",\n",
        "    # Path to the training reference file (used to exclude mappings involving train-only entities).\n",
        "\n",
        "    test_file=f\"{dataset_dir}/refs_equiv/test.tsv\",\n",
        "    # Path to the test reference file (used as the gold standard for evaluation).\n",
        "\n",
        "    src_onto=src_onto,\n",
        "    # The source ontology object, used to detect ignored classes or perform additional filtering.\n",
        "\n",
        "    tgt_onto=tgt_onto,\n",
        "    # The target ontology object, used similarly for filtering ignored or irrelevant classes.\n",
        ")\n",
        "\n",
        "# This function returns:\n",
        "# - `output_file`: the path to the filtered and evaluated output file.\n",
        "# - `metrics`: a tuple containing (Precision, Recall, F1-score).\n",
        "# - `correct`: the number of correctly predicted mappings found in the gold standard.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Metrics@1**"
      ],
      "metadata": {
        "id": "KE3WArY1SAWO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "h0y9PGOjerBh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9398c847-4cf4-4e01-c90e-f13fb92860ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Using L2 (Euclidean) distance with FAISS\n",
            "Top-1 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Results/ncit2doid_top_1_mappings_bert-base-nli-mean-tokens.tsv\n",
            "⏱️ Execution time: 2.87 seconds\n"
          ]
        }
      ],
      "source": [
        "# Compute the top-1 most similar mappings using l2 distance\n",
        "# and the similarity score is computed as the inverse of the l2 distance.\n",
        "# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.\n",
        "topk_faiss_l2(\n",
        "    src_emb_path=f\"{data_dir}/{src_ent}_final_embeddings_bert-base-nli-mean-tokens_cleaned.tsv\",\n",
        "    tgt_emb_path=f\"{data_dir}/{tgt_ent}_final_embeddings_bert-base-nli-mean-tokens_cleaned.tsv\",\n",
        "    top_k=1,\n",
        "    output_file=f\"{results_dir}/{task}_top_1_mappings_bert-base-nli-mean-tokens.tsv\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "wk-B3ayYerBh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28bedb19-6d0b-4ac7-e976-150444b94d33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📁 Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Results/ncit2doid_top_1_mappings_bert-base-nli-mean-tokens_predictions.tsv\n",
            "📌 Number of predictions in output: 2314\n",
            "{'Precision@1': 0.812, 'Recall@1': 0.8047, 'F1@1': 0.8083}\n"
          ]
        }
      ],
      "source": [
        "# === Evaluate Top-1 Mappings ===\n",
        "\n",
        "results = evaluate_topk(\n",
        "    topk_file=f\"{results_dir}/{task}_top_1_mappings_bert-base-nli-mean-tokens.tsv\",\n",
        "    # Path to the file containing the predicted mappings with scores.\n",
        "    # This file may include unfiltered predictions (e.g., over all candidates).\n",
        "\n",
        "    train_file=f\"{dataset_dir}/refs_equiv/train.tsv\",\n",
        "    # Path to the training reference mappings file.\n",
        "    # Used to remove mappings that involve entities appearing only in training.\n",
        "\n",
        "    test_file=f\"{dataset_dir}/refs_equiv/test.tsv\",\n",
        "    # Path to the test reference mappings file.\n",
        "    # Ground-truth correspondences are extracted from this file for evaluation.\n",
        "\n",
        "    k=1  # Evaluate top-1 predictions per source entity.\n",
        ")\n",
        "\n",
        "# === Display evaluation results ===\n",
        "print(results)\n",
        "# Outputs a dictionary with Precision@1, Recall@1, and F1@1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf6vZML-KewM"
      },
      "source": [
        "# **Local MRR and Hit@k**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "7xXm15EQKeE_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# === Step 1: Load input files ===\n",
        "\n",
        "# Define paths to cleaned embedding files\n",
        "src_emb_path = f\"{data_dir}/{src_ent}_final_embeddings_bert-base-nli-mean-tokens_cleaned.tsv\"\n",
        "tgt_emb_path = f\"{data_dir}/{tgt_ent}_final_embeddings_bert-base-nli-mean-tokens_cleaned.tsv\"\n",
        "\n",
        "# Load candidate mappings (SrcEntity, TgtEntity) and source/target embeddings\n",
        "df_cands = pd.read_csv(cands_path)\n",
        "src_emb_df = pd.read_csv(src_emb_path, sep=\"\\t\")\n",
        "tgt_emb_df = pd.read_csv(tgt_emb_path, sep=\"\\t\")\n",
        "\n",
        "# === Step 2: Extract unique source and target URIs from the candidate pairs ===\n",
        "\n",
        "# Keep only distinct source and target entities (URIs) for which embeddings are needed\n",
        "unique_src_df = pd.DataFrame(df_cands[\"SrcEntity\"].unique(), columns=[\"Concept\"])\n",
        "unique_tgt_df = pd.DataFrame(df_cands[\"TgtEntity\"].unique(), columns=[\"Concept\"])\n",
        "\n",
        "# === Step 3: Join embeddings for each concept based on the \"Concept\" URI ===\n",
        "\n",
        "# Merge source entities with their corresponding embeddings (if available)\n",
        "merged_src_df = pd.merge(unique_src_df, src_emb_df, on=\"Concept\", how=\"left\")\n",
        "\n",
        "# Merge target entities with their corresponding embeddings (if available)\n",
        "merged_tgt_df = pd.merge(unique_tgt_df, tgt_emb_df, on=\"Concept\", how=\"left\")\n",
        "\n",
        "# === Step 4: Save the merged results to TSV files ===\n",
        "\n",
        "# Save the source concepts and their embeddings to file\n",
        "merged_src_df.to_csv(f\"{data_dir}/{src_ent}_cands_with_embeddings_bert-base-nli-mean-tokens.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "# Save the target concepts and their embeddings to file\n",
        "merged_tgt_df.to_csv(f\"{data_dir}/{tgt_ent}_cands_with_embeddings_bert-base-nli-mean-tokens.tsv\", sep=\"\\t\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "_BgQMQzperBl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77d74cb2-746e-4009-8b8f-44b0ea2e9638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Using L2 (Euclidean) distance with FAISS\n",
            "Top-200 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Results/ncit2doid_top_200_mappings_mrr_hit_bert-base-nli-mean-tokens.tsv\n",
            "⏱️ Execution time: 6.13 seconds\n"
          ]
        }
      ],
      "source": [
        "topk_faiss_l2(\n",
        "    # Path to the source entity embeddings (already filtered and linearly encoded)\n",
        "    src_emb_path=f\"{data_dir}/{src_ent}_cands_with_embeddings_bert-base-nli-mean-tokens.tsv\",\n",
        "\n",
        "    # Path to the target entity embeddings (already filtered and linearly encoded)\n",
        "    tgt_emb_path=f\"{data_dir}/{tgt_ent}_cands_with_embeddings_bert-base-nli-mean-tokens.tsv\",\n",
        "\n",
        "    # Number of top matches to retrieve per source entity (Top-K candidates)\n",
        "    top_k=200,\n",
        "\n",
        "    # Path to save the resulting Top-K mappings sorted by FAISS L2 distance (converted to similarity)\n",
        "    output_file=f\"{results_dir}/{task}_top_200_mappings_mrr_hit_bert-base-nli-mean-tokens.tsv\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "DpzkN2-verBl"
      },
      "outputs": [],
      "source": [
        "test_cands = f\"{dataset_dir}/refs_equiv/test.cands.tsv\"\n",
        "# Compute MRR and Hits@k metrics\n",
        "# This function evaluates the predicted rankings against the reference mappings\n",
        "results = compute_mrr_and_hits(\n",
        "    reference_file=test_cands,             # Reference file with true ranks\n",
        "    predicted_file=f\"{results_dir}/{task}_top_200_mappings_mrr_hit_bert-base-nli-mean-tokens.tsv\",             # File containing predicted rankings\n",
        "    output_file=formatted_predictions_path,    # File path to save formatted predictions\n",
        "    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "quXigRGeerBl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4219d99-cf21-412d-a595-4478d31f932a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranking Evaluation Results at K=1, 5, and 10:\n",
            "{'MRR': 0.7045934843870173, 'Hits@1': 0.6329268292682927, 'Hits@5': 0.7926829268292683, 'Hits@10': 0.8426829268292683}\n"
          ]
        }
      ],
      "source": [
        "# Evaluate ranking performance using standard metrics like MRR and Hits@K\n",
        "# 'formatted_predictions_path' should point to a TSV file with columns: SrcEntity, TgtEntity, TgtCandidates\n",
        "# This function computes how well the true targets are ranked among the candidates\n",
        "results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])\n",
        "\n",
        "# Print the evaluation results for Hits@1, Hits@5, and Hits@10\n",
        "print(\"Ranking Evaluation Results at K=1, 5, and 10:\")\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yFqKl-p1aVPF"
      }
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "zqEXsgPGMVhw",
        "zxCn5ztKVztw",
        "QpwWQ2ndKGOA"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}