{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkAX88H3RNRW"
      },
      "source": [
        "# **Package Installation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bSuJvX5_qNhr",
        "outputId": "0bb0857e-9c22-4b53-e590-c95edefc1d47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.6.0\n",
            "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision==0.21.0\n",
            "  Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting filelock (from torch==2.6.0)\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting typing-extensions>=4.10.0 (from torch==2.6.0)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting networkx (from torch==2.6.0)\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch==2.6.0)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec (from torch==2.6.0)\n",
            "  Downloading fsspec-2025.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.6.0)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.2.0 (from torch==2.6.0)\n",
            "  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting sympy==1.13.1 (from torch==2.6.0)\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting numpy (from torchvision==0.21.0)\n",
            "  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m132.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.21.0)\n",
            "  Downloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch==2.6.0)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.6.0)\n",
            "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m112.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m119.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m522.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Downloading fsspec-2025.5.0-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.3.0\n",
            "    Uninstalling mpmath-1.3.0:\n",
            "      Successfully uninstalled mpmath-1.3.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.13.2\n",
            "    Uninstalling typing_extensions-4.13.2:\n",
            "      Successfully uninstalled typing_extensions-4.13.2\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.2.1\n",
            "    Uninstalling pillow-11.2.1:\n",
            "      Successfully uninstalled pillow-11.2.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.4.2\n",
            "    Uninstalling networkx-3.4.2:\n",
            "      Successfully uninstalled networkx-3.4.2\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.18.0\n",
            "    Uninstalling filelock-3.18.0:\n",
            "      Successfully uninstalled filelock-3.18.0\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cpu\n",
            "    Uninstalling torch-2.6.0+cpu:\n",
            "      Successfully uninstalled torch-2.6.0+cpu\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cpu\n",
            "    Uninstalling torchvision-0.21.0+cpu:\n",
            "      Successfully uninstalled torchvision-0.21.0+cpu\n",
            "Successfully installed MarkupSafe-3.0.2 filelock-3.18.0 fsspec-2025.5.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.6 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pillow-11.2.1 sympy-1.13.1 torch-2.6.0 torchvision-0.21.0 triton-3.2.0 typing-extensions-4.13.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "800a56d287c649059a9d7e8b6c138ab4"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Reinstall a specific version of PyTorch (v2.6.0) and torchvision (v0.21.0)\n",
        "# The \"--force-reinstall\" flag ensures that the packages are reinstalled even if the correct version is already present.\n",
        "# This is useful to resolve environment issues or when dependencies need to be reset.\n",
        "!pip install torch==2.6.0 torchvision==0.21.0 --force-reinstall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItSvFeEAfLBF",
        "outputId": "eb129d9c-8c9c-47f2-dbb9-e54d1f43215f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.2.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting optuna\n",
            "  Using cached optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Using cached alembic-1.16.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Using cached colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\n",
            "Collecting sqlalchemy>=1.4.2 (from optuna)\n",
            "  Using cached sqlalchemy-2.0.41-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Collecting greenlet>=1 (from sqlalchemy>=1.4.2->optuna)\n",
            "  Using cached greenlet-3.2.2-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
            "Using cached optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "Using cached alembic-1.16.1-py3-none-any.whl (242 kB)\n",
            "Using cached sqlalchemy-2.0.41-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "Using cached colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Using cached greenlet-3.2.2-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (583 kB)\n",
            "Installing collected packages: greenlet, colorlog, sqlalchemy, alembic, optuna\n",
            "Successfully installed alembic-1.16.1 colorlog-6.9.0 greenlet-3.2.2 optuna-4.3.0 sqlalchemy-2.0.41\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.2.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0\n",
            "Collecting torch-geometric==2.4.0\n",
            "  Downloading torch_geometric-2.4.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (2.2.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (1.15.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (3.1.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (2.32.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (3.2.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (1.6.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric==2.4.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.4.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.4.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.4.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.4.0) (2025.4.26)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->torch-geometric==2.4.0) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->torch-geometric==2.4.0) (3.6.0)\n",
            "Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.4.0\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_scatter-2.1.2%2Bpt20cpu-cp311-cp311-linux_x86_64.whl (494 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.0/494.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_sparse-0.6.18%2Bpt20cpu-cp311-cp311-linux_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_cluster-1.6.3%2Bpt20cpu-cp311-cp311-linux_x86_64.whl (750 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.9/750.9 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_spline_conv-1.2.2%2Bpt20cpu-cp311-cp311-linux_x86_64.whl (208 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.1/208.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (2.2.6)\n",
            "Installing collected packages: torch-spline-conv, torch-scatter, torch-sparse, torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt20cpu torch-scatter-2.1.2+pt20cpu torch-sparse-0.6.18+pt20cpu torch-spline-conv-1.2.2+pt20cpu\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deeponto\n",
            "  Downloading deeponto-0.9.3-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting JPype1 (from deeponto)\n",
            "  Downloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting yacs (from deeponto)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from deeponto) (2.6.0)\n",
            "Collecting anytree (from deeponto)\n",
            "  Downloading anytree-2.13.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from deeponto) (8.2.0)\n",
            "Collecting dill (from deeponto)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from deeponto) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deeponto) (2.2.6)\n",
            "Requirement already satisfied: scikit_learn in /usr/local/lib/python3.11/dist-packages (from deeponto) (1.6.1)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.11/dist-packages (from deeponto) (4.51.3)\n",
            "Collecting datasets (from deeponto)\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (from deeponto) (3.8.5)\n",
            "Collecting pprintpp (from deeponto)\n",
            "  Downloading pprintpp-0.4.0-py2.py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from deeponto) (3.4.2)\n",
            "Collecting lxml (from deeponto)\n",
            "  Downloading lxml-5.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting textdistance (from deeponto)\n",
            "  Downloading textdistance-4.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (from deeponto) (7.7.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from deeponto) (6.17.1)\n",
            "Collecting enlighten (from deeponto)\n",
            "  Downloading enlighten-1.14.1-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Collecting rdflib (from deeponto)\n",
            "  Downloading rdflib-7.1.4-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from deeponto) (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (20.0.0)\n",
            "Collecting dill (from deeponto)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (3.5.0)\n",
            "Collecting multiprocess<0.70.17 (from datasets->deeponto)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (6.0.2)\n",
            "Collecting blessed>=1.17.7 (from enlighten->deeponto)\n",
            "  Downloading blessed-1.21.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting prefixed>=0.3.2 (from enlighten->deeponto)\n",
            "  Downloading prefixed-0.9.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (1.8.0)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (6.4.2)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (5.7.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->deeponto) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->deeponto) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->deeponto) (3.0.15)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->deeponto) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->deeponto) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->deeponto) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->deeponto) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->deeponto) (2025.2)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib->deeponto) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn->deeponto) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn->deeponto) (3.6.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (0.15.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (4.13.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->deeponto) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]->deeponto) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]->deeponto) (0.5.3)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]->deeponto) (1.6.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from blessed>=1.17.7->enlighten->deeponto) (0.2.13)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (3.11.15)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel->deeponto)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (5.2.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (4.9.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel->deeponto) (5.7.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->deeponto) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->deeponto) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->deeponto) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->deeponto) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->deeponto) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->deeponto) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->deeponto) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->deeponto) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->deeponto) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->deeponto) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->deeponto) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy->deeponto) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy->deeponto) (14.0.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->deeponto) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->deeponto) (7.1.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->deeponto) (6.5.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy->deeponto) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (1.20.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->deeponto) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel->deeponto) (4.3.8)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->deeponto) (1.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (23.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->deeponto) (0.7.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->deeponto) (3.0.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->deeponto) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->deeponto) (0.1.2)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (4.23.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.24.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (2.7)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (2.22)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.3.1)\n",
            "Downloading deeponto-0.9.3-py3-none-any.whl (89.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anytree-2.13.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading enlighten-1.14.1-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml-5.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pprintpp-0.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Downloading rdflib-7.1.4-py3-none-any.whl (565 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.1/565.1 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading textdistance-4.6.3-py3-none-any.whl (31 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading blessed-1.21.0-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prefixed-0.9.0-py2.py3-none-any.whl (13 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: prefixed, pprintpp, yacs, textdistance, rdflib, lxml, JPype1, jedi, fsspec, dill, blessed, anytree, multiprocess, enlighten, datasets, deeponto\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.5.0\n",
            "    Uninstalling fsspec-2025.5.0:\n",
            "      Successfully uninstalled fsspec-2025.5.0\n",
            "Successfully installed JPype1-1.5.2 anytree-2.13.0 blessed-1.21.0 datasets-3.6.0 deeponto-0.9.3 dill-0.3.8 enlighten-1.14.1 fsspec-2025.3.0 jedi-0.19.2 lxml-5.4.0 multiprocess-0.70.16 pprintpp-0.4.0 prefixed-0.9.0 rdflib-7.1.4 textdistance-4.6.3 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "# === Base Libraries ===\n",
        "!pip install numpy --upgrade\n",
        "!pip install pandas\n",
        "!pip install optuna\n",
        "\n",
        "# === FAISS (for Approximate Nearest Neighbor Search) ===\n",
        "!pip install faiss-cpu        # CPU version (recommended unless using GPU)\n",
        "# !pip install faiss-gpu      # Uncomment if running on CUDA-enabled GPU\n",
        "\n",
        "# === PyTorch Geometric and dependencies ===\n",
        "!pip install torch-geometric==2.4.0\n",
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
        "# Optional: latest dev version from GitHub\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "\n",
        "# === DeepOnto (Ontology Matching Toolkit) ===\n",
        "!pip install deeponto\n",
        "# Optionally install custom version from a GitHub repository\n",
        "# !pip install git+https://github.com/<username>/deeponto.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nFonRjT5fMCv"
      },
      "outputs": [],
      "source": [
        "# Import pandas for working with tabular data (e.g., CSV, TSV files)\n",
        "import pandas as pd\n",
        "\n",
        "# Import numpy for numerical operations and efficient array handling\n",
        "import numpy as np\n",
        "\n",
        "# Import json for reading and writing JSON-formatted files (useful for config or ontology structures)\n",
        "import json\n",
        "\n",
        "# Import pickle for serializing and deserializing Python objects (e.g., saving models or processed data)\n",
        "import pickle\n",
        "\n",
        "# Import warnings to control or suppress warning messages during runtime\n",
        "import warnings\n",
        "\n",
        "# Import gc (garbage collector) for managing memory manually when dealing with large datasets\n",
        "import gc\n",
        "\n",
        "# Ignore all warning messages to keep the output clean\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uchfZJP2fZwe"
      },
      "outputs": [],
      "source": [
        "# Import PyTorch core library for tensor operations and model definition\n",
        "import torch\n",
        "\n",
        "# Import commonly used PyTorch components\n",
        "from torch import Tensor, optim  # Tensor type and optimization algorithms (e.g., SGD, Adam)\n",
        "\n",
        "# Import PyTorch's neural network module (base class for defining models)\n",
        "import torch.nn as nn\n",
        "\n",
        "# Import PyTorch's functional API for operations like activations and loss functions\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Import DataLoader utilities for batching and loading datasets during training\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# === PyTorch Geometric (PyG) modules for graph-based learning ===\n",
        "\n",
        "# Basic graph data structure from PyG\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# PyG-specific DataLoader for batching graphs\n",
        "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
        "\n",
        "# Import graph convolution layers and pooling functions from PyG\n",
        "from torch_geometric.nn import (\n",
        "    GCNConv,             # Graph Convolutional Network layer\n",
        "    GINConv,             # Graph Isomorphism Network convolution\n",
        "    global_mean_pool,    # Global mean pooling over node embeddings\n",
        "    global_add_pool,     # Global sum pooling over node embeddings\n",
        "    MessagePassing       # Base class for defining custom GNN layers\n",
        ")\n",
        "\n",
        "# Explicitly re-import MessagePassing (optional if already above)\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "\n",
        "# Graph utility functions from PyG\n",
        "from torch_geometric.utils import (\n",
        "    to_undirected,       # Converts a directed graph to undirected\n",
        "    softmax              # Softmax over edges (e.g., for attention)\n",
        ")\n",
        "\n",
        "# Initialization utilities for GNN layers\n",
        "from torch_geometric.nn.inits import (\n",
        "    reset,               # Reset parameters\n",
        "    glorot,              # Glorot (Xavier) weight initialization\n",
        "    zeros                # Zero initialization\n",
        ")\n",
        "\n",
        "# Typing utilities from PyG for adjacency and tensor specifications\n",
        "from torch_geometric.typing import (\n",
        "    Adj, OptTensor, PairTensor, SparseTensor\n",
        ")\n",
        "\n",
        "# Dense linear transformation layer from PyG (alternative to torch.nn.Linear)\n",
        "from torch_geometric.nn.dense.linear import Linear\n",
        "\n",
        "# Additional PyTorch neural network components\n",
        "from torch.nn import (\n",
        "    Linear,             # Fully connected (dense) layer\n",
        "    PReLU,              # Parametric ReLU activation\n",
        "    Sequential,         # Layer container for building sequential models\n",
        "    BatchNorm1d,        # Batch normalization for 1D inputs\n",
        "    Dropout             # Dropout regularization\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ziMBSWE8ff1N"
      },
      "outputs": [],
      "source": [
        "# Import matplotlib for creating visualizations (e.g., loss curves, evaluation metrics, embedding projections)\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JeAvp6PNfiLh"
      },
      "outputs": [],
      "source": [
        "# Import function to split data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import encoder to convert categorical labels into integer values (useful for classification tasks)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Import evaluation metrics for classification and regression tasks\n",
        "from sklearn.metrics import (\n",
        "    f1_score,            # Harmonic mean of precision and recall; useful for imbalanced classification\n",
        "    precision_score,     # Measures the proportion of true positives among all predicted positives\n",
        "    accuracy_score,      # Measures overall correctness of predictions (classification)\n",
        "    mean_squared_error,  # Measures average squared difference between predicted and actual values (regression)\n",
        "    mean_absolute_error  # Measures average absolute difference between predicted and actual values (regression)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jm1rMZvmfl2M",
        "outputId": "049756b5-f780-4fcf-f65c-e33de59e1357"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the maximum memory located to JVM [8g]: 8g\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import the Ontology class for loading and manipulating OWL ontologies\n",
        "from deeponto.onto import Ontology\n",
        "\n",
        "# Import all components related to OAEI (Ontology Alignment Evaluation Initiative) benchmarking\n",
        "from deeponto.align.oaei import *\n",
        "\n",
        "# Import data structures for representing mappings between ontology entities\n",
        "from deeponto.align.mapping import EntityMapping, ReferenceMapping\n",
        "# - EntityMapping: represents a predicted alignment (one or more mappings)\n",
        "# - ReferenceMapping: represents the gold standard/reference alignments\n",
        "\n",
        "# Import the evaluator to compute Precision, Recall, and F1-score for alignments\n",
        "from deeponto.align.evaluation import AlignmentEvaluator\n",
        "\n",
        "# Utility function to read TSV/CSV tables as mapping or data frames\n",
        "from deeponto.utils import read_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JYhwr3Q_ft2N"
      },
      "outputs": [],
      "source": [
        "# Import Optuna, a hyperparameter optimization framework for automating model tuning using strategies like Bayesian optimization\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UmSCo5Olfzuz"
      },
      "outputs": [],
      "source": [
        "# Import the math module for mathematical functions (e.g., sqrt, log, exp)\n",
        "import math\n",
        "\n",
        "# Import the time module for measuring execution time of code blocks or functions\n",
        "import time\n",
        "\n",
        "# Import typing annotations for function signatures and code clarity\n",
        "from typing import Optional, Tuple, Union, Callable\n",
        "# - Optional[T]: denotes a value that could be of type T or None\n",
        "# - Tuple: fixed-size ordered collection of elements\n",
        "# - Union: allows multiple possible types (e.g., Union[int, str])\n",
        "# - Callable: represents a function or method type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9WNn0OMQW2CS"
      },
      "outputs": [],
      "source": [
        "# Import Python's built-in random module for generating pseudo-random numbers\n",
        "import random\n",
        "\n",
        "# Set the seed for PyTorch's random number generator to ensure reproducibility\n",
        "import torch\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Set the seed for NumPy's random number generator to ensure reproducibility\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "\n",
        "# Set the seed for Python's built-in random module to ensure reproducibility\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-abbBHOoRdWl"
      },
      "source": [
        "# **Paths Definition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVgl_Bb42naS",
        "outputId": "53778465-1e2b-4ceb-96a5-19e0f10ed2a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Importing the 'drive' module from Google Colab to interact with Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount the user's Google Drive to the Colab environment\n",
        "# After running this, a link will appear to authorize access, and Google Drive will be mounted at '/content/gdrive'\n",
        "drive.mount('/content/gdrive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "36ttssQ3W7cx"
      },
      "outputs": [],
      "source": [
        "# Define the source ontology name\n",
        "src_ent = \"ncit\"\n",
        "\n",
        "# Define the target ontology name\n",
        "tgt_ent = \"doid\"\n",
        "\n",
        "# Define the task name for this ontology matching process\n",
        "task = \"ncit2doid\"\n",
        "\n",
        "# Define the similarity threshold for validating matches\n",
        "thres = 0.50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SJpvkdwVSQye"
      },
      "outputs": [],
      "source": [
        "dir = \"/content/gdrive/My Drive/BioGITOM-VLDB/\"\n",
        "\n",
        "# Define the directory for the dataset containing source and target ontologies\n",
        "dataset_dir = f\"{dir}/Datasets/{task}\"\n",
        "\n",
        "# Define the data directory for storing embeddings, adjacency matrices, and related files\n",
        "data_dir = f\"{dir}/{task}/Data\"\n",
        "\n",
        "# Define the directory for storing the results\n",
        "results_dir = f\"{dir}/{task}/Results\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "eFDNSFef23er"
      },
      "outputs": [],
      "source": [
        "# Load the Source ontology using the Ontology class from DeepOnto\n",
        "# This initializes the source ontology by loading its .owl file.\n",
        "src_onto = Ontology(f\"{dataset_dir}/{src_ent}.owl\")\n",
        "\n",
        "# Load the Target ontology using the Ontology class from DeepOnto\n",
        "# This initializes the target ontology by loading its .owl file.\n",
        "tgt_onto = Ontology(f\"{dataset_dir}/{tgt_ent}.owl\")\n",
        "\n",
        "# Define the file path for the Source embeddings CSV file\n",
        "# Embeddings for the source ontology entities are stored in this file.\n",
        "src_Emb = f\"{data_dir}/{src_ent}_SapBERT_emb.csv\"\n",
        "\n",
        "# Define the file path for the Target embeddings CSV file\n",
        "# Embeddings for the target ontology entities are stored in this file.\n",
        "tgt_Emb = f\"{data_dir}/{tgt_ent}_SapBERT_emb.csv\"\n",
        "\n",
        "# Define the file path for the Source adjacency matrix\n",
        "# This file represents the relationships (edges) between entities in the source ontology.\n",
        "src_Adjacence = f\"{data_dir}/{src_ent}_adjacence.csv\"\n",
        "\n",
        "# Define the file path for the Target adjacency matrix\n",
        "# This file represents the relationships (edges) between entities in the target ontology.\n",
        "tgt_Adjacence = f\"{data_dir}/{tgt_ent}_adjacence.csv\"\n",
        "\n",
        "# Define the file path for the JSON file containing the Source ontology class labels\n",
        "# This file maps the source ontology entities to their labels or names.\n",
        "src_class = f\"{data_dir}/{src_ent}_classes.json\"\n",
        "\n",
        "# Define the file path for the JSON file containing the Target ontology class labels\n",
        "# This file maps the target ontology entities to their labels or names.\n",
        "tgt_class = f\"{data_dir}/{tgt_ent}_classes.json\"\n",
        "\n",
        "# Define the file path for the train data\n",
        "train_file = f\"{data_dir}/{task}_train.csv\"\n",
        "\n",
        "# Define the file path for the test data\n",
        "# The test file contains reference mappings (ground truth) between the source and target ontologies.\n",
        "test_file = f\"{dataset_dir}/refs_equiv/test.tsv\"\n",
        "\n",
        "# Define the file path for the candidate mappings used during testing\n",
        "# This file includes the candidate pairs (source and target entities) for ranking based metrics.\n",
        "test_cands = f\"{dataset_dir}/refs_equiv/test.cands.tsv\"\n",
        "cands_path = f\"{data_dir}/{task}_cands.csv\"\n",
        "\n",
        "# Define the path where the prediction results will be saved in TSV format\n",
        "# This file will store the final predictions (mappings) between source and target entities.\n",
        "prediction_path = f\"{results_dir}/{task}_matching_results_SapBERT.tsv\"\n",
        "\n",
        "# Define the path where all prediction results will be saved in TSV format\n",
        "# This file will store detailed prediction results, including all candidate scores.\n",
        "all_predictions_path = f\"{results_dir}/{task}_all_predictions_SapBERT.tsv\"\n",
        "\n",
        "# Define the path where formatted ranking predictions will be saved in TSV format\n",
        "# This file will contain predictions formatted for evaluation using ranking-based metrics.\n",
        "formatted_predictions_path = f\"{results_dir}/{task}_formatted_predictions_SapBERT.tsv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqEXsgPGMVhw"
      },
      "source": [
        "# **GIT Architecture**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "A_d6XCsUMVhx"
      },
      "outputs": [],
      "source": [
        "# RGIT class definition which inherits from PyTorch Geometric's MessagePassing class\n",
        "class RGIT(MessagePassing):\n",
        "\n",
        "    _alpha: OptTensor  # Define _alpha as an optional tensor for storing attention weights\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        nn: Callable,  # Neural network to be used in the final layer of the GNN\n",
        "        in_channels: Union[int, Tuple[int, int]],  # Input dimension, can be a single or pair of integers\n",
        "        out_channels: int,  # Output dimension of the GNN\n",
        "        eps: float = 0.,  # GIN parameter: epsilon for GIN aggregation\n",
        "        train_eps: bool = False,  # GIN parameter: whether epsilon should be learnable\n",
        "        heads: int = 1,  # Transformer parameter: number of attention heads\n",
        "        dropout: float = 0.,  # Dropout rate for attention weights\n",
        "        edge_dim: Optional[int] = None,  # Dimension for edge attributes (optional)\n",
        "        bias: bool = True,  # Whether to use bias in linear layers\n",
        "        root_weight: bool = True,  # GIN parameter: whether to apply root weight in aggregation\n",
        "        **kwargs,  # Additional arguments passed to the parent class\n",
        "    ):\n",
        "        # Set the aggregation type to 'add' and initialize the parent class with node_dim=0\n",
        "        kwargs.setdefault('aggr', 'add')\n",
        "        super().__init__(node_dim=0, **kwargs)\n",
        "\n",
        "        # Initialize input/output dimensions, neural network, and GIN/transformer parameters\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.nn = nn  # Neural network used by the GNN\n",
        "        self.initial_eps = eps  # Initial value of epsilon for GIN\n",
        "\n",
        "        # Set epsilon to be learnable or fixed\n",
        "        if train_eps:\n",
        "            self.eps = torch.nn.Parameter(torch.empty(1))  # Learnable epsilon\n",
        "        else:\n",
        "            self.register_buffer('eps', torch.empty(1))  # Non-learnable epsilon (fixed)\n",
        "\n",
        "        # Initialize transformer-related parameters\n",
        "        self.heads = heads\n",
        "        self.dropout = dropout\n",
        "        self.edge_dim = edge_dim\n",
        "        self._alpha = None  # Placeholder for attention weights\n",
        "\n",
        "        # Handle case where in_channels is a single integer or a tuple\n",
        "        if isinstance(in_channels, int):\n",
        "            in_channels = (in_channels, in_channels)\n",
        "\n",
        "        # Define the linear layers for key, query, and value for the transformer mechanism\n",
        "        self.lin_key = Linear(in_channels[0], heads * out_channels)\n",
        "        self.lin_query = Linear(in_channels[1], heads * out_channels)\n",
        "        self.lin_value = Linear(in_channels[0], heads * out_channels)\n",
        "\n",
        "        # Define linear transformation for edge embeddings if provided\n",
        "        if edge_dim is not None:\n",
        "            self.lin_edge = Linear(edge_dim, heads * out_channels, bias=False)\n",
        "        else:\n",
        "            self.lin_edge = self.register_parameter('lin_edge', None)\n",
        "\n",
        "        # Reset all parameters to their initial values\n",
        "        self.reset_parameters()\n",
        "\n",
        "    # Function to reset model parameters\n",
        "    def reset_parameters(self):\n",
        "        super().reset_parameters()  # Call parent class reset method\n",
        "        self.lin_key.reset_parameters()  # Reset key linear layer\n",
        "        self.lin_query.reset_parameters()  # Reset query linear layer\n",
        "        self.lin_value.reset_parameters()  # Reset value linear layer\n",
        "        if self.edge_dim:\n",
        "            self.lin_edge.reset_parameters()  # Reset edge linear layer if used\n",
        "        reset(self.nn)  # Reset the neural network provided\n",
        "        self.eps.data.fill_(self.initial_eps)  # Initialize epsilon with the starting value\n",
        "\n",
        "    # Forward function defining how the input data flows through the model\n",
        "    def forward(self, x: Union[Tensor, PairTensor], edge_index: Adj,\n",
        "                edge_attr: OptTensor = None, return_attention_weights=None):\n",
        "        # Unpack number of heads and output channels\n",
        "        H, C = self.heads, self.out_channels\n",
        "\n",
        "        # If x is a tensor, treat it as a pair of tensors (source and target embeddings)\n",
        "        if isinstance(x, Tensor):\n",
        "            x: PairTensor = (x, x)\n",
        "\n",
        "        # Extract source node embeddings\n",
        "        x_t = x[0]\n",
        "\n",
        "        # Apply linear transformations and reshape query, key, and value for multi-head attention\n",
        "        query = self.lin_query(x[1]).view(-1, H, C)\n",
        "        key = self.lin_key(x[0]).view(-1, H, C)\n",
        "        value = self.lin_value(x[0]).view(-1, H, C)\n",
        "\n",
        "        # Propagate messages through the graph using the propagate function\n",
        "        out = self.propagate(edge_index, query=query, key=key, value=value,\n",
        "                             edge_attr=edge_attr, size=None)\n",
        "\n",
        "        # Retrieve attention weights and reset them\n",
        "        alpha = self._alpha\n",
        "        self._alpha = None  # Reset _alpha after use\n",
        "        out = out.mean(dim=1)  # Take the mean over all attention heads\n",
        "\n",
        "        # Apply GIN aggregation by adding epsilon-scaled original node embeddings\n",
        "        out = out + (1 + self.eps) * x_t\n",
        "        return self.nn(out)  # Pass through the neural network\n",
        "\n",
        "    # Message passing function which calculates attention and combines messages\n",
        "    def message(self, query_i: Tensor, key_j: Tensor, value_j: Tensor,\n",
        "                edge_attr: OptTensor, index: Tensor, ptr: OptTensor,\n",
        "                size_i: Optional[int]) -> Tensor:\n",
        "        # If edge attributes are used, apply linear transformation and add them to the key\n",
        "        if self.lin_edge is not None:\n",
        "            assert edge_attr is not None\n",
        "            edge_attr = self.lin_edge(edge_attr).view(-1, self.heads, self.out_channels)\n",
        "            key_j = key_j + edge_attr\n",
        "\n",
        "        # Calculate attention (alpha) using the dot product between query and key\n",
        "        alpha = (query_i * key_j).sum(dim=-1) / math.sqrt(self.out_channels)\n",
        "        alpha = softmax(alpha, index, ptr, size_i)  # Apply softmax to normalize attention\n",
        "        self._alpha = alpha  # Store attention weights\n",
        "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)  # Apply dropout\n",
        "\n",
        "        # Calculate the output message by applying attention to the value\n",
        "        out = value_j\n",
        "        if edge_attr is not None:\n",
        "            out = out + edge_attr  # Add edge embeddings to the output if present\n",
        "        out = out * alpha.view(-1, self.heads, 1)  # Scale by attention weights\n",
        "        return out\n",
        "\n",
        "    # String representation function for debugging or printing\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
        "                f'{self.out_channels}, heads={self.heads})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qwFv6RgHmGCf"
      },
      "outputs": [],
      "source": [
        "# Define the RGIT_mod class, a multi-layer GNN that uses both RGIT and linear layers\n",
        "class RGIT_mod(torch.nn.Module):\n",
        "    \"\"\"Multi-layer RGIT with optional linear layers\"\"\"\n",
        "\n",
        "    # Initialize the model with hidden dimension, number of RGIT layers, and number of linear layers\n",
        "    def __init__(self, dim_h, num_layers, num_linear_layers=1):\n",
        "        super(RGIT_mod, self).__init__()\n",
        "        self.num_layers = num_layers  # Number of RGIT layers\n",
        "        self.num_linear_layers = num_linear_layers  # Number of linear layers\n",
        "        self.linears = torch.nn.ModuleList()  # List to store linear layers\n",
        "        self.rgit_layers = torch.nn.ModuleList()  # List to store RGIT layers\n",
        "\n",
        "        # Create a list of Linear and PReLU layers (for encoding entity names)\n",
        "        for _ in range(num_linear_layers):\n",
        "            self.linears.append(Linear(dim_h, dim_h))  # Linear transformation layer\n",
        "            self.linears.append(PReLU(num_parameters=dim_h))  # Parametric ReLU activation function\n",
        "\n",
        "        # Create a list of RGIT layers\n",
        "        for _ in range(num_layers):\n",
        "            self.rgit_layers.append(RGIT(  # Each RGIT layer contains a small MLP with Linear and PReLU\n",
        "                Sequential(Linear(dim_h, dim_h), PReLU(num_parameters=dim_h),\n",
        "                           Linear(dim_h, dim_h), PReLU(num_parameters=dim_h)), dim_h, dim_h))\n",
        "\n",
        "    # Forward pass through the model\n",
        "    def forward(self, x, edge_index):\n",
        "        # Apply the linear layers first to the input\n",
        "        for layer in self.linears:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Then apply the RGIT layers for message passing\n",
        "        for layer in self.rgit_layers:\n",
        "            x = layer(x, edge_index)\n",
        "\n",
        "        return x  # Return the final node embeddings after all layers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxCn5ztKVztw"
      },
      "source": [
        "# **Gated Network Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7MKQUv7o7zay"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "class GatedCombinationWithFaiss(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Linear layers to compute gating values for the source and target embeddings\n",
        "        self.gate_A_fc = nn.Linear(input_dim, input_dim)\n",
        "        self.gate_B_fc = nn.Linear(input_dim, input_dim)\n",
        "\n",
        "        # Final linear layer to map similarity score to prediction (sigmoid output)\n",
        "        self.fc = nn.Linear(1, 1)\n",
        "\n",
        "    def faiss_l2(self, a, b):\n",
        "        \"\"\"\n",
        "        Compute L2 distances using FAISS (non-differentiable).\n",
        "        This function converts tensors to NumPy, builds a FAISS index, and performs a search.\n",
        "        Only use this during inference or evaluation — not for training.\n",
        "\n",
        "        Args:\n",
        "            a (Tensor): Query vectors (batch_size x dim)\n",
        "            b (Tensor): Database vectors (batch_size x dim)\n",
        "\n",
        "        Returns:\n",
        "            Tensor: L2 distances between aligned rows (one-to-one)\n",
        "        \"\"\"\n",
        "        # Detach tensors from the computation graph and move to CPU\n",
        "        a_np = a.detach().cpu().numpy().astype(np.float32)\n",
        "        b_np = b.detach().cpu().numpy().astype(np.float32)\n",
        "\n",
        "        # Create a FAISS index for L2 distance\n",
        "        index = faiss.IndexFlatL2(a_np.shape[1])\n",
        "        index.add(b_np)\n",
        "\n",
        "        # Perform 1-NN search\n",
        "        distances, _ = index.search(a_np, 1)  # shape: (batch_size, 1)\n",
        "\n",
        "        # Convert back to PyTorch tensor on the original device\n",
        "        return torch.tensor(distances[:, 0], dtype=torch.float32, device=a.device)\n",
        "\n",
        "    def forward(self, x1, x2, x3, x4, return_embeddings=False):\n",
        "        \"\"\"\n",
        "        Forward pass through the gated combination model.\n",
        "        Combines original and transformed embeddings using learned gates.\n",
        "\n",
        "        Args:\n",
        "            x1, x2: original and GNN-transformed embeddings for source entities\n",
        "            x3, x4: original and GNN-transformed embeddings for target entities\n",
        "            return_embeddings (bool): if True, return gated embeddings instead of prediction\n",
        "\n",
        "        Returns:\n",
        "            Tensor: similarity score (if return_embeddings=False)\n",
        "            OR\n",
        "            Tuple[Tensor, Tensor]: gated source and target embeddings (if return_embeddings=True)\n",
        "        \"\"\"\n",
        "\n",
        "        # Compute gate for source embeddings\n",
        "        gate_values1 = torch.sigmoid(self.gate_A_fc(x1))\n",
        "        a = x1 * gate_values1 + x2 * (1 - gate_values1)\n",
        "\n",
        "        # Compute gate for target embeddings\n",
        "        gate_values2 = torch.sigmoid(self.gate_B_fc(x3))\n",
        "        b = x3 * gate_values2 + x4 * (1 - gate_values2)\n",
        "\n",
        "        if return_embeddings:\n",
        "            return a, b\n",
        "\n",
        "        # Compute non-differentiable distance with FAISS (1-to-1)\n",
        "        distance = self.faiss_l2(a, b)\n",
        "\n",
        "        # Pass through a sigmoid layer for binary classification output\n",
        "        out = torch.sigmoid(self.fc(distance.unsqueeze(1)))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpwWQ2ndKGOA"
      },
      "source": [
        "# **Encoder Definition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zYDHQY8fJ6YA"
      },
      "outputs": [],
      "source": [
        "# === Transformer-based Encoder ===\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, nhead=4, num_layers=1):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        # Define a single Transformer encoder layer\n",
        "        # d_model: input/output embedding dimension\n",
        "        # nhead: number of attention heads\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=nhead)\n",
        "\n",
        "        # Stack multiple Transformer encoder layers\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape [batch_size, embedding_dim]\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Encoded output of shape [batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "\n",
        "        # Step 1: Add a sequence length dimension (required format: [seq_len, batch_size, embedding_dim])\n",
        "        x = x.unsqueeze(1)             # Shape: [batch_size, 1, embedding_dim]\n",
        "        x = x.transpose(0, 1)          # Shape: [1, batch_size, embedding_dim] (PyTorch expects seq_len first)\n",
        "\n",
        "        # Step 2: Pass through the Transformer encoder\n",
        "        x = self.transformer_encoder(x)\n",
        "\n",
        "        # Step 3: Remove the sequence dimension to return to shape [batch_size, embedding_dim]\n",
        "        x = x.transpose(0, 1).squeeze(1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLJ5j9FNMVhy"
      },
      "source": [
        "# **Utility functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "k0L86DgUQjMU"
      },
      "outputs": [],
      "source": [
        "def adjacency_matrix_to_undirected_edge_index(adjacency_matrix):\n",
        "    \"\"\"\n",
        "    Converts an adjacency matrix into an undirected edge index for use in graph-based neural networks.\n",
        "\n",
        "    Args:\n",
        "        adjacency_matrix: A 2D list or array representing the adjacency matrix of a graph.\n",
        "\n",
        "    Returns:\n",
        "        edge_index_undirected: A PyTorch tensor representing the undirected edges.\n",
        "    \"\"\"\n",
        "    # Convert each element in the adjacency matrix to an integer (from boolean or float)\n",
        "    adjacency_matrix = [[int(element) for element in sublist] for sublist in adjacency_matrix]\n",
        "\n",
        "    # Convert the adjacency matrix into a PyTorch LongTensor (used for indexing)\n",
        "    edge_index = torch.tensor(adjacency_matrix, dtype=torch.long)\n",
        "\n",
        "    # Transpose the edge_index tensor so that rows represent edges in the form [source, target]\n",
        "    edge_index = edge_index.t().contiguous()\n",
        "\n",
        "    # Convert the directed edge_index into an undirected edge_index, meaning both directions are added (i.e., (i, j) and (j, i))\n",
        "    edge_index_undirected = to_undirected(edge_index)\n",
        "\n",
        "    return edge_index_undirected  # Return the undirected edge index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YvmOxkLcpf9w"
      },
      "outputs": [],
      "source": [
        "def build_indexed_dict(file_path):\n",
        "    \"\"\"\n",
        "    Builds a dictionary with numeric indexes for each key from a JSON file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the JSON file.\n",
        "\n",
        "    Returns:\n",
        "        indexed_dict (dict): A new dictionary where each key from the JSON file is assigned a numeric index.\n",
        "    \"\"\"\n",
        "    # Load the JSON file into a Python dictionary\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Create a new dictionary with numeric indexes as keys and the original JSON keys as values\n",
        "    indexed_dict = {index: key for index, key in enumerate(data.keys())}\n",
        "\n",
        "    return indexed_dict  # Return the newly created dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "QgFINoPGl9Wg"
      },
      "outputs": [],
      "source": [
        "def select_rows_by_index(embedding_vector, index_vector):\n",
        "    \"\"\"\n",
        "    Select rows from an embedding vector using an index vector.\n",
        "\n",
        "    Args:\n",
        "        embedding_vector (torch.Tensor): 2D tensor representing the embedding vector with shape [num_rows, embedding_size].\n",
        "        index_vector (torch.Tensor): 1D tensor representing the index vector.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: New tensor with selected rows from the embedding vector.\n",
        "    \"\"\"\n",
        "    # Use torch.index_select to select the desired rows\n",
        "    new_tensor = torch.index_select(embedding_vector, 0, index_vector)\n",
        "\n",
        "    return new_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "a12L7vEmmCJq"
      },
      "outputs": [],
      "source": [
        "def contrastive_loss(source_embeddings, target_embeddings, labels, margin=1.0):\n",
        "    \"\"\"\n",
        "    Computes the contrastive loss, a type of loss function used to train models in tasks like matching or similarity learning.\n",
        "\n",
        "    Args:\n",
        "        source_embeddings (torch.Tensor): Embeddings of the source graphs, shape [batch_size, embedding_size].\n",
        "        target_embeddings (torch.Tensor): Embeddings of the target graphs, shape [batch_size, embedding_size].\n",
        "        labels (torch.Tensor): Binary labels indicating if the pairs are matched (1) or not (0), shape [batch_size].\n",
        "        margin (float): Margin value for the contrastive loss. Defaults to 1.0.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The contrastive loss value.\n",
        "    \"\"\"\n",
        "    # Calculate the pairwise Euclidean distance between source and target embeddings\n",
        "    distances = F.pairwise_distance(source_embeddings, target_embeddings)\n",
        "\n",
        "    # Compute the contrastive loss:\n",
        "    # - For matched pairs (label == 1), the loss is the squared distance between embeddings.\n",
        "    # - For non-matched pairs (label == 0), the loss is based on how far apart the embeddings are,\n",
        "    #   but penalizes them only if the distance is less than the margin.\n",
        "    loss = torch.mean(\n",
        "        labels * 0.4 * distances.pow(2) +  # For positive pairs, minimize the distance (squared)\n",
        "        (1 - labels) * 0.4 * torch.max(torch.zeros_like(distances), margin - distances).pow(2)  # For negative pairs, maximize the distance (up to the margin)\n",
        "    )\n",
        "\n",
        "    return loss  # Return the computed contrastive loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_ggVYlTiO_WA"
      },
      "outputs": [],
      "source": [
        "def compute_mrr_and_hits(reference_file, predicted_file, output_file, k_values=[1, 5, 10]):\n",
        "    \"\"\"\n",
        "    Compute Mean Reciprocal Rank (MRR) and Hits@k metrics for ontology matching results.\n",
        "\n",
        "    Args:\n",
        "        reference_file (str): Path to the reference test candidate file (usually 'test.cands.tsv').\n",
        "        predicted_file (str): Path to the prediction results (with columns: SrcEntity, TgtEntity, Score).\n",
        "        output_file (str): Path to save ranked candidate predictions with scores.\n",
        "        k_values (list): List of integers specifying which Hits@k metrics to compute.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with MRR and Hits@k scores.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load reference candidate mappings: each row = (SrcEntity, CorrectTgtEntity, [CandidateTgtEntities])\n",
        "    test_candidate_mappings = read_table(reference_file).values.tolist()\n",
        "\n",
        "    # Load predictions and ensure Score is float\n",
        "    predicted_data = pd.read_csv(predicted_file, sep=\"\\t\")\n",
        "    predicted_data[\"Score\"] = predicted_data[\"Score\"].apply(\n",
        "        lambda x: float(x.strip(\"[]\")) if isinstance(x, str) else float(x)\n",
        "    )\n",
        "\n",
        "    # Create a dictionary mapping (SrcEntity, TgtEntity) -> predicted score\n",
        "    score_lookup = {\n",
        "        (row[\"SrcEntity\"], row[\"TgtEntity\"]): row[\"Score\"]\n",
        "        for _, row in predicted_data.iterrows()\n",
        "    }\n",
        "\n",
        "    ranking_results = []\n",
        "\n",
        "    # Rank the candidates for each source entity\n",
        "    for src_ref_class, tgt_ref_class, tgt_cands in test_candidate_mappings:\n",
        "        # Safely parse the candidate list (tgt_cands is a stringified list)\n",
        "        try:\n",
        "            tgt_cands = eval(tgt_cands)\n",
        "        except Exception:\n",
        "            tgt_cands = []\n",
        "\n",
        "        # Score each candidate (use a large negative default if not found)\n",
        "        scored_cands = [\n",
        "            (tgt_cand, score_lookup.get((src_ref_class, tgt_cand), -1e9))\n",
        "            for tgt_cand in tgt_cands\n",
        "        ]\n",
        "\n",
        "        # Sort candidates by score descending\n",
        "        scored_cands = sorted(scored_cands, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Store the ranking result\n",
        "        ranking_results.append((src_ref_class, tgt_ref_class, scored_cands))\n",
        "\n",
        "    # Save ranked predictions for inspection/debugging\n",
        "    pd.DataFrame(ranking_results, columns=[\"SrcEntity\", \"TgtEntity\", \"TgtCandidates\"]).to_csv(\n",
        "        output_file, sep=\"\\t\", index=False\n",
        "    )\n",
        "\n",
        "    # === Evaluation: compute MRR and Hits@k ===\n",
        "    total_entities = len(ranking_results)\n",
        "    reciprocal_ranks = []\n",
        "    hits_at_k = {k: 0 for k in k_values}\n",
        "\n",
        "    for src_entity, tgt_ref_class, tgt_cands in ranking_results:\n",
        "        ranked_candidates = [cand[0] for cand in tgt_cands]  # candidate URIs only\n",
        "        if tgt_ref_class in ranked_candidates:\n",
        "            rank = ranked_candidates.index(tgt_ref_class) + 1\n",
        "            reciprocal_ranks.append(1 / rank)\n",
        "            for k in k_values:\n",
        "                if rank <= k:\n",
        "                    hits_at_k[k] += 1\n",
        "        else:\n",
        "            reciprocal_ranks.append(0)  # No correct match in candidate list\n",
        "\n",
        "    # Compute final metrics\n",
        "    mrr = sum(reciprocal_ranks) / total_entities\n",
        "    hits_at_k = {k: hits / total_entities for k, hits in hits_at_k.items()}\n",
        "\n",
        "    return {\"MRR\": mrr, \"Hits@k\": hits_at_k}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "zmzBcuHZDOs3"
      },
      "outputs": [],
      "source": [
        "def save_gated_embeddings(gated_model, embeddings_src, x_src, embeddings_tgt, x_tgt,\n",
        "                          indexed_dict_src, indexed_dict_tgt,\n",
        "                          output_file_src, output_file_tgt):\n",
        "    \"\"\"\n",
        "    Compute and save the final entity embeddings generated by the GatedCombination model\n",
        "    for both source and target ontologies. Outputs include entity URIs and their final vectors.\n",
        "    Measures and prints the execution time of the entire operation.\n",
        "\n",
        "    Args:\n",
        "        gated_model (nn.Module): The trained GatedCombination model.\n",
        "        embeddings_src (Tensor): Structural embeddings for the source ontology.\n",
        "        x_src (Tensor): Semantic embeddings for the source ontology.\n",
        "        embeddings_tgt (Tensor): Structural embeddings for the target ontology.\n",
        "        x_tgt (Tensor): Semantic embeddings for the target ontology.\n",
        "        indexed_dict_src (dict): Index-to-URI mapping for the source ontology.\n",
        "        indexed_dict_tgt (dict): Index-to-URI mapping for the target ontology.\n",
        "        output_file_src (str): Path to save source embeddings (TSV).\n",
        "        output_file_tgt (str): Path to save target embeddings (TSV).\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    import torch\n",
        "    import time\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Use GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    gated_model = gated_model.to(device)\n",
        "    gated_model.eval()\n",
        "\n",
        "    # Move inputs to the same device\n",
        "    embeddings_src = embeddings_src.to(device)\n",
        "    x_src = x_src.to(device)\n",
        "    embeddings_tgt = embeddings_tgt.to(device)\n",
        "    x_tgt = x_tgt.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # === Source ontology ===\n",
        "        gate_src = torch.sigmoid(gated_model.gate_A_fc(embeddings_src))\n",
        "        final_src = embeddings_src * gate_src + x_src * (1 - gate_src)\n",
        "        final_src = final_src.cpu().numpy()\n",
        "\n",
        "        # === Target ontology ===\n",
        "        gate_tgt = torch.sigmoid(gated_model.gate_B_fc(embeddings_tgt))\n",
        "        final_tgt = embeddings_tgt * gate_tgt + x_tgt * (1 - gate_tgt)\n",
        "        final_tgt = final_tgt.cpu().numpy()\n",
        "\n",
        "    # Create DataFrames with Concept URI and embedding values\n",
        "    df_src = pd.DataFrame(final_src)\n",
        "    df_src.insert(0, \"Concept\", [indexed_dict_src[i] for i in range(len(df_src))])\n",
        "\n",
        "    df_tgt = pd.DataFrame(final_tgt)\n",
        "    df_tgt.insert(0, \"Concept\", [indexed_dict_tgt[i] for i in range(len(df_tgt))])\n",
        "\n",
        "    # Save embeddings to file\n",
        "    df_src.to_csv(output_file_src, sep='\\t', index=False)\n",
        "    df_tgt.to_csv(output_file_tgt, sep='\\t', index=False)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"✅ Gated embeddings saved:\\n- Source: {output_file_src}\\n- Target: {output_file_tgt}\")\n",
        "    print(f\"⏱️ Execution time: {elapsed_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_KZdtF46GHL4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def filter_ignored_class(src_emb_path, tgt_emb_path, src_onto, tgt_onto):\n",
        "    \"\"\"\n",
        "    Filters the source and target embedding files by removing concepts considered \"ignored classes\"\n",
        "    (e.g., owl:Thing, deprecated entities, etc.) based on both source and target ontologies.\n",
        "\n",
        "    Args:\n",
        "        src_emb_path (str): Path to the TSV file containing source embeddings with 'Concept' column.\n",
        "        tgt_emb_path (str): Path to the TSV file containing target embeddings with 'Concept' column.\n",
        "        src_onto (Ontology): Source ontology object loaded with DeepOnto.\n",
        "        tgt_onto (Ontology): Target ontology object loaded with DeepOnto.\n",
        "\n",
        "    Returns:\n",
        "        (str, str): Paths to the cleaned source and target embedding files.\n",
        "    \"\"\"\n",
        "\n",
        "    # === Load the embedding files ===\n",
        "    df_src = pd.read_csv(src_emb_path, sep='\\t', dtype=str)\n",
        "    print(f\"🔍 Initial source file: {len(df_src)} rows\")\n",
        "\n",
        "    df_tgt = pd.read_csv(tgt_emb_path, sep='\\t', dtype=str)\n",
        "    print(f\"🔍 Initial target file: {len(df_tgt)} rows\")\n",
        "\n",
        "    # === Step 1: Retrieve ignored classes from both ontologies ===\n",
        "    ignored_class_index = get_ignored_class_index(src_onto)  # e.g., owl:Thing, non-usable classes\n",
        "    ignored_class_index.update(get_ignored_class_index(tgt_onto))  # Merge with target ontology's ignored classes\n",
        "    ignored_uris = set(str(uri).strip() for uri in ignored_class_index)\n",
        "\n",
        "    # === Step 2: Remove rows where the 'Concept' column matches ignored URIs ===\n",
        "    df_src_cleaned = df_src[~df_src['Concept'].isin(ignored_uris)].reset_index(drop=True)\n",
        "    df_tgt_cleaned = df_tgt[~df_tgt['Concept'].isin(ignored_uris)].reset_index(drop=True)\n",
        "\n",
        "    print(f\"✅ Source after removing ignored classes: {len(df_src_cleaned)} rows\")\n",
        "    print(f\"✅ Target after removing ignored classes: {len(df_tgt_cleaned)} rows\")\n",
        "\n",
        "    # === Step 3: Save the cleaned embedding files ===\n",
        "    output_file_src = src_emb_path.replace(\".tsv\", \"_cleaned.tsv\")\n",
        "    output_file_tgt = tgt_emb_path.replace(\".tsv\", \"_cleaned.tsv\")\n",
        "\n",
        "    df_src_cleaned.to_csv(output_file_src, sep='\\t', index=False)\n",
        "    df_tgt_cleaned.to_csv(output_file_tgt, sep='\\t', index=False)\n",
        "\n",
        "    print(f\"📁 Cleaned source file saved to: {output_file_src}\")\n",
        "    print(f\"📁 Cleaned target file saved to: {output_file_tgt}\")\n",
        "\n",
        "    return output_file_src, output_file_tgt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "_9YDcnTbKaHk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "def encode_embeddings_with_concept_column(encoder_model, input_file, output_file):\n",
        "    \"\"\"\n",
        "    Applies an encoder model to a set of embeddings (while preserving the 'Concept' column),\n",
        "    and saves the encoded results in the same tabular format.\n",
        "\n",
        "    Args:\n",
        "        encoder_model: A PyTorch model (e.g., LinearEncoder, MLPEncoder, etc.)\n",
        "        input_file (str): Path to the input TSV file containing 'Concept' and embedding vectors.\n",
        "        output_file (str): Path to save the encoded embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    # Select device (GPU if available, else CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Move the encoder model to the selected device and set it to evaluation mode\n",
        "    encoder_model = encoder_model.to(device)\n",
        "    encoder_model.eval()\n",
        "\n",
        "    # Load the input TSV file containing concept URIs and embeddings\n",
        "    df = pd.read_csv(input_file, sep='\\t')\n",
        "\n",
        "    # Extract the 'Concept' column to preserve URIs\n",
        "    concepts = df['Concept'].tolist()\n",
        "\n",
        "    # Extract the numerical embedding values (excluding the 'Concept' column)\n",
        "    embedding_values = df.drop(columns=['Concept']).values\n",
        "\n",
        "    # Convert the embedding matrix into a PyTorch tensor and move to the device\n",
        "    embeddings = torch.FloatTensor(embedding_values).to(device)\n",
        "\n",
        "    # Pass the embeddings through the encoder model without computing gradients\n",
        "    with torch.no_grad():\n",
        "        encoded = encoder_model(embeddings).cpu().numpy()\n",
        "\n",
        "    # Reconstruct a new DataFrame with the encoded vectors and corresponding URIs\n",
        "    df_encoded = pd.DataFrame(encoded, columns=[f'dim_{i}' for i in range(encoded.shape[1])])\n",
        "    df_encoded.insert(0, \"Concept\", concepts)  # Re-insert the 'Concept' column at the first position\n",
        "\n",
        "    # Save the encoded embeddings to a TSV file\n",
        "    df_encoded.to_csv(output_file, sep='\\t', index=False)\n",
        "    print(f\"✅ Encoded embeddings saved to: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HigIe6n_lQ8X"
      },
      "source": [
        "# **FAISS Similarity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "zaX6JH9wj_WR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "import time\n",
        "\n",
        "def load_embeddings(src_emb_path, tgt_emb_path):\n",
        "    df_src = pd.read_csv(src_emb_path, sep='\\t')\n",
        "    df_tgt = pd.read_csv(tgt_emb_path, sep='\\t')\n",
        "    uris_src = df_src[\"Concept\"].values\n",
        "    uris_tgt = df_tgt[\"Concept\"].values\n",
        "    src_vecs = df_src.drop(columns=[\"Concept\"]).values.astype('float32')\n",
        "    tgt_vecs = df_tgt.drop(columns=[\"Concept\"]).values.astype('float32')\n",
        "    return uris_src, uris_tgt, src_vecs, tgt_vecs\n",
        "\n",
        "def save_results(uris_src, uris_tgt, indices, scores, output_file, top_k):\n",
        "    rows = []\n",
        "    for i, (ind_row, score_row) in enumerate(zip(indices, scores)):\n",
        "        src_uri = uris_src[i]\n",
        "        for j, tgt_idx in enumerate(ind_row):\n",
        "            tgt_uri = uris_tgt[tgt_idx]\n",
        "            score = score_row[j]\n",
        "            rows.append((src_uri, tgt_uri, score))\n",
        "    df_result = pd.DataFrame(rows, columns=[\"SrcEntity\", \"TgtEntity\", \"Score\"])\n",
        "    df_result.to_csv(output_file, sep='\\t', index=False)\n",
        "    print(f\"Top-{top_k} FAISS similarity results saved to: {output_file}\")\n",
        "\n",
        "def topk_faiss_l2(src_emb_path, tgt_emb_path, top_k=15, output_file=\"topk_l2.tsv\"):\n",
        "    print(\"🔹 Using L2 (Euclidean) distance with FAISS\")\n",
        "    start = time.time()\n",
        "\n",
        "    uris_src, uris_tgt, src_vecs, tgt_vecs = load_embeddings(src_emb_path, tgt_emb_path)\n",
        "    dim = src_vecs.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(tgt_vecs)\n",
        "    distances, indices = index.search(src_vecs, top_k)\n",
        "    similarity_scores = 1 / (1 + distances)\n",
        "\n",
        "    save_results(uris_src, uris_tgt, indices, similarity_scores, output_file, top_k)\n",
        "\n",
        "    print(f\"⏱️ Execution time: {time.time() - start:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjUYOFO7pdCg"
      },
      "source": [
        "# **Mappings Evaluation Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6m04nFw_R00"
      },
      "source": [
        "# **Precision, Recall, F1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GW0Am-TmVMR"
      },
      "source": [
        "### Evaluation Strategy and Filtering Justification\n",
        "\n",
        "### Filtering Justification\n",
        "\n",
        "In the `evaluate_predictions` function, two important filtering steps are applied to ensure that the evaluation metrics (such as Precision, Recall, and F1-score) accurately reflect the model's performance:\n",
        "\n",
        "\n",
        "#### 1. Filtering Out Training-Only Entities\n",
        "\n",
        "We remove all predicted mappings involving source or target entities that are present **only in the training set** and not in the test set.\n",
        "\n",
        "This step is critical because:\n",
        "\n",
        "- In some datasets like **Bio-ML**, the same entity can appear in both training and test sets, although with **different correspondences**.\n",
        "- If we don't remove training-only entities, it can lead to **label leakage** and **metric distortion**.\n",
        "\n",
        "#### 2. Filtering on `SrcEntity` present in the test set\n",
        "\n",
        "The second step keeps only the predictions where the `SrcEntity` is included in the test reference set.\n",
        "\n",
        "- This eliminates **non-evaluable false positives**, i.e., predicted mappings for source entities that do not appear in the test set and therefore have no ground-truth correspondences. Including such predictions **unfairly penalizes precision and F1-score**, even though they are technically not verifiable errors.\n",
        "\n",
        "- It focuses the evaluation on entities with defined ground-truth mappings, which is critical for computing metrics such as :\n",
        "\n",
        "$P_{\\text{test}} = \\frac{|\\mathcal{M}_{\\text{out}} \\cap \\mathcal{M}_{\\text{test}}|}{|\\mathcal{M}_{\\text{out}} \\setminus (\\mathcal{M}_{\\text{ref}} \\setminus \\mathcal{M}_{\\text{test}})|}$.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "CdP6iYirLJW6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def evaluate_predictions(\n",
        "    topk_file,\n",
        "    train_file,\n",
        "    test_file,\n",
        "    src_onto,\n",
        "    tgt_onto,\n",
        "    threshold=0.0\n",
        "):\n",
        "    # === Step 1: Load input files ===\n",
        "    df = pd.read_csv(topk_file, sep='\\t', dtype=str)\n",
        "    train_df = pd.read_csv(train_file, sep=\"\\t\", dtype=str)\n",
        "    test_df = pd.read_csv(test_file, sep=\"\\t\", dtype=str)\n",
        "\n",
        "    # === Step 2: Remove URIs only present in training set ===\n",
        "    train_uris = set(train_df['SrcEntity']) | set(train_df['TgtEntity'])\n",
        "    test_uris = set(test_df['SrcEntity']) | set(test_df['TgtEntity'])\n",
        "    uris_to_exclude = train_uris - test_uris\n",
        "    df = df[~(df['SrcEntity'].isin(uris_to_exclude) | df['TgtEntity'].isin(uris_to_exclude))].reset_index(drop=True)\n",
        "\n",
        "    # === Step 3: Keep only source entities from the test set ===\n",
        "    src_entities_test = set(test_df['SrcEntity'])\n",
        "    df = df[df['SrcEntity'].isin(src_entities_test)].reset_index(drop=True)\n",
        "\n",
        "    # === Step 4: Save filtered Top-K predictions ===\n",
        "    output_file1 = topk_file.replace(\".tsv\", \"_filtered.tsv\")\n",
        "    df.to_csv(output_file1, sep='\\t', index=False)\n",
        "\n",
        "    # === Step 5: Convert score column to float\n",
        "    df['Score'] = df['Score'].apply(lambda x: float(x.strip(\"[]\")) if isinstance(x, str) else float(x))\n",
        "    df_sorted = df.sort_values(by='Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # === Step 6: Apply 1-1 constraint (greedy matching)\n",
        "    matched_sources = set()\n",
        "    matched_targets = set()\n",
        "    result = []\n",
        "\n",
        "    for _, row in df_sorted.iterrows():\n",
        "        src, tgt, score = row['SrcEntity'], row['TgtEntity'], row['Score']\n",
        "        if src not in matched_sources and tgt not in matched_targets and score >= threshold:\n",
        "            result.append((src, tgt, score))\n",
        "            matched_sources.add(src)\n",
        "            matched_targets.add(tgt)\n",
        "\n",
        "    matching_results_df = pd.DataFrame(result, columns=['SrcEntity', 'TgtEntity', 'Score'])\n",
        "\n",
        "    # === Step 7: Save Top-1 predictions ===\n",
        "    output_file2 = topk_file.replace(\".tsv\", f\"_predictions.tsv\")\n",
        "    matching_results_df.to_csv(output_file2, sep='\\t', index=False)\n",
        "    print(f\"📁 Top-1 file saved: {output_file2}\")\n",
        "    print(f\"📌 Number of predictions in output: {len(matching_results_df)}\")\n",
        "\n",
        "    # === Step 8: Evaluate predictions ===\n",
        "    preds = EntityMapping.read_table_mappings(output_file2)\n",
        "    refs = ReferenceMapping.read_table_mappings(test_file)\n",
        "    results = AlignmentEvaluator.f1(preds, refs)\n",
        "\n",
        "    preds_set = {p.to_tuple() for p in preds}\n",
        "    refs_set = {r.to_tuple() for r in refs}\n",
        "    correct = len(preds_set & refs_set)\n",
        "\n",
        "    print(f\"🎯 Correct mappings (Top-1): {correct}\")\n",
        "    print(f\"📊 Evaluation (P / R / F1): {results}\")\n",
        "\n",
        "    return output_file2, results, correct\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPuzmu6f_Y8W"
      },
      "source": [
        "# **Precision@k, Recall@k, F1@k**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "UwdxR-ZzAgS3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "def evaluate_topk(topk_file, train_file, test_file, k=1, threshold=0.0):\n",
        "    \"\"\"\n",
        "    Evaluate Top-K predictions using Precision, Recall, and F1-score,\n",
        "    after filtering out training-only URIs, keeping only test sources, and applying 1-1 constraint.\n",
        "\n",
        "    Args:\n",
        "        topk_file (str): Path to the top-k prediction file (TSV with SrcEntity, TgtEntity, Score)\n",
        "        train_file (str): Path to the training mappings file (TSV)\n",
        "        test_file (str): Path to the test mappings file (TSV)\n",
        "        k (int): Value of K for top-k evaluation\n",
        "        threshold (float): Minimum score to consider a prediction valid\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing Precision@K, Recall@K, and F1@K\n",
        "    \"\"\"\n",
        "\n",
        "    # === Step 1: Load input files ===\n",
        "    df = pd.read_csv(topk_file, sep='\\t', dtype=str)\n",
        "    train_df = pd.read_csv(train_file, sep='\\t', dtype=str)\n",
        "    test_df = pd.read_csv(test_file, sep='\\t', dtype=str)\n",
        "\n",
        "    # === Step 2: Remove URIs only present in the training set ===\n",
        "    train_uris = set(train_df['SrcEntity']) | set(train_df['TgtEntity'])\n",
        "    test_uris = set(test_df['SrcEntity']) | set(test_df['TgtEntity'])\n",
        "    uris_to_exclude = train_uris - test_uris\n",
        "    df = df[~(df['SrcEntity'].isin(uris_to_exclude) | df['TgtEntity'].isin(uris_to_exclude))].reset_index(drop=True)\n",
        "\n",
        "    # === Step 3: Keep only source entities from the test set ===\n",
        "    src_entities_test = set(test_df['SrcEntity'])\n",
        "    df = df[df['SrcEntity'].isin(src_entities_test)].reset_index(drop=True)\n",
        "\n",
        "    # === Step 4: Convert score column to float and sort ===\n",
        "    df['Score'] = df['Score'].apply(lambda x: float(x.strip(\"[]\")) if isinstance(x, str) else float(x))\n",
        "    df_sorted = df.sort_values(by='Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # === Step 5: Apply 1-to-1 constraint (greedy strategy with optional threshold)\n",
        "    matched_sources = set()\n",
        "    matched_targets = set()\n",
        "    result = []\n",
        "\n",
        "    for _, row in df_sorted.iterrows():\n",
        "        src, tgt, score = row['SrcEntity'], row['TgtEntity'], row['Score']\n",
        "        if src not in matched_sources and tgt not in matched_targets and score >= threshold:\n",
        "            result.append((src, tgt, score))\n",
        "            matched_sources.add(src)\n",
        "            matched_targets.add(tgt)\n",
        "\n",
        "    # === Step 6: Create and save Top-1 prediction dataframe\n",
        "    matching_results_df = pd.DataFrame(result, columns=['SrcEntity', 'TgtEntity', 'Score'])\n",
        "    output_file = topk_file.replace(\".tsv\", \"_predictions.tsv\")\n",
        "    matching_results_df.to_csv(output_file, sep='\\t', index=False)\n",
        "    print(f\"📁 Top-1 file saved: {output_file}\")\n",
        "    print(f\"📌 Number of predictions in output: {len(matching_results_df)}\")\n",
        "\n",
        "    # === Step 7: Build reference dictionary from test set\n",
        "    ref_dict = defaultdict(set)\n",
        "    for _, row in test_df.iterrows():\n",
        "        ref_dict[row['SrcEntity']].add(row['TgtEntity'])\n",
        "\n",
        "    # === Step 8: Select Top-K from the 1-1 results\n",
        "    matching_results_df['Score'] = matching_results_df['Score'].astype(float)\n",
        "    topk_df = matching_results_df.sort_values(by='Score', ascending=False).groupby('SrcEntity').head(k)\n",
        "\n",
        "    # === Step 9: Compute metrics\n",
        "    total_tp = total_pred = total_ref = 0\n",
        "\n",
        "    for src, group in topk_df.groupby('SrcEntity'):\n",
        "        predicted = set(group['TgtEntity'])\n",
        "        true = ref_dict.get(src, set())\n",
        "        tp = len(predicted & true)\n",
        "        total_tp += tp\n",
        "        total_pred += len(predicted)\n",
        "        total_ref += len(true)\n",
        "\n",
        "    precision = total_tp / total_pred if total_pred else 0.0\n",
        "    recall = total_tp / total_ref if total_ref else 0.0\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-8) if precision + recall > 0 else 0.0\n",
        "\n",
        "    return {\n",
        "        f'Precision@{k}': round(precision, 4),\n",
        "        f'Recall@{k}': round(recall, 4),\n",
        "        f'F1@{k}': round(f1, 4)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "WAsAVJEy3o9a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "def encode_embeddings_with_concept_column(encoder_model, input_file, output_file):\n",
        "    \"\"\"\n",
        "    Applies an encoder model to a set of embeddings (while preserving the 'Concept' column),\n",
        "    and saves the encoded results in the same tabular format.\n",
        "\n",
        "    Args:\n",
        "        encoder_model: A PyTorch model (e.g., LinearEncoder, MLPEncoder, etc.)\n",
        "        input_file (str): Path to the input TSV file containing 'Concept' and embedding vectors.\n",
        "        output_file (str): Path to save the encoded embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    # Select device (GPU if available, else CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Move the encoder model to the selected device and set it to evaluation mode\n",
        "    encoder_model = encoder_model.to(device)\n",
        "    encoder_model.eval()\n",
        "\n",
        "    # Load the input TSV file containing concept URIs and embeddings\n",
        "    df = pd.read_csv(input_file, sep='\\t')\n",
        "\n",
        "    # Extract the 'Concept' column to preserve URIs\n",
        "    concepts = df['Concept'].tolist()\n",
        "\n",
        "    # Extract the numerical embedding values (excluding the 'Concept' column)\n",
        "    embedding_values = df.drop(columns=['Concept']).values\n",
        "\n",
        "    # Convert the embedding matrix into a PyTorch tensor and move to the device\n",
        "    embeddings = torch.FloatTensor(embedding_values).to(device)\n",
        "\n",
        "    # Pass the embeddings through the encoder model without computing gradients\n",
        "    with torch.no_grad():\n",
        "        encoded = encoder_model(embeddings).cpu().numpy()\n",
        "\n",
        "    # Reconstruct a new DataFrame with the encoded vectors and corresponding URIs\n",
        "    df_encoded = pd.DataFrame(encoded, columns=[f'dim_{i}' for i in range(encoded.shape[1])])\n",
        "    df_encoded.insert(0, \"Concept\", concepts)  # Re-insert the 'Concept' column at the first position\n",
        "\n",
        "    # Save the encoded embeddings to a TSV file\n",
        "    df_encoded.to_csv(output_file, sep='\\t', index=False)\n",
        "    print(f\"✅ Encoded embeddings saved to: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HyWMsw1MVhz"
      },
      "source": [
        "# **Main Code**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC37FlwGDqGM"
      },
      "source": [
        "\n",
        "\n",
        "# Reading semantic node embeddings provided by the ENE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "FuEfSnw5mod0"
      },
      "outputs": [],
      "source": [
        "# Read the source embeddings from a CSV file into a pandas DataFrame\n",
        "df_embbedings_src = pd.read_csv(src_Emb, index_col=0)\n",
        "\n",
        "# Convert the DataFrame to a NumPy array, which will remove the index and store the data as a raw matrix\n",
        "numpy_array = df_embbedings_src.to_numpy()\n",
        "\n",
        "# Convert the NumPy array into a PyTorch FloatTensor, which is the format required for PyTorch operations\n",
        "x_src = torch.FloatTensor(numpy_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "STUwqMUXmlG2"
      },
      "outputs": [],
      "source": [
        "# Read the target embeddings from a CSV file into a pandas DataFrame\n",
        "df_embbedings_tgt = pd.read_csv(tgt_Emb, index_col=0)\n",
        "\n",
        "# Convert the DataFrame to a NumPy array, which removes the index and converts the data to a raw matrix\n",
        "numpy_array = df_embbedings_tgt.to_numpy()\n",
        "\n",
        "# Convert the NumPy array into a PyTorch FloatTensor, which is required for PyTorch operations\n",
        "x_tgt = torch.FloatTensor(numpy_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZIu9P08DqGN"
      },
      "source": [
        "# Reading adjacency Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "pH69Up40mycz"
      },
      "outputs": [],
      "source": [
        "# Read the source adjacency matrix from a CSV file into a pandas DataFrame\n",
        "df_ma1 = pd.read_csv(src_Adjacence, index_col=0)\n",
        "\n",
        "# Convert the DataFrame to a list of lists (Python native list format)\n",
        "ma1 = df_ma1.values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "hYCmAO5Ymzpl"
      },
      "outputs": [],
      "source": [
        "# Read the target adjacency matrix from a CSV file into a pandas DataFrame\n",
        "df_ma2 = pd.read_csv(tgt_Adjacence, index_col=0)\n",
        "\n",
        "# Convert the DataFrame to a list of lists (Python native list format)\n",
        "ma2 = df_ma2.values.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSyksqh3TrU-"
      },
      "source": [
        "# Convert Adjacency matrix (in list format) to an undirected edge index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "uVt-Pce5m5ll"
      },
      "outputs": [],
      "source": [
        "# Convert the source adjacency matrix (in list format) to an undirected edge index for PyTorch Geometric\n",
        "edge_src = adjacency_matrix_to_undirected_edge_index(ma1)\n",
        "\n",
        "# Convert the target adjacency matrix (in list format) to an undirected edge index for PyTorch Geometric\n",
        "edge_tgt = adjacency_matrix_to_undirected_edge_index(ma2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9wMTRdqT4aY"
      },
      "source": [
        "# GIT Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "eqiEKCLSMVh3"
      },
      "outputs": [],
      "source": [
        "def train_model_gnn(model, x_src, edge_src, x_tgt, edge_tgt,\n",
        "                    tensor_term1, tensor_term2, tensor_score,\n",
        "                    learning_rate, weight_decay_value, num_epochs, print_interval=10):\n",
        "    \"\"\"\n",
        "    Trains a graph neural network (GNN) model using source and target embeddings and contrastive loss.\n",
        "\n",
        "    Args:\n",
        "        model: The GNN model to be trained.\n",
        "        x_src (torch.Tensor): Source node embeddings.\n",
        "        edge_src (torch.Tensor): Source graph edges.\n",
        "        x_tgt (torch.Tensor): Target node embeddings.\n",
        "        edge_tgt (torch.Tensor): Target graph edges.\n",
        "        tensor_term1 (torch.Tensor): Indices of the source nodes to be compared.\n",
        "        tensor_term2 (torch.Tensor): Indices of the target nodes to be compared.\n",
        "        tensor_score (torch.Tensor): Labels indicating if the pairs are matched (1) or not (0).\n",
        "        learning_rate (float): Learning rate for the optimizer.\n",
        "        weight_decay_value (float): Weight decay (L2 regularization) value for the optimizer.\n",
        "        num_epochs (int): Number of epochs for training.\n",
        "        print_interval (int): Interval at which training progress is printed (every `print_interval` epochs).\n",
        "\n",
        "    Returns:\n",
        "        model: The trained GNN model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Set device (GPU or CPU) for computation\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Step 2: Move the model and all inputs to the selected device\n",
        "    model.to(device)\n",
        "    x_tgt = x_tgt.to(device)               # Target node embeddings\n",
        "    edge_tgt = edge_tgt.to(device)         # Target graph edges\n",
        "    x_src = x_src.to(device)               # Source node embeddings\n",
        "    edge_src = edge_src.to(device)         # Source graph edges\n",
        "    tensor_term1 = tensor_term1.to(device) # Indices for source nodes\n",
        "    tensor_term2 = tensor_term2.to(device) # Indices for target nodes\n",
        "    tensor_score = tensor_score.to(device) # Ground truth labels\n",
        "\n",
        "    # Step 3: Define optimizer with learning rate and regularization\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay_value)\n",
        "\n",
        "    # Step 4: Initialize list to store training losses\n",
        "    train_losses = []\n",
        "\n",
        "    # Record the start time of training\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Step 5: Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        # Zero out gradients from the previous iteration\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: Compute embeddings for source and target graphs\n",
        "        out1 = model(x_src, edge_src)  # Updated source embeddings\n",
        "        out2 = model(x_tgt, edge_tgt)  # Updated target embeddings\n",
        "\n",
        "        # Extract specific rows of embeddings for terms being compared\n",
        "        src_embeddings = select_rows_by_index(out1, tensor_term1)\n",
        "        tgt_embeddings = select_rows_by_index(out2, tensor_term2)\n",
        "\n",
        "        # Compute contrastive loss based on the embeddings and ground truth labels\n",
        "        loss = contrastive_loss(src_embeddings, tgt_embeddings, tensor_score)\n",
        "\n",
        "        # Backward pass: Compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the model's parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Append the loss for this iteration to the list\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        # Print loss every `print_interval` epochs\n",
        "        if (epoch + 1) % print_interval == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item()}\")\n",
        "\n",
        "    # Step 6: Record end time of training\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Step 7: Plot the training loss over time\n",
        "    plt.semilogy(range(1, num_epochs + 1), train_losses, label=\"Training Loss\", marker='o')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Print the total training time\n",
        "    training_time = end_time - start_time\n",
        "    print(f\"Training complete! Total training time: {training_time:.2f} seconds\")\n",
        "\n",
        "    # Step 8: Return the trained model\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "6_tzUG_emtBg"
      },
      "outputs": [],
      "source": [
        "# Initialize the GIT_mod model with the dimensionality of the target embeddings\n",
        "# The first argument is the dimensionality of the target node embeddings (x_tgt.shape[1])\n",
        "# The second argument (1) represents the number of RGIT layers in the model\n",
        "GIT_model = RGIT_mod(x_tgt.shape[1], 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "wVo-s7UQssSp"
      },
      "outputs": [],
      "source": [
        "# Reading the training pairs from a CSV file into a pandas DataFrame\n",
        "df_embbedings = pd.read_csv(train_file, index_col=0)\n",
        "\n",
        "# Extract the 'SrcEntity' and 'TgtEntity' columns as NumPy arrays and convert them to integers\n",
        "tensor_term1 = df_embbedings['SrcEntity'].values.astype(int)  # Source entity indices\n",
        "tensor_term2 = df_embbedings['TgtEntity'].values.astype(int)  # Target entity indices\n",
        "\n",
        "# Extract the 'Score' column as a NumPy array and convert it to floats\n",
        "tensor_score = df_embbedings['Score'].values.astype(float)  # Scores (labels) indicating if pairs match (1) or not (0)\n",
        "\n",
        "# Convert the NumPy arrays to PyTorch LongTensors (for indices) and FloatTensors (for scores)\n",
        "tensor_term1_o = torch.from_numpy(tensor_term1).type(torch.LongTensor)  # Source entity tensor\n",
        "tensor_term2_o = torch.from_numpy(tensor_term2).type(torch.LongTensor)  # Target entity tensor\n",
        "tensor_score_o = torch.from_numpy(tensor_score).type(torch.FloatTensor)  # Score tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "agHlFNesMVh3",
        "outputId": "85326b2b-b04d-45f6-c0de-efe97b059db9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Training Loss: 0.003425230970606208\n",
            "Epoch [20/1000], Training Loss: 0.002716905903071165\n",
            "Epoch [30/1000], Training Loss: 0.0023135575465857983\n",
            "Epoch [40/1000], Training Loss: 0.0020285279024392366\n",
            "Epoch [50/1000], Training Loss: 0.0018252474255859852\n",
            "Epoch [60/1000], Training Loss: 0.001670632278546691\n",
            "Epoch [70/1000], Training Loss: 0.0015458388952538371\n",
            "Epoch [80/1000], Training Loss: 0.0014420008519664407\n",
            "Epoch [90/1000], Training Loss: 0.0013542650267481804\n",
            "Epoch [100/1000], Training Loss: 0.001279068412259221\n",
            "Epoch [110/1000], Training Loss: 0.0012133019044995308\n",
            "Epoch [120/1000], Training Loss: 0.0011558611877262592\n",
            "Epoch [130/1000], Training Loss: 0.0011047751177102327\n",
            "Epoch [140/1000], Training Loss: 0.001059117610566318\n",
            "Epoch [150/1000], Training Loss: 0.0010179919190704823\n",
            "Epoch [160/1000], Training Loss: 0.0009804170113056898\n",
            "Epoch [170/1000], Training Loss: 0.0009461171575821936\n",
            "Epoch [180/1000], Training Loss: 0.0009143849601969123\n",
            "Epoch [190/1000], Training Loss: 0.0008850962622091174\n",
            "Epoch [200/1000], Training Loss: 0.0008579313871450722\n",
            "Epoch [210/1000], Training Loss: 0.0008324655937030911\n",
            "Epoch [220/1000], Training Loss: 0.0008086822926998138\n",
            "Epoch [230/1000], Training Loss: 0.0007861995836719871\n",
            "Epoch [240/1000], Training Loss: 0.0007649374892935157\n",
            "Epoch [250/1000], Training Loss: 0.0007447587558999658\n",
            "Epoch [260/1000], Training Loss: 0.0007254744414240122\n",
            "Epoch [270/1000], Training Loss: 0.0007067961851134896\n",
            "Epoch [280/1000], Training Loss: 0.000688831030856818\n",
            "Epoch [290/1000], Training Loss: 0.0006716500502079725\n",
            "Epoch [300/1000], Training Loss: 0.0006550534162670374\n",
            "Epoch [310/1000], Training Loss: 0.0006390481721609831\n",
            "Epoch [320/1000], Training Loss: 0.0006236120243556798\n",
            "Epoch [330/1000], Training Loss: 0.0006085873465053737\n",
            "Epoch [340/1000], Training Loss: 0.0005940845003351569\n",
            "Epoch [350/1000], Training Loss: 0.0005800288054160774\n",
            "Epoch [360/1000], Training Loss: 0.0005664609489031136\n",
            "Epoch [370/1000], Training Loss: 0.0005532641080208123\n",
            "Epoch [380/1000], Training Loss: 0.0005405707634054124\n",
            "Epoch [390/1000], Training Loss: 0.0005283392383717\n",
            "Epoch [400/1000], Training Loss: 0.0005165100446902215\n",
            "Epoch [410/1000], Training Loss: 0.0005051138577982783\n",
            "Epoch [420/1000], Training Loss: 0.0004941308870911598\n",
            "Epoch [430/1000], Training Loss: 0.0004835090658161789\n",
            "Epoch [440/1000], Training Loss: 0.00047316544805653393\n",
            "Epoch [450/1000], Training Loss: 0.0004632054769899696\n",
            "Epoch [460/1000], Training Loss: 0.0004535694024525583\n",
            "Epoch [470/1000], Training Loss: 0.00044417227036319673\n",
            "Epoch [480/1000], Training Loss: 0.0004350969975348562\n",
            "Epoch [490/1000], Training Loss: 0.00042624041088856757\n",
            "Epoch [500/1000], Training Loss: 0.0004176422953605652\n",
            "Epoch [510/1000], Training Loss: 0.00040929761598818004\n",
            "Epoch [520/1000], Training Loss: 0.0004011217679362744\n",
            "Epoch [530/1000], Training Loss: 0.0003930824459530413\n",
            "Epoch [540/1000], Training Loss: 0.0003852138470392674\n",
            "Epoch [550/1000], Training Loss: 0.0003775661753024906\n",
            "Epoch [560/1000], Training Loss: 0.0003700334345921874\n",
            "Epoch [570/1000], Training Loss: 0.0003629947896115482\n",
            "Epoch [580/1000], Training Loss: 0.00035538579686544836\n",
            "Epoch [590/1000], Training Loss: 0.000349448382621631\n",
            "Epoch [600/1000], Training Loss: 0.00034207123098894954\n",
            "Epoch [610/1000], Training Loss: 0.0003361284907441586\n",
            "Epoch [620/1000], Training Loss: 0.00032996697700582445\n",
            "Epoch [630/1000], Training Loss: 0.00032359999022446573\n",
            "Epoch [640/1000], Training Loss: 0.0003176543687004596\n",
            "Epoch [650/1000], Training Loss: 0.0003127435629721731\n",
            "Epoch [660/1000], Training Loss: 0.0003084763593506068\n",
            "Epoch [670/1000], Training Loss: 0.0003038003051187843\n",
            "Epoch [680/1000], Training Loss: 0.0002982996229548007\n",
            "Epoch [690/1000], Training Loss: 0.00029315680149011314\n",
            "Epoch [700/1000], Training Loss: 0.00028885682695545256\n",
            "Epoch [710/1000], Training Loss: 0.0002853522601071745\n",
            "Epoch [720/1000], Training Loss: 0.0002826041600201279\n",
            "Epoch [730/1000], Training Loss: 0.0002779820642899722\n",
            "Epoch [740/1000], Training Loss: 0.0002734329318627715\n",
            "Epoch [750/1000], Training Loss: 0.0002697500749491155\n",
            "Epoch [760/1000], Training Loss: 0.0002673788112588227\n",
            "Epoch [770/1000], Training Loss: 0.0002653780684340745\n",
            "Epoch [780/1000], Training Loss: 0.0002620607556309551\n",
            "Epoch [790/1000], Training Loss: 0.00025829742662608624\n",
            "Epoch [800/1000], Training Loss: 0.0002551300567574799\n",
            "Epoch [810/1000], Training Loss: 0.00025349127827212214\n",
            "Epoch [820/1000], Training Loss: 0.0002520237467251718\n",
            "Epoch [830/1000], Training Loss: 0.0002493293723091483\n",
            "Epoch [840/1000], Training Loss: 0.0002456989896018058\n",
            "Epoch [850/1000], Training Loss: 0.0002430747845210135\n",
            "Epoch [860/1000], Training Loss: 0.00024243925872724503\n",
            "Epoch [870/1000], Training Loss: 0.00024079339345917106\n",
            "Epoch [880/1000], Training Loss: 0.00023707312357146293\n",
            "Epoch [890/1000], Training Loss: 0.00023367950052488595\n",
            "Epoch [900/1000], Training Loss: 0.00023208477068692446\n",
            "Epoch [910/1000], Training Loss: 0.0002314153971383348\n",
            "Epoch [920/1000], Training Loss: 0.00022841227473691106\n",
            "Epoch [930/1000], Training Loss: 0.00022429830278269947\n",
            "Epoch [940/1000], Training Loss: 0.00022119974892120808\n",
            "Epoch [950/1000], Training Loss: 0.00022014975547790527\n",
            "Epoch [960/1000], Training Loss: 0.00021884047600906342\n",
            "Epoch [970/1000], Training Loss: 0.00021503563039004803\n",
            "Epoch [980/1000], Training Loss: 0.00021096706041134894\n",
            "Epoch [990/1000], Training Loss: 0.0002084648294840008\n",
            "Epoch [1000/1000], Training Loss: 0.00020801875507459044\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete! Total training time: 600.16 seconds\n"
          ]
        }
      ],
      "source": [
        "# Train the GNN model using the provided source and target graph embeddings, edges, and training data\n",
        "trained_model = train_model_gnn(\n",
        "    model=GIT_model,                # The GNN model to be trained (initialized earlier)\n",
        "    x_src=x_src,                    # Source node embeddings (tensor for source graph)\n",
        "    edge_src=edge_src,              # Source graph edges (undirected edge index for source graph)\n",
        "    x_tgt=x_tgt,                    # Target node embeddings (tensor for target graph)\n",
        "    edge_tgt=edge_tgt,              # Target graph edges (undirected edge index for target graph)\n",
        "    tensor_term1=tensor_term1_o,    # Indices of source entities for training\n",
        "    tensor_term2=tensor_term2_o,    # Indices of target entities for training\n",
        "    tensor_score=tensor_score_o,    # Scores (labels) indicating if pairs match (1) or not (0)\n",
        "    learning_rate=0.0001,            # Learning rate for the Adam optimizer\n",
        "    weight_decay_value=1e-4,        # Weight decay for L2 regularization to prevent overfitting\n",
        "    num_epochs=1000,                # Number of training epochs\n",
        "    print_interval=10               # Interval at which to print training progress (every 10 epochs)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYyhvjcTUaae"
      },
      "source": [
        "# GIT Application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "BZ_VqP6tq6iD"
      },
      "outputs": [],
      "source": [
        "# Determine if a GPU is available and move the computations to it; otherwise, use the CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Assuming the model has been trained and hyperparameters (x_src, edge_src, x_tgt, edge_tgt) are set\n",
        "\n",
        "# Move the trained GIT_model to the device (GPU or CPU)\n",
        "GIT_model.to(device)\n",
        "\n",
        "# Move the data tensors to the same device (GPU or CPU)\n",
        "x_tgt = x_tgt.to(device)         # Target node embeddings\n",
        "edge_tgt = edge_tgt.to(device)   # Target graph edges\n",
        "x_src = x_src.to(device)         # Source node embeddings\n",
        "edge_src = edge_src.to(device)   # Source graph edges\n",
        "\n",
        "# Set the model to evaluation mode; this disables dropout and batch normalization\n",
        "GIT_model.eval()\n",
        "\n",
        "# Pass the source and target embeddings through the trained GNN model to update the embeddings\n",
        "with torch.no_grad():  # Disable gradient computation (inference mode)\n",
        "    embeddings_tgt = GIT_model(x_tgt, edge_tgt)  # Get updated embeddings for the target graph\n",
        "    embeddings_src = GIT_model(x_src, edge_src)  # Get updated embeddings for the source graph\n",
        "\n",
        "# Detach the embeddings from the computation graph and move them back to the CPU\n",
        "# This step is useful if you need to use the embeddings for tasks outside PyTorch (e.g., saving to disk)\n",
        "embeddings_tgt = embeddings_tgt.detach().cpu()  # Target graph embeddings\n",
        "embeddings_src = embeddings_src.detach().cpu()  # Source graph embeddings\n",
        "\n",
        "# At this point, embeddings_tgt and embeddings_src contain the updated embeddings, ready for downstream tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Og5fdoGJrTCG"
      },
      "source": [
        "# Selecting embedding pairs to train the Gated Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "J0nTwc-dnjLn"
      },
      "outputs": [],
      "source": [
        "# Read the training pairs from a CSV file into a pandas DataFrame\n",
        "df_embeddings = pd.read_csv(train_file, index_col=0)\n",
        "\n",
        "# Extract columns and convert to NumPy arrays\n",
        "tensor_term1 = df_embeddings['SrcEntity'].values.astype(int)  # Source entity indices\n",
        "tensor_term2 = df_embeddings['TgtEntity'].values.astype(int)  # Target entity indices\n",
        "tensor_score = df_embeddings['Score'].values.astype(float)  # Matching scores\n",
        "\n",
        "# Split data into training and validation sets\n",
        "tensor_term1_train, tensor_term1_val, tensor_term2_train, tensor_term2_val, tensor_score_train, tensor_score_val = train_test_split(\n",
        "    tensor_term1, tensor_term2, tensor_score, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Convert split data to PyTorch tensors\n",
        "tensor_term1_train = torch.from_numpy(tensor_term1_train).type(torch.LongTensor)\n",
        "tensor_term2_train = torch.from_numpy(tensor_term2_train).type(torch.LongTensor)\n",
        "tensor_score_train = torch.from_numpy(tensor_score_train).type(torch.FloatTensor)\n",
        "\n",
        "tensor_term1_val = torch.from_numpy(tensor_term1_val).type(torch.LongTensor)\n",
        "tensor_term2_val = torch.from_numpy(tensor_term2_val).type(torch.LongTensor)\n",
        "tensor_score_val = torch.from_numpy(tensor_score_val).type(torch.FloatTensor)\n",
        "\n",
        "# Move the embeddings back to the CPU if not already there\n",
        "x_tgt = x_tgt.cpu()  # Target node embeddings\n",
        "x_src = x_src.cpu()  # Source node embeddings\n",
        "\n",
        "# Select embeddings for the training set\n",
        "X1_train = select_rows_by_index(embeddings_src, tensor_term1_train)\n",
        "X2_train = select_rows_by_index(x_src, tensor_term1_train)\n",
        "X3_train = select_rows_by_index(embeddings_tgt, tensor_term2_train)\n",
        "X4_train = select_rows_by_index(x_tgt, tensor_term2_train)\n",
        "\n",
        "# Select embeddings for the validation set\n",
        "X1_val = select_rows_by_index(embeddings_src, tensor_term1_val)\n",
        "X2_val = select_rows_by_index(x_src, tensor_term1_val)\n",
        "X3_val = select_rows_by_index(embeddings_tgt, tensor_term2_val)\n",
        "X4_val = select_rows_by_index(x_tgt, tensor_term2_val)\n",
        "\n",
        "# Now you have:\n",
        "# - Training tensors: X1_train, X2_train, X3_train, X4_train, tensor_score_train\n",
        "# - Validation tensors: X1_val, X2_val, X3_val, X4_val, tensor_score_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNdCgaTMExPK"
      },
      "source": [
        "# Gated Network Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Gof1eIPIWSVU"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def train_gated_combination_model(X1_t, X2_t, X3_t, X4_t, tensor_score_o,\n",
        "                                  X1_val, X2_val, X3_val, X4_val, tensor_score_val,\n",
        "                                  epochs=120, batch_size=32, learning_rate=0.001, weight_decay=1e-5):\n",
        "    \"\"\"\n",
        "    Trains the GatedCombination model with training and validation data, using ReduceLROnPlateau scheduler.\n",
        "    Also calculates and displays F1-score during training and validation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create datasets and DataLoaders\n",
        "    train_dataset = TensorDataset(X1_t, X2_t, X3_t, X4_t, tensor_score_o)\n",
        "    val_dataset = TensorDataset(X1_val, X2_val, X3_val, X4_val, tensor_score_val)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = GatedCombinationWithFaiss(X1_t.shape[1]).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # Use ReduceLROnPlateau scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0.0\n",
        "        y_true_train, y_pred_train = [], []\n",
        "\n",
        "        for batch_X1, batch_X2, batch_X3, batch_X4, batch_y in train_loader:\n",
        "            batch_X1, batch_X2, batch_X3, batch_X4, batch_y = (\n",
        "                batch_X1.to(device),\n",
        "                batch_X2.to(device),\n",
        "                batch_X3.to(device),\n",
        "                batch_X4.to(device),\n",
        "                batch_y.to(device),\n",
        "            )\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(batch_X1, batch_X2, batch_X3, batch_X4)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = F.binary_cross_entropy(outputs, batch_y.unsqueeze(1).float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            # Store true labels and predictions for F1-score\n",
        "            y_true_train.extend(batch_y.cpu().numpy())\n",
        "            y_pred_train.extend((outputs > 0.5).float().cpu().numpy())\n",
        "\n",
        "        train_loss = total_train_loss / len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Calculate F1-score for training\n",
        "        train_f1 = f1_score(y_true_train, y_pred_train)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        total_val_loss = 0.0\n",
        "        y_true_val, y_pred_val = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_X1, batch_X2, batch_X3, batch_X4, batch_y in val_loader:\n",
        "                batch_X1, batch_X2, batch_X3, batch_X4, batch_y = (\n",
        "                    batch_X1.to(device),\n",
        "                    batch_X2.to(device),\n",
        "                    batch_X3.to(device),\n",
        "                    batch_X4.to(device),\n",
        "                    batch_y.to(device),\n",
        "                )\n",
        "                outputs = model(batch_X1, batch_X2, batch_X3, batch_X4)\n",
        "\n",
        "                # Compute loss\n",
        "                val_loss = F.binary_cross_entropy(outputs, batch_y.unsqueeze(1).float())\n",
        "                total_val_loss += val_loss.item()\n",
        "\n",
        "                # Store true labels and predictions for F1-score\n",
        "                y_true_val.extend(batch_y.cpu().numpy())\n",
        "                y_pred_val.extend((outputs > 0.5).float().cpu().numpy())\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Calculate F1-score for validation\n",
        "        val_f1 = f1_score(y_true_val, y_pred_val)\n",
        "\n",
        "        # Step the scheduler with validation loss\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        # Print training and validation metrics\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}] Training Loss: {train_loss:.4f}, F1 Score: {train_f1:.4f} | \"\n",
        "              f\"Validation Loss: {avg_val_loss:.4f}, F1 Score: {val_f1:.4f}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Plotting training and validation loss\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_losses, label=\"Training Loss\", marker='o')\n",
        "    plt.plot(val_losses, label=\"Validation Loss\", marker='x')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Training complete! Total time: {end_time - start_time:.2f} seconds\")\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QrpFp6aDbtSW",
        "outputId": "6579a7f1-e012-479c-8b53-e1c845d2e9a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100] Training Loss: 0.0972, F1 Score: 0.0419 | Validation Loss: 0.0555, F1 Score: 0.1290\n",
            "Epoch [2/100] Training Loss: 0.0524, F1 Score: 0.1332 | Validation Loss: 0.0532, F1 Score: 0.2369\n",
            "Epoch [3/100] Training Loss: 0.0501, F1 Score: 0.2246 | Validation Loss: 0.0513, F1 Score: 0.3036\n",
            "Epoch [4/100] Training Loss: 0.0490, F1 Score: 0.3135 | Validation Loss: 0.0499, F1 Score: 0.3636\n",
            "Epoch [5/100] Training Loss: 0.0478, F1 Score: 0.3731 | Validation Loss: 0.0489, F1 Score: 0.3894\n",
            "Epoch [6/100] Training Loss: 0.0459, F1 Score: 0.4207 | Validation Loss: 0.0481, F1 Score: 0.4274\n",
            "Epoch [7/100] Training Loss: 0.0458, F1 Score: 0.4350 | Validation Loss: 0.0474, F1 Score: 0.4530\n",
            "Epoch [8/100] Training Loss: 0.0451, F1 Score: 0.4659 | Validation Loss: 0.0471, F1 Score: 0.4721\n",
            "Epoch [9/100] Training Loss: 0.0450, F1 Score: 0.4689 | Validation Loss: 0.0470, F1 Score: 0.4661\n",
            "Epoch [10/100] Training Loss: 0.0447, F1 Score: 0.4848 | Validation Loss: 0.0464, F1 Score: 0.4773\n",
            "Epoch [11/100] Training Loss: 0.0439, F1 Score: 0.4989 | Validation Loss: 0.0462, F1 Score: 0.4847\n",
            "Epoch [12/100] Training Loss: 0.0436, F1 Score: 0.5090 | Validation Loss: 0.0461, F1 Score: 0.5317\n",
            "Epoch [13/100] Training Loss: 0.0439, F1 Score: 0.5193 | Validation Loss: 0.0458, F1 Score: 0.5134\n",
            "Epoch [14/100] Training Loss: 0.0432, F1 Score: 0.5241 | Validation Loss: 0.0457, F1 Score: 0.5421\n",
            "Epoch [15/100] Training Loss: 0.0433, F1 Score: 0.5290 | Validation Loss: 0.0462, F1 Score: 0.5024\n",
            "Epoch [16/100] Training Loss: 0.0435, F1 Score: 0.5265 | Validation Loss: 0.0458, F1 Score: 0.5317\n",
            "Epoch [17/100] Training Loss: 0.0424, F1 Score: 0.5490 | Validation Loss: 0.0458, F1 Score: 0.5354\n",
            "Epoch [18/100] Training Loss: 0.0438, F1 Score: 0.5384 | Validation Loss: 0.0455, F1 Score: 0.5517\n",
            "Epoch [19/100] Training Loss: 0.0438, F1 Score: 0.5449 | Validation Loss: 0.0454, F1 Score: 0.5522\n",
            "Epoch [20/100] Training Loss: 0.0429, F1 Score: 0.5573 | Validation Loss: 0.0455, F1 Score: 0.5509\n",
            "Epoch [21/100] Training Loss: 0.0431, F1 Score: 0.5516 | Validation Loss: 0.0459, F1 Score: 0.5399\n",
            "Epoch [22/100] Training Loss: 0.0430, F1 Score: 0.5530 | Validation Loss: 0.0454, F1 Score: 0.5585\n",
            "Epoch [23/100] Training Loss: 0.0436, F1 Score: 0.5519 | Validation Loss: 0.0455, F1 Score: 0.5509\n",
            "Epoch [24/100] Training Loss: 0.0427, F1 Score: 0.5668 | Validation Loss: 0.0457, F1 Score: 0.5640\n",
            "Epoch [25/100] Training Loss: 0.0428, F1 Score: 0.5675 | Validation Loss: 0.0453, F1 Score: 0.5593\n",
            "Epoch [26/100] Training Loss: 0.0436, F1 Score: 0.5610 | Validation Loss: 0.0455, F1 Score: 0.5522\n",
            "Epoch [27/100] Training Loss: 0.0435, F1 Score: 0.5550 | Validation Loss: 0.0453, F1 Score: 0.5585\n",
            "Epoch [28/100] Training Loss: 0.0424, F1 Score: 0.5670 | Validation Loss: 0.0454, F1 Score: 0.5578\n",
            "Epoch [29/100] Training Loss: 0.0434, F1 Score: 0.5646 | Validation Loss: 0.0459, F1 Score: 0.5477\n",
            "Epoch [30/100] Training Loss: 0.0422, F1 Score: 0.5725 | Validation Loss: 0.0458, F1 Score: 0.5504\n",
            "Epoch [31/100] Training Loss: 0.0436, F1 Score: 0.5645 | Validation Loss: 0.0453, F1 Score: 0.5586\n",
            "Epoch [32/100] Training Loss: 0.0434, F1 Score: 0.5652 | Validation Loss: 0.0453, F1 Score: 0.5593\n",
            "Epoch [33/100] Training Loss: 0.0439, F1 Score: 0.5625 | Validation Loss: 0.0459, F1 Score: 0.5477\n",
            "Epoch [34/100] Training Loss: 0.0440, F1 Score: 0.5658 | Validation Loss: 0.0454, F1 Score: 0.5640\n",
            "Epoch [35/100] Training Loss: 0.0431, F1 Score: 0.5556 | Validation Loss: 0.0453, F1 Score: 0.5586\n",
            "Epoch [36/100] Training Loss: 0.0432, F1 Score: 0.5614 | Validation Loss: 0.0453, F1 Score: 0.5578\n",
            "Epoch [37/100] Training Loss: 0.0433, F1 Score: 0.5638 | Validation Loss: 0.0453, F1 Score: 0.5601\n",
            "Epoch [38/100] Training Loss: 0.0427, F1 Score: 0.5774 | Validation Loss: 0.0454, F1 Score: 0.5567\n",
            "Epoch [39/100] Training Loss: 0.0437, F1 Score: 0.5633 | Validation Loss: 0.0456, F1 Score: 0.5522\n",
            "Epoch [40/100] Training Loss: 0.0431, F1 Score: 0.5663 | Validation Loss: 0.0453, F1 Score: 0.5577\n",
            "Epoch [41/100] Training Loss: 0.0433, F1 Score: 0.5667 | Validation Loss: 0.0456, F1 Score: 0.5536\n",
            "Epoch [42/100] Training Loss: 0.0433, F1 Score: 0.5622 | Validation Loss: 0.0454, F1 Score: 0.5601\n",
            "Epoch [43/100] Training Loss: 0.0424, F1 Score: 0.5783 | Validation Loss: 0.0453, F1 Score: 0.5598\n",
            "Epoch [44/100] Training Loss: 0.0440, F1 Score: 0.5677 | Validation Loss: 0.0457, F1 Score: 0.5522\n",
            "Epoch [45/100] Training Loss: 0.0428, F1 Score: 0.5720 | Validation Loss: 0.0453, F1 Score: 0.5586\n",
            "Epoch [46/100] Training Loss: 0.0429, F1 Score: 0.5723 | Validation Loss: 0.0454, F1 Score: 0.5601\n",
            "Epoch [47/100] Training Loss: 0.0432, F1 Score: 0.5643 | Validation Loss: 0.0453, F1 Score: 0.5593\n",
            "Epoch [48/100] Training Loss: 0.0428, F1 Score: 0.5751 | Validation Loss: 0.0453, F1 Score: 0.5593\n",
            "Epoch [49/100] Training Loss: 0.0424, F1 Score: 0.5773 | Validation Loss: 0.0455, F1 Score: 0.5726\n",
            "Epoch [50/100] Training Loss: 0.0425, F1 Score: 0.5782 | Validation Loss: 0.0453, F1 Score: 0.5577\n",
            "Epoch [51/100] Training Loss: 0.0426, F1 Score: 0.5842 | Validation Loss: 0.0453, F1 Score: 0.5619\n",
            "Epoch [52/100] Training Loss: 0.0436, F1 Score: 0.5627 | Validation Loss: 0.0453, F1 Score: 0.5578\n",
            "Epoch [53/100] Training Loss: 0.0430, F1 Score: 0.5747 | Validation Loss: 0.0454, F1 Score: 0.5601\n",
            "Epoch [54/100] Training Loss: 0.0425, F1 Score: 0.5858 | Validation Loss: 0.0455, F1 Score: 0.5787\n",
            "Epoch [55/100] Training Loss: 0.0427, F1 Score: 0.5767 | Validation Loss: 0.0453, F1 Score: 0.5585\n",
            "Epoch [56/100] Training Loss: 0.0426, F1 Score: 0.5723 | Validation Loss: 0.0458, F1 Score: 0.5823\n",
            "Epoch [57/100] Training Loss: 0.0437, F1 Score: 0.5698 | Validation Loss: 0.0453, F1 Score: 0.5640\n",
            "Epoch [58/100] Training Loss: 0.0431, F1 Score: 0.5727 | Validation Loss: 0.0454, F1 Score: 0.5714\n",
            "Epoch [59/100] Training Loss: 0.0429, F1 Score: 0.5730 | Validation Loss: 0.0453, F1 Score: 0.5585\n",
            "Epoch [60/100] Training Loss: 0.0438, F1 Score: 0.5639 | Validation Loss: 0.0455, F1 Score: 0.5554\n",
            "Epoch [61/100] Training Loss: 0.0429, F1 Score: 0.5705 | Validation Loss: 0.0455, F1 Score: 0.5580\n",
            "Epoch [62/100] Training Loss: 0.0425, F1 Score: 0.5773 | Validation Loss: 0.0455, F1 Score: 0.5580\n",
            "Epoch [63/100] Training Loss: 0.0436, F1 Score: 0.5705 | Validation Loss: 0.0461, F1 Score: 0.5504\n",
            "Epoch [64/100] Training Loss: 0.0429, F1 Score: 0.5690 | Validation Loss: 0.0453, F1 Score: 0.5578\n",
            "Epoch [65/100] Training Loss: 0.0430, F1 Score: 0.5720 | Validation Loss: 0.0453, F1 Score: 0.5578\n",
            "Epoch [66/100] Training Loss: 0.0428, F1 Score: 0.5776 | Validation Loss: 0.0453, F1 Score: 0.5585\n",
            "Epoch [67/100] Training Loss: 0.0421, F1 Score: 0.5807 | Validation Loss: 0.0453, F1 Score: 0.5586\n",
            "Epoch [68/100] Training Loss: 0.0436, F1 Score: 0.5664 | Validation Loss: 0.0453, F1 Score: 0.5594\n",
            "Epoch [69/100] Training Loss: 0.0426, F1 Score: 0.5736 | Validation Loss: 0.0453, F1 Score: 0.5585\n",
            "Epoch [70/100] Training Loss: 0.0435, F1 Score: 0.5678 | Validation Loss: 0.0453, F1 Score: 0.5585\n",
            "Epoch [71/100] Training Loss: 0.0428, F1 Score: 0.5761 | Validation Loss: 0.0453, F1 Score: 0.5585\n",
            "Epoch [72/100] Training Loss: 0.0433, F1 Score: 0.5771 | Validation Loss: 0.0454, F1 Score: 0.5593\n",
            "Epoch [73/100] Training Loss: 0.0431, F1 Score: 0.5794 | Validation Loss: 0.0453, F1 Score: 0.5585\n",
            "Epoch [74/100] Training Loss: 0.0428, F1 Score: 0.5764 | Validation Loss: 0.0453, F1 Score: 0.5585\n",
            "Epoch [75/100] Training Loss: 0.0422, F1 Score: 0.5778 | Validation Loss: 0.0453, F1 Score: 0.5577\n",
            "Epoch [76/100] Training Loss: 0.0430, F1 Score: 0.5709 | Validation Loss: 0.0453, F1 Score: 0.5598\n",
            "Epoch [77/100] Training Loss: 0.0433, F1 Score: 0.5687 | Validation Loss: 0.0453, F1 Score: 0.5577\n",
            "Epoch [78/100] Training Loss: 0.0422, F1 Score: 0.5800 | Validation Loss: 0.0453, F1 Score: 0.5598\n",
            "Epoch [79/100] Training Loss: 0.0427, F1 Score: 0.5753 | Validation Loss: 0.0453, F1 Score: 0.5619\n",
            "Epoch [80/100] Training Loss: 0.0422, F1 Score: 0.5742 | Validation Loss: 0.0453, F1 Score: 0.5598\n",
            "Epoch [81/100] Training Loss: 0.0430, F1 Score: 0.5745 | Validation Loss: 0.0453, F1 Score: 0.5598\n",
            "Epoch [82/100] Training Loss: 0.0428, F1 Score: 0.5753 | Validation Loss: 0.0453, F1 Score: 0.5619\n",
            "Epoch [83/100] Training Loss: 0.0433, F1 Score: 0.5669 | Validation Loss: 0.0453, F1 Score: 0.5598\n",
            "Epoch [84/100] Training Loss: 0.0427, F1 Score: 0.5762 | Validation Loss: 0.0453, F1 Score: 0.5598\n",
            "Epoch [85/100] Training Loss: 0.0423, F1 Score: 0.5773 | Validation Loss: 0.0453, F1 Score: 0.5598\n",
            "Epoch [86/100] Training Loss: 0.0425, F1 Score: 0.5734 | Validation Loss: 0.0453, F1 Score: 0.5598\n",
            "Epoch [87/100] Training Loss: 0.0428, F1 Score: 0.5738 | Validation Loss: 0.0453, F1 Score: 0.5598\n",
            "Epoch [88/100] Training Loss: 0.0432, F1 Score: 0.5722 | Validation Loss: 0.0453, F1 Score: 0.5598\n",
            "Epoch [89/100] Training Loss: 0.0425, F1 Score: 0.5720 | Validation Loss: 0.0453, F1 Score: 0.5598\n",
            "Epoch [90/100] Training Loss: 0.0427, F1 Score: 0.5731 | Validation Loss: 0.0453, F1 Score: 0.5598\n",
            "Epoch [91/100] Training Loss: 0.0426, F1 Score: 0.5753 | Validation Loss: 0.0453, F1 Score: 0.5598\n",
            "Epoch [92/100] Training Loss: 0.0430, F1 Score: 0.5749 | Validation Loss: 0.0453, F1 Score: 0.5598\n",
            "Epoch [93/100] Training Loss: 0.0427, F1 Score: 0.5773 | Validation Loss: 0.0453, F1 Score: 0.5598\n",
            "Epoch [94/100] Training Loss: 0.0426, F1 Score: 0.5782 | Validation Loss: 0.0453, F1 Score: 0.5598\n",
            "Epoch [95/100] Training Loss: 0.0431, F1 Score: 0.5727 | Validation Loss: 0.0453, F1 Score: 0.5598\n",
            "Epoch [96/100] Training Loss: 0.0432, F1 Score: 0.5702 | Validation Loss: 0.0453, F1 Score: 0.5598\n",
            "Epoch [97/100] Training Loss: 0.0429, F1 Score: 0.5756 | Validation Loss: 0.0453, F1 Score: 0.5598\n",
            "Epoch [98/100] Training Loss: 0.0430, F1 Score: 0.5716 | Validation Loss: 0.0453, F1 Score: 0.5598\n",
            "Epoch [99/100] Training Loss: 0.0428, F1 Score: 0.5720 | Validation Loss: 0.0453, F1 Score: 0.5598\n",
            "Epoch [100/100] Training Loss: 0.0424, F1 Score: 0.5806 | Validation Loss: 0.0453, F1 Score: 0.5598\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete! Total time: 539.43 seconds\n"
          ]
        }
      ],
      "source": [
        "# Train the GatedCombination model using training and validation data\n",
        "trained_model = train_gated_combination_model(\n",
        "    X1_train,          # Updated source embeddings (after applying the GNN model)\n",
        "    X2_train,          # Original source embeddings (before applying the GNN model)\n",
        "    X3_train,          # Updated target embeddings (after applying the GNN model)\n",
        "    X4_train,          # Original target embeddings (before applying the GNN model)\n",
        "    tensor_score_train, # Ground truth labels for the training set (1 for matched pairs, 0 for unmatched pairs)\n",
        "\n",
        "    X1_val,            # Updated source embeddings for the validation set\n",
        "    X2_val,            # Original source embeddings for the validation set\n",
        "    X3_val,            # Updated target embeddings for the validation set\n",
        "    X4_val,            # Original target embeddings for the validation set\n",
        "    tensor_score_val,  # Ground truth labels for the validation set (1 for matched pairs, 0 for unmatched pairs)\n",
        "\n",
        "    epochs=100,        # Number of epochs (iterations over the entire training dataset)\n",
        "    batch_size=32,     # Number of training samples processed in one forward/backward pass\n",
        "    learning_rate=0.001, # Learning rate for the optimizer (controls step size during optimization)\n",
        "    weight_decay=1e-4 # Weight decay (L2 regularization) to prevent overfitting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAmLuMtGW9c2"
      },
      "source": [
        "# **Second Round Modifications**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVDqN5AxFu9m"
      },
      "source": [
        "# **Generate Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "ITZZcElm8qRN"
      },
      "outputs": [],
      "source": [
        "# Build an indexed dictionary for the source ontology classes\n",
        "# src_class is the file path to the JSON file containing the source ontology classes\n",
        "indexed_dict_src = build_indexed_dict(src_class)\n",
        "\n",
        "# Build an indexed dictionary for the target ontology classes\n",
        "# tgt_class is the file path to the JSON file containing the target ontology classes\n",
        "indexed_dict_tgt = build_indexed_dict(tgt_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "GbSbmPlRDOs3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "616382a8-d19a-4ea0-ce54-5f7cbc651bad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Gated embeddings saved:\n",
            "- Source: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Data/ncit_final_embeddings_SapBERT.tsv\n",
            "- Target: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Data/doid_final_embeddings_SapBERT.tsv\n",
            "⏱️ Execution time: 22.65 seconds\n"
          ]
        }
      ],
      "source": [
        "# Define output file paths for final embeddings of source and target ontologies\n",
        "output_file_src = f\"{data_dir}/{src_ent}_final_embeddings_SapBERT.tsv\"\n",
        "output_file_tgt = f\"{data_dir}/{tgt_ent}_final_embeddings_SapBERT.tsv\"\n",
        "\n",
        "# Save the final gated embeddings for all concepts in source and target ontologies\n",
        "save_gated_embeddings(\n",
        "    gated_model=trained_model,          # The trained GatedCombination model\n",
        "    embeddings_src=embeddings_src,      # GNN-transformed embeddings for source entities\n",
        "    x_src=x_src,                        # Initial semantic embeddings for source entities\n",
        "    embeddings_tgt=embeddings_tgt,      # GNN-transformed embeddings for target entities\n",
        "    x_tgt=x_tgt,                        # Initial semantic embeddings for target entities\n",
        "    indexed_dict_src=indexed_dict_src,  # Index-to-URI mapping for source ontology\n",
        "    indexed_dict_tgt=indexed_dict_tgt,  # Index-to-URI mapping for target ontology\n",
        "    output_file_src=output_file_src,    # Destination file path for source embeddings\n",
        "    output_file_tgt=output_file_tgt     # Destination file path for target embeddings\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIDvbZj2GIGo"
      },
      "source": [
        "# **Filter No Used Concepts**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "6Gl_wUG9KADo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2acf947-9480-499f-e7ba-65cdb4283ed6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Initial source file: 15992 rows\n",
            "🔍 Initial target file: 8480 rows\n",
            "✅ Source after removing ignored classes: 7065 rows\n",
            "✅ Target after removing ignored classes: 8463 rows\n",
            "📁 Cleaned source file saved to: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Data/ncit_final_embeddings_SapBERT_cleaned.tsv\n",
            "📁 Cleaned target file saved to: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Data/doid_final_embeddings_SapBERT_cleaned.tsv\n"
          ]
        }
      ],
      "source": [
        "# Call the function to filter out ignored concepts (e.g., owl:Thing, deprecated, etc.)\n",
        "# from the source and target ontology embeddings.\n",
        "\n",
        "# Input:\n",
        "# - src_emb_path: Path to the TSV file containing embeddings for the source ontology\n",
        "# - tgt_emb_path: Path to the TSV file containing embeddings for the target ontology\n",
        "# - src_onto / tgt_onto: DeepOnto ontology objects used to identify ignored concepts\n",
        "\n",
        "# Output:\n",
        "# - src_file: Path to the cleaned source embeddings (with ignored concepts removed)\n",
        "# - tgt_file: Path to the cleaned target embeddings (with ignored concepts removed)\n",
        "\n",
        "src_file, tgt_file = filter_ignored_class(\n",
        "    src_emb_path=f\"{data_dir}/{src_ent}_final_embeddings_SapBERT.tsv\",\n",
        "    tgt_emb_path=f\"{data_dir}/{tgt_ent}_final_embeddings_SapBERT.tsv\",\n",
        "    src_onto=src_onto,\n",
        "    tgt_onto=tgt_onto\n",
        "\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpUklR4xnVMH"
      },
      "source": [
        "# **Mappings Generation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljGEyKNOerBT"
      },
      "source": [
        "# **Using faiss l2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "xOSRYREwerBi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c9ba545-ed88-4c50-894c-d82d1aecfb16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Using L2 (Euclidean) distance with FAISS\n",
            "Top-10 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Results/ncit2doid_top_10_mappings_SapBERT.tsv\n",
            "⏱️ Execution time: 8.78 seconds\n"
          ]
        }
      ],
      "source": [
        "# Compute the top-10 most similar mappings using l2 distance\n",
        "# between ResMLP-encoded embeddings of the source and target ontologies.\n",
        "# The input embeddings were previously encoded using the ResMLPEncoder,\n",
        "# and the similarity score is computed as the inverse of the l2 distance.\n",
        "# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.\n",
        "topk_faiss_l2(\n",
        "    src_emb_path=f\"{data_dir}/{src_ent}_final_embeddings_SapBERT_cleaned.tsv\",\n",
        "    tgt_emb_path=f\"{data_dir}/{tgt_ent}_final_embeddings_SapBERT_cleaned.tsv\",\n",
        "    top_k=10,\n",
        "    output_file=f\"{results_dir}/{task}_top_10_mappings_SapBERT.tsv\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD-mvVjaerBh"
      },
      "source": [
        "# **Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8GRfT_pR1kD"
      },
      "source": [
        "# **Global Metrics: Precision, Recall and F1 score**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "9WZKJM46erBi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e95085d8-e096-45a0-e753-2d54a82ccc9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📁 Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Results/ncit2doid_top_10_mappings_SapBERT_predictions.tsv\n",
            "📌 Number of predictions in output: 3083\n",
            "🎯 Correct mappings (Top-1): 2578\n",
            "📊 Evaluation (P / R / F1): {'P': 0.836, 'R': 0.786, 'F1': 0.81}\n"
          ]
        }
      ],
      "source": [
        "# Run the evaluation on the predicted mappings using a filtering and evaluation function.\n",
        "\n",
        "output_file, metrics, correct = evaluate_predictions(\n",
        "    topk_file=f\"{results_dir}/{task}_top_10_mappings_SapBERT.tsv\",\n",
        "    # Path to the TSV file containing predicted mappings with scores (before filtering).\n",
        "\n",
        "    train_file=f\"{dataset_dir}/refs_equiv/train.tsv\",\n",
        "    # Path to the training reference file (used to exclude mappings involving train-only entities).\n",
        "\n",
        "    test_file=f\"{dataset_dir}/refs_equiv/test.tsv\",\n",
        "    # Path to the test reference file (used as the gold standard for evaluation).\n",
        "\n",
        "    src_onto=src_onto,\n",
        "    # The source ontology object, used to detect ignored classes or perform additional filtering.\n",
        "\n",
        "    tgt_onto=tgt_onto,\n",
        "    # The target ontology object, used similarly for filtering ignored or irrelevant classes.\n",
        ")\n",
        "\n",
        "# This function returns:\n",
        "# - `output_file`: the path to the filtered and evaluated output file.\n",
        "# - `metrics`: a tuple containing (Precision, Recall, F1-score).\n",
        "# - `correct`: the number of correctly predicted mappings found in the gold standard.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE3WArY1SAWO"
      },
      "source": [
        "# **Metrics@1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "h0y9PGOjerBh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3fd29a5-db64-4707-bd58-c519a1e232b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Using L2 (Euclidean) distance with FAISS\n",
            "Top-1 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Results/ncit2doid_top_1_mappings_SapBERT.tsv\n",
            "⏱️ Execution time: 3.15 seconds\n"
          ]
        }
      ],
      "source": [
        "# Compute the top-1 most similar mappings using l2 distance\n",
        "# and the similarity score is computed as the inverse of the l2 distance.\n",
        "# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.\n",
        "topk_faiss_l2(\n",
        "    src_emb_path=f\"{data_dir}/{src_ent}_final_embeddings_SapBERT_cleaned.tsv\",\n",
        "    tgt_emb_path=f\"{data_dir}/{tgt_ent}_final_embeddings_SapBERT_cleaned.tsv\",\n",
        "    top_k=1,\n",
        "    output_file=f\"{results_dir}/{task}_top_1_mappings_SapBERT.tsv\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "wk-B3ayYerBh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05aafb17-a8c1-4eff-ef6b-052f49290ff2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📁 Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Results/ncit2doid_top_1_mappings_SapBERT_predictions.tsv\n",
            "📌 Number of predictions in output: 2543\n",
            "{'Precision@1': 0.8966, 'Recall@1': 0.8889, 'F1@1': 0.8927}\n"
          ]
        }
      ],
      "source": [
        "# === Evaluate Top-1 Mappings ===\n",
        "\n",
        "results = evaluate_topk(\n",
        "    topk_file=f\"{results_dir}/{task}_top_1_mappings_SapBERT.tsv\",\n",
        "    # Path to the file containing the predicted mappings with scores.\n",
        "    # This file may include unfiltered predictions (e.g., over all candidates).\n",
        "\n",
        "    train_file=f\"{dataset_dir}/refs_equiv/train.tsv\",\n",
        "    # Path to the training reference mappings file.\n",
        "    # Used to remove mappings that involve entities appearing only in training.\n",
        "\n",
        "    test_file=f\"{dataset_dir}/refs_equiv/test.tsv\",\n",
        "    # Path to the test reference mappings file.\n",
        "    # Ground-truth correspondences are extracted from this file for evaluation.\n",
        "\n",
        "    k=1  # Evaluate top-1 predictions per source entity.\n",
        ")\n",
        "\n",
        "# === Display evaluation results ===\n",
        "print(results)\n",
        "# Outputs a dictionary with Precision@1, Recall@1, and F1@1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf6vZML-KewM"
      },
      "source": [
        "# **Local MRR and Hit@k**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "7xXm15EQKeE_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# === Step 1: Load input files ===\n",
        "\n",
        "# Define paths to cleaned embedding files\n",
        "src_emb_path = f\"{data_dir}/{src_ent}_final_embeddings_SapBERT_cleaned.tsv\"\n",
        "tgt_emb_path = f\"{data_dir}/{tgt_ent}_final_embeddings_SapBERT_cleaned.tsv\"\n",
        "\n",
        "# Load candidate mappings (SrcEntity, TgtEntity) and source/target embeddings\n",
        "df_cands = pd.read_csv(cands_path)\n",
        "src_emb_df = pd.read_csv(src_emb_path, sep=\"\\t\")\n",
        "tgt_emb_df = pd.read_csv(tgt_emb_path, sep=\"\\t\")\n",
        "\n",
        "# === Step 2: Extract unique source and target URIs from the candidate pairs ===\n",
        "\n",
        "# Keep only distinct source and target entities (URIs) for which embeddings are needed\n",
        "unique_src_df = pd.DataFrame(df_cands[\"SrcEntity\"].unique(), columns=[\"Concept\"])\n",
        "unique_tgt_df = pd.DataFrame(df_cands[\"TgtEntity\"].unique(), columns=[\"Concept\"])\n",
        "\n",
        "# === Step 3: Join embeddings for each concept based on the \"Concept\" URI ===\n",
        "\n",
        "# Merge source entities with their corresponding embeddings (if available)\n",
        "merged_src_df = pd.merge(unique_src_df, src_emb_df, on=\"Concept\", how=\"left\")\n",
        "\n",
        "# Merge target entities with their corresponding embeddings (if available)\n",
        "merged_tgt_df = pd.merge(unique_tgt_df, tgt_emb_df, on=\"Concept\", how=\"left\")\n",
        "\n",
        "# === Step 4: Save the merged results to TSV files ===\n",
        "\n",
        "# Save the source concepts and their embeddings to file\n",
        "merged_src_df.to_csv(f\"{data_dir}/{src_ent}_cands_with_embeddings_SapBERT.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "# Save the target concepts and their embeddings to file\n",
        "merged_tgt_df.to_csv(f\"{data_dir}/{tgt_ent}_cands_with_embeddings_SapBERT.tsv\", sep=\"\\t\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "_BgQMQzperBl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06a19cf3-ee76-4db1-9ca2-b26a72aaaaee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Using L2 (Euclidean) distance with FAISS\n",
            "Top-200 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//ncit2doid/Results/ncit2doid_top_200_mappings_mrr_hit_SapBERT.tsv\n",
            "⏱️ Execution time: 6.57 seconds\n"
          ]
        }
      ],
      "source": [
        "topk_faiss_l2(\n",
        "    # Path to the source entity embeddings (already filtered and linearly encoded)\n",
        "    src_emb_path=f\"{data_dir}/{src_ent}_cands_with_embeddings_SapBERT.tsv\",\n",
        "\n",
        "    # Path to the target entity embeddings (already filtered and linearly encoded)\n",
        "    tgt_emb_path=f\"{data_dir}/{tgt_ent}_cands_with_embeddings_SapBERT.tsv\",\n",
        "\n",
        "    # Number of top matches to retrieve per source entity (Top-K candidates)\n",
        "    top_k=200,\n",
        "\n",
        "    # Path to save the resulting Top-K mappings sorted by FAISS L2 distance (converted to similarity)\n",
        "    output_file=f\"{results_dir}/{task}_top_200_mappings_mrr_hit_SapBERT.tsv\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "DpzkN2-verBl"
      },
      "outputs": [],
      "source": [
        "test_cands = f\"{dataset_dir}/refs_equiv/test.cands.tsv\"\n",
        "# Compute MRR and Hits@k metrics\n",
        "# This function evaluates the predicted rankings against the reference mappings\n",
        "results = compute_mrr_and_hits(\n",
        "    reference_file=test_cands,             # Reference file with true ranks\n",
        "    predicted_file=f\"{results_dir}/{task}_top_200_mappings_mrr_hit_SapBERT.tsv\",             # File containing predicted rankings\n",
        "    output_file=formatted_predictions_path,    # File path to save formatted predictions\n",
        "    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "quXigRGeerBl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f25e943c-4e6b-4c96-8e6c-d6b827eb0961"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranking Evaluation Results at K=1, 5, and 10:\n",
            "{'MRR': 0.8228326035097456, 'Hits@1': 0.7551829268292682, 'Hits@5': 0.9085365853658537, 'Hits@10': 0.9445121951219512}\n"
          ]
        }
      ],
      "source": [
        "# Evaluate ranking performance using standard metrics like MRR and Hits@K\n",
        "# 'formatted_predictions_path' should point to a TSV file with columns: SrcEntity, TgtEntity, TgtCandidates\n",
        "# This function computes how well the true targets are ranked among the candidates\n",
        "results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])\n",
        "\n",
        "# Print the evaluation results for Hits@1, Hits@5, and Hits@10\n",
        "print(\"Ranking Evaluation Results at K=1, 5, and 10:\")\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFqKl-p1aVPF"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "zqEXsgPGMVhw",
        "zxCn5ztKVztw",
        "QpwWQ2ndKGOA"
      ],
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}