# **Package Installation**


```python

!pip install torch==2.6.0 torchvision==0.21.0 --force-reinstall
```

    Collecting torch==2.6.0
      Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)
    Collecting torchvision==0.21.0
      Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)
    Collecting filelock (from torch==2.6.0)
      Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)
    Collecting typing-extensions>=4.10.0 (from torch==2.6.0)
      Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)
    Collecting networkx (from torch==2.6.0)
      Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)
    Collecting jinja2 (from torch==2.6.0)
      Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)
    Collecting fsspec (from torch==2.6.0)
      Downloading fsspec-2025.5.0-py3-none-any.whl.metadata (11 kB)
    Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0)
      Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
    Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0)
      Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
    Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0)
      Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)
    Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0)
      Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)
    Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0)
      Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
    Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0)
      Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
    Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0)
      Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
    Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0)
      Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)
    Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0)
      Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)
    Collecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0)
      Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)
    Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.6.0)
      Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)
    Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6.0)
      Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)
    Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0)
      Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
    Collecting triton==3.2.0 (from torch==2.6.0)
      Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)
    Collecting sympy==1.13.1 (from torch==2.6.0)
      Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)
    Collecting numpy (from torchvision==0.21.0)
      Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m62.0/62.0 kB[0m [31m2.9 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.21.0)
      Downloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (8.9 kB)
    Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch==2.6.0)
      Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)
    Collecting MarkupSafe>=2.0 (from jinja2->torch==2.6.0)
      Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)
    Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m766.7/766.7 MB[0m [31m453.1 kB/s[0m eta [36m0:00:00[0m
    [?25hDownloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m7.2/7.2 MB[0m [31m62.2 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m363.4/363.4 MB[0m [31m2.9 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m13.8/13.8 MB[0m [31m52.1 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m24.6/24.6 MB[0m [31m43.3 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m883.7/883.7 kB[0m [31m35.6 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m664.8/664.8 MB[0m [31m1.6 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m211.5/211.5 MB[0m [31m2.1 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m56.3/56.3 MB[0m [31m20.6 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m127.9/127.9 MB[0m [31m8.6 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m207.5/207.5 MB[0m [31m3.0 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m150.1/150.1 MB[0m [31m1.8 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m188.7/188.7 MB[0m [31m2.0 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m21.1/21.1 MB[0m [31m68.9 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m99.1/99.1 kB[0m [31m9.6 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m6.2/6.2 MB[0m [31m75.5 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m253.2/253.2 MB[0m [31m4.3 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl (4.6 MB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m4.6/4.6 MB[0m [31m78.5 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m45.8/45.8 kB[0m [31m3.9 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)
    Downloading fsspec-2025.5.0-py3-none-any.whl (196 kB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m196.2/196.2 kB[0m [31m17.3 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m134.9/134.9 kB[0m [31m12.1 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.7/1.7 MB[0m [31m59.6 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m16.8/16.8 MB[0m [31m79.7 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)
    Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m536.2/536.2 kB[0m [31m36.3 MB/s[0m eta [36m0:00:00[0m
    [?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, mpmath, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision
      Attempting uninstall: mpmath
        Found existing installation: mpmath 1.3.0
        Uninstalling mpmath-1.3.0:
          Successfully uninstalled mpmath-1.3.0
      Attempting uninstall: typing-extensions
        Found existing installation: typing_extensions 4.13.2
        Uninstalling typing_extensions-4.13.2:
          Successfully uninstalled typing_extensions-4.13.2
      Attempting uninstall: sympy
        Found existing installation: sympy 1.13.1
        Uninstalling sympy-1.13.1:
          Successfully uninstalled sympy-1.13.1
      Attempting uninstall: pillow
        Found existing installation: pillow 11.2.1
        Uninstalling pillow-11.2.1:
          Successfully uninstalled pillow-11.2.1
      Attempting uninstall: numpy
        Found existing installation: numpy 2.0.2
        Uninstalling numpy-2.0.2:
          Successfully uninstalled numpy-2.0.2
      Attempting uninstall: networkx
        Found existing installation: networkx 3.4.2
        Uninstalling networkx-3.4.2:
          Successfully uninstalled networkx-3.4.2
      Attempting uninstall: MarkupSafe
        Found existing installation: MarkupSafe 3.0.2
        Uninstalling MarkupSafe-3.0.2:
          Successfully uninstalled MarkupSafe-3.0.2
      Attempting uninstall: fsspec
        Found existing installation: fsspec 2025.3.2
        Uninstalling fsspec-2025.3.2:
          Successfully uninstalled fsspec-2025.3.2
      Attempting uninstall: filelock
        Found existing installation: filelock 3.18.0
        Uninstalling filelock-3.18.0:
          Successfully uninstalled filelock-3.18.0
      Attempting uninstall: jinja2
        Found existing installation: Jinja2 3.1.6
        Uninstalling Jinja2-3.1.6:
          Successfully uninstalled Jinja2-3.1.6
      Attempting uninstall: torch
        Found existing installation: torch 2.6.0+cpu
        Uninstalling torch-2.6.0+cpu:
          Successfully uninstalled torch-2.6.0+cpu
      Attempting uninstall: torchvision
        Found existing installation: torchvision 0.21.0+cpu
        Uninstalling torchvision-0.21.0+cpu:
          Successfully uninstalled torchvision-0.21.0+cpu
    Successfully installed MarkupSafe-3.0.2 filelock-3.18.0 fsspec-2025.5.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.6 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pillow-11.2.1 sympy-1.13.1 torch-2.6.0 torchvision-0.21.0 triton-3.2.0 typing-extensions-4.13.2





```python
# We assume that PyTorch is already installed in the environment.
# If not, this command installs it.

# Install PyTorch Geometric, a library for creating graph neural networks using PyTorch.
!pip install torch-geometric==2.4.0

# Import PyTorch to access its functionalities.
import torch

# Install additional PyTorch Geometric dependencies for graph processing (scatter, sparse, cluster, spline-conv).
# These packages enable operations like sparse tensors and convolutions on graphs.
!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html

# Reinstall PyTorch Geometric to ensure all dependencies are correctly loaded.
!pip install torch-geometric

# Retrieve the installed version of PyTorch to ensure compatibility with other packages.
torchversion = torch.__version__

# Install the latest version of PyTorch Geometric directly from the GitHub repository.
# This allows access to the most recent updates and features for graph-based neural networks.
!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git

# Install DeepOnto, a package specifically designed for ontology matching, particularly useful in biomedical applications.
!pip install deeponto

# Install a custom version of DeepOnto from a GitHub repository.
# The '<username>' part should be replaced with the actual GitHub username of the repository maintainer.
!pip install git+https://github.com/<username>/deeponto.git

```

    Collecting torch-geometric==2.4.0
      Downloading torch_geometric-2.4.0-py3-none-any.whl.metadata (63 kB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m63.9/63.9 kB[0m [31m1.9 MB/s[0m eta [36m0:00:00[0m
    [?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (4.67.1)
    Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (2.2.6)
    Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (1.15.3)
    Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (3.1.6)
    Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (2.32.3)
    Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (3.2.3)
    Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (1.6.1)
    Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.4.0) (5.9.5)
    Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric==2.4.0) (3.0.2)
    Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.4.0) (3.4.2)
    Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.4.0) (3.10)
    Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.4.0) (2.4.0)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.4.0) (2025.4.26)
    Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->torch-geometric==2.4.0) (1.5.0)
    Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->torch-geometric==2.4.0) (3.6.0)
    Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.0/1.0 MB[0m [31m17.3 MB/s[0m eta [36m0:00:00[0m
    [?25hInstalling collected packages: torch-geometric
    Successfully installed torch-geometric-2.4.0
    Looking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html
    Collecting torch-scatter
      Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_scatter-2.1.2%2Bpt20cpu-cp311-cp311-linux_x86_64.whl (494 kB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m494.0/494.0 kB[0m [31m8.1 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting torch-sparse
      Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_sparse-0.6.18%2Bpt20cpu-cp311-cp311-linux_x86_64.whl (1.2 MB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.2/1.2 MB[0m [31m29.6 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting torch-cluster
      Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_cluster-1.6.3%2Bpt20cpu-cp311-cp311-linux_x86_64.whl (750 kB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m750.9/750.9 kB[0m [31m43.1 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting torch-spline-conv
      Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_spline_conv-1.2.2%2Bpt20cpu-cp311-cp311-linux_x86_64.whl (208 kB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m208.1/208.1 kB[0m [31m18.1 MB/s[0m eta [36m0:00:00[0m
    [?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.3)
    Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (2.2.6)
    Installing collected packages: torch-spline-conv, torch-scatter, torch-sparse, torch-cluster
    Successfully installed torch-cluster-1.6.3+pt20cpu torch-scatter-2.1.2+pt20cpu torch-sparse-0.6.18+pt20cpu torch-spline-conv-1.2.2+pt20cpu
    Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.4.0)
    Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)
    Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.2.6)
    Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.15.3)
    Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)
    Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)
    Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)
    Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.6.1)
    Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)
    Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)
    Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)
    Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)
    Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)
    Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->torch-geometric) (1.5.0)
    Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->torch-geometric) (3.6.0)
      Installing build dependencies ... [?25l[?25hdone
      Getting requirements to build wheel ... [?25l[?25hdone
      Preparing metadata (pyproject.toml) ... [?25l[?25hdone
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m194.8/194.8 kB[0m [31m4.4 MB/s[0m eta [36m0:00:00[0m
    [?25h  Building wheel for torch-geometric (pyproject.toml) ... [?25l[?25hdone
    Collecting deeponto
      Downloading deeponto-0.9.3-py3-none-any.whl.metadata (16 kB)
    Collecting JPype1 (from deeponto)
      Downloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)
    Collecting yacs (from deeponto)
      Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)
    Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from deeponto) (2.6.0)
    Collecting anytree (from deeponto)
      Downloading anytree-2.13.0-py3-none-any.whl.metadata (8.0 kB)
    Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from deeponto) (8.2.0)
    Collecting dill (from deeponto)
      Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)
    Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from deeponto) (2.2.2)
    Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deeponto) (2.2.6)
    Requirement already satisfied: scikit_learn in /usr/local/lib/python3.11/dist-packages (from deeponto) (1.6.1)
    Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.11/dist-packages (from deeponto) (4.51.3)
    Collecting datasets (from deeponto)
      Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)
    Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (from deeponto) (3.8.5)
    Collecting pprintpp (from deeponto)
      Downloading pprintpp-0.4.0-py2.py3-none-any.whl.metadata (7.9 kB)
    Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from deeponto) (3.4.2)
    Collecting lxml (from deeponto)
      Downloading lxml-5.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.5 kB)
    Collecting textdistance (from deeponto)
      Downloading textdistance-4.6.3-py3-none-any.whl.metadata (18 kB)
    Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (from deeponto) (7.7.1)
    Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from deeponto) (6.17.1)
    Collecting enlighten (from deeponto)
      Downloading enlighten-1.14.1-py2.py3-none-any.whl.metadata (18 kB)
    Collecting rdflib (from deeponto)
      Downloading rdflib-7.1.4-py3-none-any.whl.metadata (11 kB)
    Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from deeponto) (3.9.1)
    Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (3.18.0)
    Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (20.0.0)
    Collecting dill (from deeponto)
      Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)
    Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (2.32.3)
    Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (4.67.1)
    Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (3.5.0)
    Collecting multiprocess<0.70.17 (from datasets->deeponto)
      Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)
    Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto)
      Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)
    Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (0.31.2)
    Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (25.0)
    Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->deeponto) (6.0.2)
    Collecting blessed>=1.17.7 (from enlighten->deeponto)
      Downloading blessed-1.21.0-py2.py3-none-any.whl.metadata (13 kB)
    Collecting prefixed>=0.3.2 (from enlighten->deeponto)
      Downloading prefixed-0.9.0-py2.py3-none-any.whl.metadata (11 kB)
    Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (1.8.0)
    Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (7.34.0)
    Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (6.1.12)
    Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (0.1.7)
    Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (1.6.0)
    Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (5.9.5)
    Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (24.0.1)
    Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (6.4.2)
    Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->deeponto) (5.7.1)
    Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->deeponto) (0.2.0)
    Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->deeponto) (3.6.10)
    Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->deeponto) (3.0.15)
    Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->deeponto) (1.5.0)
    Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->deeponto) (2024.11.6)
    Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->deeponto) (2.9.0.post0)
    Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->deeponto) (2025.2)
    Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->deeponto) (2025.2)
    Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib->deeponto) (3.2.3)
    Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn->deeponto) (1.15.3)
    Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn->deeponto) (3.6.0)
    Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (3.0.12)
    Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (1.0.5)
    Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (1.0.12)
    Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (2.0.11)
    Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (3.0.9)
    Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (8.3.6)
    Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (1.1.3)
    Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (2.5.1)
    Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (2.0.10)
    Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (0.4.1)
    Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (0.15.3)
    Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (2.11.4)
    Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (3.1.6)
    Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (75.2.0)
    Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy->deeponto) (3.5.0)
    Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (4.13.2)
    Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.127)
    Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.127)
    Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.127)
    Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (9.1.0.70)
    Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.5.8)
    Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (11.2.1.3)
    Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (10.3.5.147)
    Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (11.6.1.9)
    Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.3.1.170)
    Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (0.6.2)
    Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (2.21.5)
    Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.127)
    Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (12.4.127)
    Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (3.2.0)
    Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->deeponto) (1.13.1)
    Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->deeponto) (1.3.0)
    Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]->deeponto) (0.21.1)
    Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]->deeponto) (0.5.3)
    Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]->deeponto) (1.6.0)
    Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from blessed>=1.17.7->enlighten->deeponto) (0.2.13)
    Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (3.11.15)
    Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel->deeponto)
      Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)
    Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (5.2.1)
    Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (0.7.5)
    Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (3.0.51)
    Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (2.19.1)
    Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (0.2.0)
    Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->deeponto) (4.9.0)
    Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel->deeponto) (5.7.2)
    Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->deeponto) (1.3.0)
    Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->deeponto) (0.7.0)
    Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->deeponto) (2.33.2)
    Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->deeponto) (0.4.0)
    Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->deeponto) (1.17.0)
    Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->deeponto) (3.4.2)
    Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->deeponto) (3.10)
    Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->deeponto) (2.4.0)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->deeponto) (2025.4.26)
    Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->deeponto) (1.3.0)
    Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->deeponto) (0.1.5)
    Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy->deeponto) (1.5.4)
    Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy->deeponto) (14.0.0)
    Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->deeponto) (0.21.0)
    Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->deeponto) (7.1.0)
    Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->deeponto) (6.5.7)
    Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy->deeponto) (3.0.2)
    Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (2.6.1)
    Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (1.3.2)
    Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (25.3.0)
    Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (1.6.0)
    Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (6.4.3)
    Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (0.3.1)
    Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->deeponto) (1.20.0)
    Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->deeponto) (0.8.4)
    Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel->deeponto) (4.3.8)
    Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->deeponto) (1.2.1)
    Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (23.1.0)
    Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (5.10.4)
    Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (7.16.6)
    Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.8.3)
    Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.18.1)
    Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.21.1)
    Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.3.1)
    Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->deeponto) (0.7.0)
    Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->deeponto) (3.0.0)
    Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->deeponto) (1.17.2)
    Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->deeponto) (0.1.2)
    Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.2.4)
    Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (4.13.4)
    Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (6.2.0)
    Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.7.1)
    Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.3.0)
    Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (3.1.3)
    Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.10.2)
    Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.5.1)
    Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (2.21.1)
    Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (4.23.0)
    Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (21.2.0)
    Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.5.1)
    Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.4.0)
    Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (2025.4.1)
    Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.36.2)
    Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (0.24.0)
    Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.16.0)
    Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.17.1)
    Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (2.7)
    Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (2.22)
    Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (4.9.0)
    Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.8.0)
    Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->deeponto) (1.3.1)
    Downloading deeponto-0.9.3-py3-none-any.whl (89.7 MB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m89.7/89.7 MB[0m [31m13.5 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading anytree-2.13.0-py3-none-any.whl (45 kB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m45.1/45.1 kB[0m [31m3.6 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m491.5/491.5 kB[0m [31m36.0 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m116.3/116.3 kB[0m [31m11.1 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading enlighten-1.14.1-py2.py3-none-any.whl (42 kB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m42.3/42.3 kB[0m [31m3.5 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m494.1/494.1 kB[0m [31m35.7 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading lxml-5.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.9 MB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m4.9/4.9 MB[0m [31m116.6 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading pprintpp-0.4.0-py2.py3-none-any.whl (16 kB)
    Downloading rdflib-7.1.4-py3-none-any.whl (565 kB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m565.1/565.1 kB[0m [31m25.4 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading textdistance-4.6.3-py3-none-any.whl (31 kB)
    Downloading yacs-0.1.8-py3-none-any.whl (14 kB)
    Downloading blessed-1.21.0-py2.py3-none-any.whl (84 kB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m84.7/84.7 kB[0m [31m7.5 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m193.6/193.6 kB[0m [31m16.1 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m143.5/143.5 kB[0m [31m12.2 MB/s[0m eta [36m0:00:00[0m
    [?25hDownloading prefixed-0.9.0-py2.py3-none-any.whl (13 kB)
    Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.6/1.6 MB[0m [31m50.2 MB/s[0m eta [36m0:00:00[0m
    [?25hInstalling collected packages: prefixed, pprintpp, yacs, textdistance, rdflib, lxml, JPype1, jedi, fsspec, dill, blessed, anytree, multiprocess, enlighten, datasets, deeponto
      Attempting uninstall: fsspec
        Found existing installation: fsspec 2025.5.0
        Uninstalling fsspec-2025.5.0:
          Successfully uninstalled fsspec-2025.5.0
    Successfully installed JPype1-1.5.2 anytree-2.13.0 blessed-1.21.0 datasets-3.6.0 deeponto-0.9.3 dill-0.3.8 enlighten-1.14.1 fsspec-2025.3.0 jedi-0.19.2 lxml-5.4.0 multiprocess-0.70.16 pprintpp-0.4.0 prefixed-0.9.0 rdflib-7.1.4 textdistance-4.6.3 yacs-0.1.8
    /bin/bash: line 1: username: No such file or directory



```python
!pip install numpy --upgrade
```

    Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.2.6)



```python
# Import pandas for data manipulation and analysis, such as loading, processing, and saving tabular data.
import pandas as pd

# Import pickle for saving and loading serialized objects (e.g., trained models or preprocessed data).
import pickle

# Import function to convert a directed graph to an undirected one, useful for certain graph algorithms.
from torch_geometric.utils import to_undirected

# Import optimizer module from PyTorch for training models using gradient-based optimization techniques.
import torch.optim as optim

# Import PyTorch's modules for defining neural network architectures and operations:
from torch.nn import (
    Linear,       # For linear transformations (dense layers).
    Sequential,   # For stacking layers sequentially.
    BatchNorm1d,  # For normalizing input within mini-batches.
    PReLU,        # Parametric ReLU activation function.
    Dropout       # For regularization by randomly dropping connections during training.
)

# Import functional API from PyTorch for operations like activations and loss functions.
import torch.nn.functional as F

# Import Matplotlib for visualizations, such as plotting training loss curves.
import matplotlib.pyplot as plt

# Import PyTorch Geometric's graph convolutional layers:
from torch_geometric.nn import GCNConv, GINConv

# Import pooling operations for aggregating node embeddings to graph-level representations:
from torch_geometric.nn import global_mean_pool, global_add_pool

# Import NumPy for numerical operations, such as working with arrays and matrices.
import numpy as np


# Import time module for measuring execution time of code blocks.
import time

# Import typing module for specifying types in function arguments and return values.
from typing import Optional, Tuple, Union, Callable

# Import PyTorch's DataLoader and TensorDataset for handling data batching and loading during training.
from torch.utils.data import DataLoader, TensorDataset

# Import PyTorch's Parameter class for defining learnable parameters in custom models.
from torch.nn import Parameter

# Import math module for performing mathematical computations.
import math

# Import Tensor type from PyTorch for defining and manipulating tensors.
from torch import Tensor

# Import PyTorch's nn module for defining and building neural network architectures.
import torch.nn as nn

# Import initialization utilities from PyTorch Geometric for resetting weights and biases in layers.
from torch_geometric.nn.inits import reset

# Import the base class for defining message-passing layers in graph neural networks (GNNs).
from torch_geometric.nn.conv import MessagePassing

# Import linear transformation utilities for creating dense representations in graph models.
from torch_geometric.nn.dense.linear import Linear

# Import typing utilities for defining adjacency matrices and tensor types specific to PyTorch Geometric.
from torch_geometric.typing import Adj, OptTensor, PairTensor, SparseTensor

# Import softmax function for normalizing attention scores in GNNs.
from torch_geometric.utils import softmax

# Import initialization utilities for weight initialization (e.g., Glorot initialization).
from torch_geometric.nn.inits import glorot, zeros

# Import F1 score metric from scikit-learn for evaluating model performance in binary/multi-class tasks.
from sklearn.metrics import f1_score

# Import JSON module for reading and writing JSON files, useful for storing configuration or ontology data.
import json

# Import Ontology class from DeepOnto for representing and manipulating ontologies in the pipeline.
from deeponto.onto import Ontology

# Import tools from DeepOnto for handling Ontology Alignment Evaluation Initiative (OAEI) tasks.
from deeponto.align.oaei import *

# Import evaluation tools from DeepOnto for assessing alignment results using metrics like precision, recall, and F1.
from deeponto.align.evaluation import AlignmentEvaluator

# Import mapping utilities from DeepOnto for working with reference mappings and entity pairs.
from deeponto.align.mapping import ReferenceMapping, EntityMapping

# Import utility function for reading tables (e.g., TSV, CSV) from DeepOnto.
from deeponto.utils import read_table

# Importing the train_test_split function from sklearn's model_selection module.
from sklearn.model_selection import train_test_split
```


```python
import random

# Set the seed for PyTorch's random number generator to ensure reproducibility
torch.manual_seed(42)

# Set the seed for NumPy's random number generator to ensure reproducibility
np.random.seed(42)

# Set the seed for Python's built-in random module to ensure reproducibility
random.seed(42)
```

# **Paths Definition**


```python
# Importing the 'drive' module from Google Colab to interact with Google Drive
from google.colab import drive

# Mount the user's Google Drive to the Colab environment
# After running this, a link will appear to authorize access, and Google Drive will be mounted at '/content/gdrive'
drive.mount('/content/gdrive')

```


```python
# Define the source ontology name
src_ent = "snomed.neoplas"

# Define the target ontology name
tgt_ent = "ncit.neoplas"

# Define the task name for this ontology matching process
task = "neoplas"

# Define the similarity threshold for validating matches
thres = 0.20
```


```python
dir = "/content/gdrive/My Drive/BioGITOM-VLDB/"

# Define the directory for the dataset containing source and target ontologies
dataset_dir = f"{dir}/Datasets/{task}"

# Define the data directory for storing embeddings, adjacency matrices, and related files
data_dir = f"{dir}/{task}/Data"

# Define the directory for storing the results
results_dir = f"{dir}/{task}/Results"
```


```python
# Load the Source ontology using the Ontology class from DeepOnto
# This initializes the source ontology by loading its .owl file.
src_onto = Ontology(f"{dataset_dir}/{src_ent}.owl")

# Load the Target ontology using the Ontology class from DeepOnto
# This initializes the target ontology by loading its .owl file.
tgt_onto = Ontology(f"{dataset_dir}/{tgt_ent}.owl")

# Define the file path for the Source embeddings CSV file
# Embeddings for the source ontology entities are stored in this file.
src_Emb = f"{data_dir}/{src_ent}_Sentence_SapBERT_emb.csv"

# Define the file path for the Target embeddings CSV file
# Embeddings for the target ontology entities are stored in this file.
tgt_Emb = f"{data_dir}/{tgt_ent}_Sentence_SapBERT_emb.csv"

# Define the file path for the Source adjacency matrix
# This file represents the relationships (edges) between entities in the source ontology.
src_Adjacence = f"{data_dir}/{src_ent}_adjacence.csv"

# Define the file path for the Target adjacency matrix
# This file represents the relationships (edges) between entities in the target ontology.
tgt_Adjacence = f"{data_dir}/{tgt_ent}_adjacence.csv"

# Define the file path for the JSON file containing the Source ontology class labels
# This file maps the source ontology entities to their labels or names.
src_class = f"{data_dir}/{src_ent}_classes.json"

# Define the file path for the JSON file containing the Target ontology class labels
# This file maps the target ontology entities to their labels or names.
tgt_class = f"{data_dir}/{tgt_ent}_classes.json"

# Define the file path for the train data
train_file = f"{data_dir}/{task}_train.csv"

# Define the file path for the test data
# The test file contains reference mappings (ground truth) between the source and target ontologies.
test_file = f"{dataset_dir}/refs_equiv/test.tsv"

# Define the file path for the candidate mappings used during testing
# This file includes the candidate pairs (source and target entities) for ranking and evaluation.
test_cands = f"{dataset_dir}/refs_equiv/test.cands.tsv"

# Define the file path for the candidate mappings between Source to Target entities
# This file contains cleaned, combined, and encoded candidates used for predictions.
candidates_Prediction = "/content/gdrive/My Drive/BioGITOM-VLDB/neoplas/Results/neoplas_top1000_mappings_euclidean2_filtered_encoded.csv"

# Define the file path for the candidate mappings between Source to Target entities for ranking-based metrics
# This file is used to compute ranking-based metrics like MRR and Hits@k.
candidates_Rank = f"{data_dir}/{task}_candidates.csv"

# Define the path where the prediction results will be saved in TSV format
# This file will store the final predictions (mappings) between source and target entities.
prediction_path = f"{results_dir}/{task}_matching_results.tsv"

# Define the path where all prediction results will be saved in TSV format
# This file will store detailed prediction results, including all candidate scores.
all_predictions_path = f"{results_dir}/{task}_all_predictions.tsv"

# Define the path where all ranking prediction results will be saved in TSV format
# This file will store predictions sorted by rank based on their scores.
all_predictions_path_ranked = f"{results_dir}/{task}_all_predictions_ranked.tsv"

# Define the path where formatted ranking predictions will be saved in TSV format
# This file will contain predictions formatted for evaluation using ranking-based metrics.
formatted_predictions_path = f"{results_dir}/{task}_formatted_predictions.tsv"
```

# **GIT Architecture**



```python
# RGIT class definition which inherits from PyTorch Geometric's MessagePassing class
class RGIT(MessagePassing):

    _alpha: OptTensor  # Define _alpha as an optional tensor for storing attention weights

    def __init__(
        self,
        nn: Callable,  # Neural network to be used in the final layer of the GNN
        in_channels: Union[int, Tuple[int, int]],  # Input dimension, can be a single or pair of integers
        out_channels: int,  # Output dimension of the GNN
        eps: float = 0.,  # GIN parameter: epsilon for GIN aggregation
        train_eps: bool = False,  # GIN parameter: whether epsilon should be learnable
        heads: int = 1,  # Transformer parameter: number of attention heads
        dropout: float = 0.,  # Dropout rate for attention weights
        edge_dim: Optional[int] = None,  # Dimension for edge attributes (optional)
        bias: bool = True,  # Whether to use bias in linear layers
        root_weight: bool = True,  # GIN parameter: whether to apply root weight in aggregation
        **kwargs,  # Additional arguments passed to the parent class
    ):
        # Set the aggregation type to 'add' and initialize the parent class with node_dim=0
        kwargs.setdefault('aggr', 'add')
        super().__init__(node_dim=0, **kwargs)

        # Initialize input/output dimensions, neural network, and GIN/transformer parameters
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.nn = nn  # Neural network used by the GNN
        self.initial_eps = eps  # Initial value of epsilon for GIN

        # Set epsilon to be learnable or fixed
        if train_eps:
            self.eps = torch.nn.Parameter(torch.empty(1))  # Learnable epsilon
        else:
            self.register_buffer('eps', torch.empty(1))  # Non-learnable epsilon (fixed)

        # Initialize transformer-related parameters
        self.heads = heads
        self.dropout = dropout
        self.edge_dim = edge_dim
        self._alpha = None  # Placeholder for attention weights

        # Handle case where in_channels is a single integer or a tuple
        if isinstance(in_channels, int):
            in_channels = (in_channels, in_channels)

        # Define the linear layers for key, query, and value for the transformer mechanism
        self.lin_key = Linear(in_channels[0], heads * out_channels)
        self.lin_query = Linear(in_channels[1], heads * out_channels)
        self.lin_value = Linear(in_channels[0], heads * out_channels)

        # Define linear transformation for edge embeddings if provided
        if edge_dim is not None:
            self.lin_edge = Linear(edge_dim, heads * out_channels, bias=False)
        else:
            self.lin_edge = self.register_parameter('lin_edge', None)

        # Reset all parameters to their initial values
        self.reset_parameters()

    # Function to reset model parameters
    def reset_parameters(self):
        super().reset_parameters()  # Call parent class reset method
        self.lin_key.reset_parameters()  # Reset key linear layer
        self.lin_query.reset_parameters()  # Reset query linear layer
        self.lin_value.reset_parameters()  # Reset value linear layer
        if self.edge_dim:
            self.lin_edge.reset_parameters()  # Reset edge linear layer if used
        reset(self.nn)  # Reset the neural network provided
        self.eps.data.fill_(self.initial_eps)  # Initialize epsilon with the starting value

    # Forward function defining how the input data flows through the model
    def forward(self, x: Union[Tensor, PairTensor], edge_index: Adj,
                edge_attr: OptTensor = None, return_attention_weights=None):
        # Unpack number of heads and output channels
        H, C = self.heads, self.out_channels

        # If x is a tensor, treat it as a pair of tensors (source and target embeddings)
        if isinstance(x, Tensor):
            x: PairTensor = (x, x)

        # Extract source node embeddings
        x_t = x[0]

        # Apply linear transformations and reshape query, key, and value for multi-head attention
        query = self.lin_query(x[1]).view(-1, H, C)
        key = self.lin_key(x[0]).view(-1, H, C)
        value = self.lin_value(x[0]).view(-1, H, C)

        # Propagate messages through the graph using the propagate function
        out = self.propagate(edge_index, query=query, key=key, value=value,
                             edge_attr=edge_attr, size=None)

        # Retrieve attention weights and reset them
        alpha = self._alpha
        self._alpha = None  # Reset _alpha after use
        out = out.mean(dim=1)  # Take the mean over all attention heads

        # Apply GIN aggregation by adding epsilon-scaled original node embeddings
        out = out + (1 + self.eps) * x_t
        return self.nn(out)  # Pass through the neural network

    # Message passing function which calculates attention and combines messages
    def message(self, query_i: Tensor, key_j: Tensor, value_j: Tensor,
                edge_attr: OptTensor, index: Tensor, ptr: OptTensor,
                size_i: Optional[int]) -> Tensor:
        # If edge attributes are used, apply linear transformation and add them to the key
        if self.lin_edge is not None:
            assert edge_attr is not None
            edge_attr = self.lin_edge(edge_attr).view(-1, self.heads, self.out_channels)
            key_j = key_j + edge_attr

        # Calculate attention (alpha) using the dot product between query and key
        alpha = (query_i * key_j).sum(dim=-1) / math.sqrt(self.out_channels)
        alpha = softmax(alpha, index, ptr, size_i)  # Apply softmax to normalize attention
        self._alpha = alpha  # Store attention weights
        alpha = F.dropout(alpha, p=self.dropout, training=self.training)  # Apply dropout

        # Calculate the output message by applying attention to the value
        out = value_j
        if edge_attr is not None:
            out = out + edge_attr  # Add edge embeddings to the output if present
        out = out * alpha.view(-1, self.heads, 1)  # Scale by attention weights
        return out

    # String representation function for debugging or printing
    def __repr__(self) -> str:
        return (f'{self.__class__.__name__}({self.in_channels}, '
                f'{self.out_channels}, heads={self.heads})')
```


```python
# Define the RGIT_mod class, a multi-layer GNN that uses both RGIT and linear layers
class RGIT_mod(torch.nn.Module):
    """Multi-layer RGIT with optional linear layers"""

    # Initialize the model with hidden dimension, number of RGIT layers, and number of linear layers
    def __init__(self, dim_h, num_layers, num_linear_layers=1):
        super(RGIT_mod, self).__init__()
        self.num_layers = num_layers  # Number of RGIT layers
        self.num_linear_layers = num_linear_layers  # Number of linear layers
        self.linears = torch.nn.ModuleList()  # List to store linear layers
        self.rgit_layers = torch.nn.ModuleList()  # List to store RGIT layers

        # Create a list of Linear and PReLU layers (for encoding entity names)
        for _ in range(num_linear_layers):
            self.linears.append(Linear(dim_h, dim_h))  # Linear transformation layer
            self.linears.append(PReLU(num_parameters=dim_h))  # Parametric ReLU activation function

        # Create a list of RGIT layers
        for _ in range(num_layers):
            self.rgit_layers.append(RGIT(  # Each RGIT layer contains a small MLP with Linear and PReLU
                Sequential(Linear(dim_h, dim_h), PReLU(num_parameters=dim_h),
                           Linear(dim_h, dim_h), PReLU(num_parameters=dim_h)), dim_h, dim_h))

    # Forward pass through the model
    def forward(self, x, edge_index):
        # Apply the linear layers first to the input
        for layer in self.linears:
            x = layer(x)

        # Then apply the RGIT layers for message passing
        for layer in self.rgit_layers:
            x = layer(x, edge_index)

        return x  # Return the final node embeddings after all layers

```

# **Gated Network Architecture**


```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class GatedCombination(nn.Module):
    def __init__(self, input_dim):
        super(GatedCombination, self).__init__()
        self.gate_A_fc = nn.Linear(input_dim, input_dim)
        self.gate_B_fc = nn.Linear(input_dim, input_dim)
        self.fc = nn.Linear(1, 1)

    def euclidean_distance(self, a, b):
        """
        Compute the Euclidean distance between two tensors.
        Args:
            a: Tensor of shape [batch, dim]
            b: Tensor of shape [batch, dim]
        Returns:
            Tensor of shape [batch] representing the L2 distance.
        """
        return torch.norm(a - b, p=2, dim=1)

    def forward(self, x1, x2, x3, x4, return_embeddings=False):
        gate_values1 = torch.sigmoid(self.gate_A_fc(x1))
        a = x1 * gate_values1 + x2 * (1 - gate_values1)

        gate_values2 = torch.sigmoid(self.gate_B_fc(x3))
        b = x3 * gate_values2 + x4 * (1 - gate_values2)

        if return_embeddings:
            return a, b

        # Utilisation de la distance Euclidienne
        distance = self.euclidean_distance(a, b)

        # Passage dans couche de classification
        out = torch.sigmoid(self.fc(distance.unsqueeze(1)))
        return out

```

# **Utility functions**


```python
def adjacency_matrix_to_undirected_edge_index(adjacency_matrix):
    """
    Converts an adjacency matrix into an undirected edge index for use in graph-based neural networks.

    Args:
        adjacency_matrix: A 2D list or array representing the adjacency matrix of a graph.

    Returns:
        edge_index_undirected: A PyTorch tensor representing the undirected edges.
    """
    # Convert each element in the adjacency matrix to an integer (from boolean or float)
    adjacency_matrix = [[int(element) for element in sublist] for sublist in adjacency_matrix]

    # Convert the adjacency matrix into a PyTorch LongTensor (used for indexing)
    edge_index = torch.tensor(adjacency_matrix, dtype=torch.long)

    # Transpose the edge_index tensor so that rows represent edges in the form [source, target]
    edge_index = edge_index.t().contiguous()

    # Convert the directed edge_index into an undirected edge_index, meaning both directions are added (i.e., (i, j) and (j, i))
    edge_index_undirected = to_undirected(edge_index)

    return edge_index_undirected  # Return the undirected edge index
```


```python
def build_indexed_dict(file_path):
    """
    Builds a dictionary with numeric indexes for each key from a JSON file.

    Args:
        file_path (str): The path to the JSON file.

    Returns:
        indexed_dict (dict): A new dictionary where each key from the JSON file is assigned a numeric index.
    """
    # Load the JSON file into a Python dictionary
    with open(file_path, 'r') as file:
        data = json.load(file)

    # Create a new dictionary with numeric indexes as keys and the original JSON keys as values
    indexed_dict = {index: key for index, key in enumerate(data.keys())}

    return indexed_dict  # Return the newly created dictionary
```


```python
def select_rows_by_index(embedding_vector, index_vector):
    """
    Select rows from an embedding vector using an index vector.

    Args:
        embedding_vector (torch.Tensor): 2D tensor representing the embedding vector with shape [num_rows, embedding_size].
        index_vector (torch.Tensor): 1D tensor representing the index vector.

    Returns:
        torch.Tensor: New tensor with selected rows from the embedding vector.
    """
    # Use torch.index_select to select the desired rows
    new_tensor = torch.index_select(embedding_vector, 0, index_vector)

    return new_tensor
```


```python
def contrastive_loss(source_embeddings, target_embeddings, labels, margin=1.0):
    """
    Computes the contrastive loss, a type of loss function used to train models in tasks like matching or similarity learning.

    Args:
        source_embeddings (torch.Tensor): Embeddings of the source graphs, shape [batch_size, embedding_size].
        target_embeddings (torch.Tensor): Embeddings of the target graphs, shape [batch_size, embedding_size].
        labels (torch.Tensor): Binary labels indicating if the pairs are matched (1) or not (0), shape [batch_size].
        margin (float): Margin value for the contrastive loss. Defaults to 1.0.

    Returns:
        torch.Tensor: The contrastive loss value.
    """
    # Calculate the pairwise Euclidean distance between source and target embeddings
    distances = F.pairwise_distance(source_embeddings, target_embeddings)

    # Compute the contrastive loss:
    # - For matched pairs (label == 1), the loss is the squared distance between embeddings.
    # - For non-matched pairs (label == 0), the loss is based on how far apart the embeddings are,
    #   but penalizes them only if the distance is less than the margin.
    loss = torch.mean(
        labels * 0.4 * distances.pow(2) +  # For positive pairs, minimize the distance (squared)
        (1 - labels) * 0.4 * torch.max(torch.zeros_like(distances), margin - distances).pow(2)  # For negative pairs, maximize the distance (up to the margin)
    )

    return loss  # Return the computed contrastive loss

```


```python
import torch
from torch.utils.data import TensorDataset, DataLoader
import pandas as pd
import time

def compute_diem(a, b, eps=1e-8):
    """
    Compute DIEM distance: squared L2 distance / (||a|| * ||b||)
    """
    diff = a - b
    euclidean_sq = torch.sum(diff ** 2, dim=1)
    norm_a = torch.norm(a, dim=1)
    norm_b = torch.norm(b, dim=1)
    denom = norm_a * norm_b + eps
    return euclidean_sq / denom
```


```python
def Prediction_with_candidates(model, X1_tt, X2_tt, X3_tt, X4_tt,
                               src_entity_tensor_o, tgt_entity_tensor_o,
                               indexed_dict_src, indexed_dict_tgt, all_predictions_path):
    """
    Evaluates similarity scores between candidate pairs using DIEM distance
    after combining embeddings with a gating mechanism.
    """
    model = model.to("cpu")
    model.eval()
    batch_size_test = 32

    test_dataset = TensorDataset(X1_tt, X2_tt, X3_tt, X4_tt)
    test_dataloader = DataLoader(test_dataset, batch_size=batch_size_test)

    predictions = []
    results = []
    count_predictions = 0

    start_time = time.time()

    with torch.no_grad():
        for batch_X1, batch_X2, batch_X3, batch_X4 in test_dataloader:
            a, b = model(batch_X1, batch_X2, batch_X3, batch_X4, return_embeddings=True)
            diem_scores = compute_diem(a, b)
            predictions.extend(diem_scores.cpu().numpy())

    end_time = time.time()
    print(f"Predicting time: {end_time - start_time:.2f} seconds")

    # Convert tensors to list
    src_indices = src_entity_tensor_o.tolist()
    tgt_indices = tgt_entity_tensor_o.tolist()

    for i in range(len(predictions)):
        score = predictions[i]
        src_code = src_indices[i]
        tgt_code = tgt_indices[i]

        src_uri = indexed_dict_src.get(int(src_code), "Unknown URI")
        tgt_uri = indexed_dict_tgt.get(int(tgt_code), "Unknown URI")

        # DIEM est une distance : on peut convertir en similarité si nécessaire
        similarity = 1 / (1 + score)  # optionnel

        results.append({
            'SrcEntity': src_uri,
            'TgtEntity': tgt_uri,
            'Score': similarity  # ou score si tu préfères la distance brute
        })
        count_predictions += 1

    df_results = pd.DataFrame(results)
    df_results.to_csv(all_predictions_path, sep='\t', index=False)
    print(f"Predictions saved to {all_predictions_path}")


    # Convert the results into a pandas DataFrame
    df_results = pd.DataFrame(results)

    # Save the results to a TSV file
    df_results.to_csv(all_predictions_path, sep='\t', index=False)

    print(f"Predictions saved to {all_predictions_path}")
```


```python
import pandas as pd

def filter_highest_predictions(input_file, output_file, threshold=0.3):
    """
    Filter the highest scoring predictions for each source entity from a TSV file.

    Args:
        input_file (str): Path to the TSV file containing predictions with 'SrcEntity', 'TgtEntity', 'Score'.
        output_file (str): Path to save the filtered predictions.
        threshold (float): Minimum score to consider a mapping.

    Returns:
        pd.DataFrame: DataFrame containing filtered top-1 mappings.
    """
    df = pd.read_csv(input_file, sep='\t')

    # Ensure Score column is float, even if saved as strings with brackets
    def safe_parse(x):
        if isinstance(x, str):
            x = x.strip('[]')
        return float(x)

    df['Score'] = df['Score'].apply(safe_parse)

    # Keep only mappings with score above threshold
    df = df[df['Score'] >= threshold]

    # Select top-1 TgtEntity per SrcEntity (highest scoring)
    df_sorted = df.sort_values(by='Score', ascending=False)
    df_top = df_sorted.groupby('SrcEntity').first().reset_index()

    # Save filtered predictions
    df_top.to_csv(output_file, sep='\t', index=False)
    print(f"Filtered top-1 predictions saved to: {output_file}")
    return df_top

```


```python
def compute_mrr_and_hits(reference_file, predicted_file, output_file, k_values=[1, 5, 10]):
    """
    Compute MRR and Hits@k for ontology matching predictions based on a reference file.

    Args:
        reference_file (str): Path to the reference file (test.cands.tsv format).
        predicted_file (str): Path to the predictions file with scores.
        output_file (str): Path to save the scored results.
        k_values (list): List of k values for Hits@k.

    Returns:
        dict: A dictionary containing MRR and Hits@k metrics.
    """
    # Read the reference mappings
    test_candidate_mappings = read_table(reference_file).values.tolist()
    ranking_results = []

    # Read the predicted scores
    predicted_data = pd.read_csv(predicted_file, sep="\t")
    predicted_data["Score"] = predicted_data["Score"].apply(lambda x: float(x.strip("[]")) if isinstance(x, str) else float(x)
)

    # Create a lookup dictionary for predicted scores
    score_lookup = {}
    for _, row in predicted_data.iterrows():
        score_lookup[(row["SrcEntity"], row["TgtEntity"])] = row["Score"]

    for src_ref_class, tgt_ref_class, tgt_cands in test_candidate_mappings:
        tgt_cands = eval(tgt_cands)  # Convert string to list of candidates
        scored_cands = []
        for tgt_cand in tgt_cands:
            # Retrieve score for each candidate, defaulting to a very low score if not found
            matching_score = score_lookup.get((src_ref_class, tgt_cand), -1e9)
            scored_cands.append((tgt_cand, matching_score))

        # Sort candidates by score in descending order
        scored_cands = sorted(scored_cands, key=lambda x: x[1], reverse=True)
        ranking_results.append((src_ref_class, tgt_ref_class, scored_cands))

    # Save the ranked results to a file
    pd.DataFrame(ranking_results, columns=["SrcEntity", "TgtEntity", "TgtCandidates"]).to_csv(output_file, sep="\t", index=False)

    # Compute MRR and Hits@k
    total_entities = len(ranking_results)
    reciprocal_ranks = []
    hits_at_k = {k: 0 for k in k_values}

    for src_entity, tgt_ref_class, tgt_cands in ranking_results:
        ranked_candidates = [candidate[0] for candidate in tgt_cands]
        if tgt_ref_class in ranked_candidates:
            rank = ranked_candidates.index(tgt_ref_class) + 1
            reciprocal_ranks.append(1 / rank)
            for k in k_values:
                if rank <= k:
                    hits_at_k[k] += 1
        else:
            reciprocal_ranks.append(0)

    mrr = sum(reciprocal_ranks) / total_entities
    hits_at_k = {k: hits / total_entities for k, hits in hits_at_k.items()}

    return {"MRR": mrr, "Hits@k": hits_at_k}
```

# **Main Code**






# Reading semantic node embeddings provided by the ENE


```python
# Read the source embeddings from a CSV file into a pandas DataFrame
df_embbedings_src = pd.read_csv(src_Emb, index_col=0)

# Convert the DataFrame to a NumPy array, which will remove the index and store the data as a raw matrix
numpy_array = df_embbedings_src.to_numpy()

# Convert the NumPy array into a PyTorch FloatTensor, which is the format required for PyTorch operations
x_src = torch.FloatTensor(numpy_array)
```


```python
# Read the target embeddings from a CSV file into a pandas DataFrame
df_embbedings_tgt = pd.read_csv(tgt_Emb, index_col=0)

# Convert the DataFrame to a NumPy array, which removes the index and converts the data to a raw matrix
numpy_array = df_embbedings_tgt.to_numpy()

# Convert the NumPy array into a PyTorch FloatTensor, which is required for PyTorch operations
x_tgt = torch.FloatTensor(numpy_array)
```

# Reading adjacency Matrix


```python
# Read the source adjacency matrix from a CSV file into a pandas DataFrame
df_ma1 = pd.read_csv(src_Adjacence, index_col=0)

# Convert the DataFrame to a list of lists (Python native list format)
ma1 = df_ma1.values.tolist()
```


```python
# Read the target adjacency matrix from a CSV file into a pandas DataFrame
df_ma2 = pd.read_csv(tgt_Adjacence, index_col=0)

# Convert the DataFrame to a list of lists (Python native list format)
ma2 = df_ma2.values.tolist()
```

# Convert Adjacency matrix (in list format) to an undirected edge index


```python
# Convert the source adjacency matrix (in list format) to an undirected edge index for PyTorch Geometric
edge_src = adjacency_matrix_to_undirected_edge_index(ma1)

# Convert the target adjacency matrix (in list format) to an undirected edge index for PyTorch Geometric
edge_tgt = adjacency_matrix_to_undirected_edge_index(ma2)
```

# GIT Training


```python
def train_model_gnn(model, x_src, edge_src, x_tgt, edge_tgt,
                    tensor_term1, tensor_term2, tensor_score,
                    learning_rate, weight_decay_value, num_epochs, print_interval=10):
    """
    Trains a graph neural network (GNN) model using source and target embeddings and contrastive loss.

    Args:
        model: The GNN model to be trained.
        x_src (torch.Tensor): Source node embeddings.
        edge_src (torch.Tensor): Source graph edges.
        x_tgt (torch.Tensor): Target node embeddings.
        edge_tgt (torch.Tensor): Target graph edges.
        tensor_term1 (torch.Tensor): Indices of the source nodes to be compared.
        tensor_term2 (torch.Tensor): Indices of the target nodes to be compared.
        tensor_score (torch.Tensor): Labels indicating if the pairs are matched (1) or not (0).
        learning_rate (float): Learning rate for the optimizer.
        weight_decay_value (float): Weight decay (L2 regularization) value for the optimizer.
        num_epochs (int): Number of epochs for training.
        print_interval (int): Interval at which training progress is printed (every `print_interval` epochs).

    Returns:
        model: The trained GNN model.
    """

    # Step 1: Set device (GPU or CPU) for computation
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # Step 2: Move the model and all inputs to the selected device
    model.to(device)
    x_tgt = x_tgt.to(device)               # Target node embeddings
    edge_tgt = edge_tgt.to(device)         # Target graph edges
    x_src = x_src.to(device)               # Source node embeddings
    edge_src = edge_src.to(device)         # Source graph edges
    tensor_term1 = tensor_term1.to(device) # Indices for source nodes
    tensor_term2 = tensor_term2.to(device) # Indices for target nodes
    tensor_score = tensor_score.to(device) # Ground truth labels

    # Step 3: Define optimizer with learning rate and regularization
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay_value)

    # Step 4: Initialize list to store training losses
    train_losses = []

    # Record the start time of training
    start_time = time.time()

    # Step 5: Training loop
    for epoch in range(num_epochs):
        # Zero out gradients from the previous iteration
        optimizer.zero_grad()

        # Forward pass: Compute embeddings for source and target graphs
        out1 = model(x_src, edge_src)  # Updated source embeddings
        out2 = model(x_tgt, edge_tgt)  # Updated target embeddings

        # Extract specific rows of embeddings for terms being compared
        src_embeddings = select_rows_by_index(out1, tensor_term1)
        tgt_embeddings = select_rows_by_index(out2, tensor_term2)

        # Compute contrastive loss based on the embeddings and ground truth labels
        loss = contrastive_loss(src_embeddings, tgt_embeddings, tensor_score)

        # Backward pass: Compute gradients
        loss.backward()

        # Update the model's parameters
        optimizer.step()

        # Append the loss for this iteration to the list
        train_losses.append(loss.item())

        # Print loss every `print_interval` epochs
        if (epoch + 1) % print_interval == 0:
            print(f"Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item()}")

    # Step 6: Record end time of training
    end_time = time.time()

    # Step 7: Plot the training loss over time
    plt.semilogy(range(1, num_epochs + 1), train_losses, label="Training Loss", marker='o')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Print the total training time
    training_time = end_time - start_time
    print(f"Training complete! Total training time: {training_time:.2f} seconds")

    # Step 8: Return the trained model
    return model
```


```python
# Initialize the GIT_mod model with the dimensionality of the target embeddings
# The first argument is the dimensionality of the target node embeddings (x_tgt.shape[1])
# The second argument (1) represents the number of RGIT layers in the model
GIT_model = RGIT_mod(x_tgt.shape[1], 1)
```


```python
# Reading the training pairs from a CSV file into a pandas DataFrame
df_embbedings = pd.read_csv(train_file, index_col=0)

# Extract the 'SrcEntity' and 'TgtEntity' columns as NumPy arrays and convert them to integers
tensor_term1 = df_embbedings['SrcEntity'].values.astype(int)  # Source entity indices
tensor_term2 = df_embbedings['TgtEntity'].values.astype(int)  # Target entity indices

# Extract the 'Score' column as a NumPy array and convert it to floats
tensor_score = df_embbedings['Score'].values.astype(float)  # Scores (labels) indicating if pairs match (1) or not (0)

# Convert the NumPy arrays to PyTorch LongTensors (for indices) and FloatTensors (for scores)
tensor_term1_o = torch.from_numpy(tensor_term1).type(torch.LongTensor)  # Source entity tensor
tensor_term2_o = torch.from_numpy(tensor_term2).type(torch.LongTensor)  # Target entity tensor
tensor_score_o = torch.from_numpy(tensor_score).type(torch.FloatTensor)  # Score tensor
```


```python
# Train the GNN model using the provided source and target graph embeddings, edges, and training data
trained_model = train_model_gnn(
    model=GIT_model,                # The GNN model to be trained (initialized earlier)
    x_src=x_src,                    # Source node embeddings (tensor for source graph)
    edge_src=edge_src,              # Source graph edges (undirected edge index for source graph)
    x_tgt=x_tgt,                    # Target node embeddings (tensor for target graph)
    edge_tgt=edge_tgt,              # Target graph edges (undirected edge index for target graph)
    tensor_term1=tensor_term1_o,    # Indices of source entities for training
    tensor_term2=tensor_term2_o,    # Indices of target entities for training
    tensor_score=tensor_score_o,    # Scores (labels) indicating if pairs match (1) or not (0)
    learning_rate=0.0001,            # Learning rate for the Adam optimizer
    weight_decay_value=1e-4,        # Weight decay for L2 regularization to prevent overfitting
    num_epochs=1000,                # Number of training epochs
    print_interval=10               # Interval at which to print training progress (every 10 epochs)
)
```

    Epoch [10/1000], Training Loss: 0.0012123031774535775
    Epoch [20/1000], Training Loss: 0.0010304501047357917
    Epoch [30/1000], Training Loss: 0.0009226836264133453
    Epoch [40/1000], Training Loss: 0.0008444603299722075
    Epoch [50/1000], Training Loss: 0.0007805248606018722
    Epoch [60/1000], Training Loss: 0.0007267696782946587
    Epoch [70/1000], Training Loss: 0.000680740806274116
    Epoch [80/1000], Training Loss: 0.000640733283944428
    Epoch [90/1000], Training Loss: 0.0006054544937796891
    Epoch [100/1000], Training Loss: 0.0005741106579080224
    Epoch [110/1000], Training Loss: 0.0005459738313220441
    Epoch [120/1000], Training Loss: 0.0005211565876379609
    Epoch [130/1000], Training Loss: 0.000499098387081176
    Epoch [140/1000], Training Loss: 0.0004799014132004231
    Epoch [150/1000], Training Loss: 0.000463052187114954
    Epoch [160/1000], Training Loss: 0.00044837623136118054
    Epoch [170/1000], Training Loss: 0.0004354756965767592
    Epoch [180/1000], Training Loss: 0.0004240996204316616
    Epoch [190/1000], Training Loss: 0.00041412824066355824
    Epoch [200/1000], Training Loss: 0.0004054151941090822
    Epoch [210/1000], Training Loss: 0.0003978025633841753
    Epoch [220/1000], Training Loss: 0.00039113263483159244
    Epoch [230/1000], Training Loss: 0.0003853263333439827
    Epoch [240/1000], Training Loss: 0.000380210141884163
    Epoch [250/1000], Training Loss: 0.00037576601607725024
    Epoch [260/1000], Training Loss: 0.0003718039079103619
    Epoch [270/1000], Training Loss: 0.00036824794369749725
    Epoch [280/1000], Training Loss: 0.0003650466096587479
    Epoch [290/1000], Training Loss: 0.000362140970537439
    Epoch [300/1000], Training Loss: 0.0003595475573092699
    Epoch [310/1000], Training Loss: 0.00035710076917894185
    Epoch [320/1000], Training Loss: 0.000354828720446676
    Epoch [330/1000], Training Loss: 0.000352643895894289
    Epoch [340/1000], Training Loss: 0.00035053561441600323
    Epoch [350/1000], Training Loss: 0.0003485203778836876
    Epoch [360/1000], Training Loss: 0.00034662216785363853
    Epoch [370/1000], Training Loss: 0.00034482128103263676
    Epoch [380/1000], Training Loss: 0.00034305654116906226
    Epoch [390/1000], Training Loss: 0.0003415184619370848
    Epoch [400/1000], Training Loss: 0.0003401192370802164
    Epoch [410/1000], Training Loss: 0.0003388397744856775
    Epoch [420/1000], Training Loss: 0.0003376654349267483
    Epoch [430/1000], Training Loss: 0.0003365444717928767
    Epoch [440/1000], Training Loss: 0.00033547315979376435
    Epoch [450/1000], Training Loss: 0.00033445365261286497
    Epoch [460/1000], Training Loss: 0.0003334904904477298
    Epoch [470/1000], Training Loss: 0.0003325174911879003
    Epoch [480/1000], Training Loss: 0.0003315286012366414
    Epoch [490/1000], Training Loss: 0.0003305293503217399
    Epoch [500/1000], Training Loss: 0.00032945393468253314
    Epoch [510/1000], Training Loss: 0.0003283653350081295
    Epoch [520/1000], Training Loss: 0.00032727865618653595
    Epoch [530/1000], Training Loss: 0.0003262061800342053
    Epoch [540/1000], Training Loss: 0.0003251689486205578
    Epoch [550/1000], Training Loss: 0.00032417679904028773
    Epoch [560/1000], Training Loss: 0.00032326672226190567
    Epoch [570/1000], Training Loss: 0.0003223654057364911
    Epoch [580/1000], Training Loss: 0.0003214765165466815
    Epoch [590/1000], Training Loss: 0.00032062813988886774
    Epoch [600/1000], Training Loss: 0.00031978206243366003
    Epoch [610/1000], Training Loss: 0.00031894943094812334
    Epoch [620/1000], Training Loss: 0.0003181369393132627
    Epoch [630/1000], Training Loss: 0.000317336933221668
    Epoch [640/1000], Training Loss: 0.0003165398084092885
    Epoch [650/1000], Training Loss: 0.00031573441810905933
    Epoch [660/1000], Training Loss: 0.000314921053359285
    Epoch [670/1000], Training Loss: 0.0003141292545478791
    Epoch [680/1000], Training Loss: 0.0003133536665700376
    Epoch [690/1000], Training Loss: 0.00031254111672751606
    Epoch [700/1000], Training Loss: 0.00031171966111287475
    Epoch [710/1000], Training Loss: 0.0003108693636022508
    Epoch [720/1000], Training Loss: 0.0003099982859566808
    Epoch [730/1000], Training Loss: 0.0003090610262006521
    Epoch [740/1000], Training Loss: 0.0003080223686993122
    Epoch [750/1000], Training Loss: 0.0003069174417760223
    Epoch [760/1000], Training Loss: 0.0003057533467654139
    Epoch [770/1000], Training Loss: 0.0003045519406441599
    Epoch [780/1000], Training Loss: 0.00030340655939653516
    Epoch [790/1000], Training Loss: 0.00030235055601224303
    Epoch [800/1000], Training Loss: 0.0003013079985976219
    Epoch [810/1000], Training Loss: 0.00030031311325728893
    Epoch [820/1000], Training Loss: 0.0002993424714077264
    Epoch [830/1000], Training Loss: 0.0002984016318805516
    Epoch [840/1000], Training Loss: 0.00029746105428785086
    Epoch [850/1000], Training Loss: 0.00029653290403075516
    Epoch [860/1000], Training Loss: 0.00029565230943262577
    Epoch [870/1000], Training Loss: 0.00029479488148353994
    Epoch [880/1000], Training Loss: 0.00029392956639640033
    Epoch [890/1000], Training Loss: 0.0002930810151156038
    Epoch [900/1000], Training Loss: 0.0002922479761764407
    Epoch [910/1000], Training Loss: 0.00029142340645194054
    Epoch [920/1000], Training Loss: 0.0002906383597292006
    Epoch [930/1000], Training Loss: 0.0002898111124522984
    Epoch [940/1000], Training Loss: 0.00028900898178108037
    Epoch [950/1000], Training Loss: 0.00028822707827202976
    Epoch [960/1000], Training Loss: 0.0002874226192943752
    Epoch [970/1000], Training Loss: 0.0002866491849999875
    Epoch [980/1000], Training Loss: 0.0002858791849575937
    Epoch [990/1000], Training Loss: 0.0002851177705451846
    Epoch [1000/1000], Training Loss: 0.0002843774273060262



    
![png](output_38_1.png)
    


    Training complete! Total training time: 1596.70 seconds


# GIT Application


```python
# Determine if a GPU is available and move the computations to it; otherwise, use the CPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Assuming the model has been trained and hyperparameters (x_src, edge_src, x_tgt, edge_tgt) are set

# Move the trained GIT_model to the device (GPU or CPU)
GIT_model.to(device)

# Move the data tensors to the same device (GPU or CPU)
x_tgt = x_tgt.to(device)         # Target node embeddings
edge_tgt = edge_tgt.to(device)   # Target graph edges
x_src = x_src.to(device)         # Source node embeddings
edge_src = edge_src.to(device)   # Source graph edges

# Set the model to evaluation mode; this disables dropout and batch normalization
GIT_model.eval()

# Pass the source and target embeddings through the trained GNN model to update the embeddings
with torch.no_grad():  # Disable gradient computation (inference mode)
    embeddings_tgt = GIT_model(x_tgt, edge_tgt)  # Get updated embeddings for the target graph
    embeddings_src = GIT_model(x_src, edge_src)  # Get updated embeddings for the source graph

# Detach the embeddings from the computation graph and move them back to the CPU
# This step is useful if you need to use the embeddings for tasks outside PyTorch (e.g., saving to disk)
embeddings_tgt = embeddings_tgt.detach().cpu()  # Target graph embeddings
embeddings_src = embeddings_src.detach().cpu()  # Source graph embeddings

# At this point, embeddings_tgt and embeddings_src contain the updated embeddings, ready for downstream tasks
```

# Selecting embedding pairs to train the Gated Network


```python
# Read the training pairs from a CSV file into a pandas DataFrame
df_embeddings = pd.read_csv(train_file, index_col=0)

# Extract columns and convert to NumPy arrays
tensor_term1 = df_embeddings['SrcEntity'].values.astype(int)  # Source entity indices
tensor_term2 = df_embeddings['TgtEntity'].values.astype(int)  # Target entity indices
tensor_score = df_embeddings['Score'].values.astype(float)  # Matching scores

# Split data into training and validation sets
tensor_term1_train, tensor_term1_val, tensor_term2_train, tensor_term2_val, tensor_score_train, tensor_score_val = train_test_split(
    tensor_term1, tensor_term2, tensor_score, test_size=0.3, random_state=42
)

# Convert split data to PyTorch tensors
tensor_term1_train = torch.from_numpy(tensor_term1_train).type(torch.LongTensor)
tensor_term2_train = torch.from_numpy(tensor_term2_train).type(torch.LongTensor)
tensor_score_train = torch.from_numpy(tensor_score_train).type(torch.FloatTensor)

tensor_term1_val = torch.from_numpy(tensor_term1_val).type(torch.LongTensor)
tensor_term2_val = torch.from_numpy(tensor_term2_val).type(torch.LongTensor)
tensor_score_val = torch.from_numpy(tensor_score_val).type(torch.FloatTensor)

# Move the embeddings back to the CPU if not already there
x_tgt = x_tgt.cpu()  # Target node embeddings
x_src = x_src.cpu()  # Source node embeddings

# Select embeddings for the training set
X1_train = select_rows_by_index(embeddings_src, tensor_term1_train)
X2_train = select_rows_by_index(x_src, tensor_term1_train)
X3_train = select_rows_by_index(embeddings_tgt, tensor_term2_train)
X4_train = select_rows_by_index(x_tgt, tensor_term2_train)

# Select embeddings for the validation set
X1_val = select_rows_by_index(embeddings_src, tensor_term1_val)
X2_val = select_rows_by_index(x_src, tensor_term1_val)
X3_val = select_rows_by_index(embeddings_tgt, tensor_term2_val)
X4_val = select_rows_by_index(x_tgt, tensor_term2_val)

# Now you have:
# - Training tensors: X1_train, X2_train, X3_train, X4_train, tensor_score_train
# - Validation tensors: X1_val, X2_val, X3_val, X4_val, tensor_score_val
```

# Gated Network Training


```python
from sklearn.metrics import f1_score

def train_gated_combination_model(X1_t, X2_t, X3_t, X4_t, tensor_score_o,
                                  X1_val, X2_val, X3_val, X4_val, tensor_score_val,
                                  epochs=120, batch_size=32, learning_rate=0.001, weight_decay=1e-5):
    """
    Trains the GatedCombination model with training and validation data, using ReduceLROnPlateau scheduler.
    Also calculates and displays F1-score during training and validation.
    """

    # Create datasets and DataLoaders
    train_dataset = TensorDataset(X1_t, X2_t, X3_t, X4_t, tensor_score_o)
    val_dataset = TensorDataset(X1_val, X2_val, X3_val, X4_val, tensor_score_val)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = GatedCombination(X1_t.shape[1]).to(device)
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

    # Use ReduceLROnPlateau scheduler
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)

    train_losses, val_losses = [], []

    start_time = time.time()

    for epoch in range(epochs):
        model.train()
        total_train_loss = 0.0
        y_true_train, y_pred_train = [], []

        for batch_X1, batch_X2, batch_X3, batch_X4, batch_y in train_loader:
            batch_X1, batch_X2, batch_X3, batch_X4, batch_y = (
                batch_X1.to(device),
                batch_X2.to(device),
                batch_X3.to(device),
                batch_X4.to(device),
                batch_y.to(device),
            )
            optimizer.zero_grad()

            # Forward pass
            outputs = model(batch_X1, batch_X2, batch_X3, batch_X4)

            # Compute loss
            loss = F.binary_cross_entropy(outputs, batch_y.unsqueeze(1).float())
            loss.backward()
            optimizer.step()
            total_train_loss += loss.item()

            # Store true labels and predictions for F1-score
            y_true_train.extend(batch_y.cpu().numpy())
            y_pred_train.extend((outputs > 0.5).float().cpu().numpy())

        train_loss = total_train_loss / len(train_loader)
        train_losses.append(train_loss)

        # Calculate F1-score for training
        train_f1 = f1_score(y_true_train, y_pred_train)

        # Validation phase
        model.eval()
        total_val_loss = 0.0
        y_true_val, y_pred_val = [], []

        with torch.no_grad():
            for batch_X1, batch_X2, batch_X3, batch_X4, batch_y in val_loader:
                batch_X1, batch_X2, batch_X3, batch_X4, batch_y = (
                    batch_X1.to(device),
                    batch_X2.to(device),
                    batch_X3.to(device),
                    batch_X4.to(device),
                    batch_y.to(device),
                )
                outputs = model(batch_X1, batch_X2, batch_X3, batch_X4)

                # Compute loss
                val_loss = F.binary_cross_entropy(outputs, batch_y.unsqueeze(1).float())
                total_val_loss += val_loss.item()

                # Store true labels and predictions for F1-score
                y_true_val.extend(batch_y.cpu().numpy())
                y_pred_val.extend((outputs > 0.5).float().cpu().numpy())

        avg_val_loss = total_val_loss / len(val_loader)
        val_losses.append(avg_val_loss)

        # Calculate F1-score for validation
        val_f1 = f1_score(y_true_val, y_pred_val)

        # Step the scheduler with validation loss
        scheduler.step(avg_val_loss)

        # Print training and validation metrics
        print(f"Epoch [{epoch + 1}/{epochs}] Training Loss: {train_loss:.4f}, F1 Score: {train_f1:.4f} | "
              f"Validation Loss: {avg_val_loss:.4f}, F1 Score: {val_f1:.4f}")

    end_time = time.time()

    # Plotting training and validation loss
    plt.figure(figsize=(10, 5))
    plt.plot(train_losses, label="Training Loss", marker='o')
    plt.plot(val_losses, label="Validation Loss", marker='x')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    print(f"Training complete! Total time: {end_time - start_time:.2f} seconds")
    return model


```


```python
# Train the GatedCombination model using training and validation data
trained_model = train_gated_combination_model(
    X1_train,          # Updated source embeddings (after applying the GNN model)
    X2_train,          # Original source embeddings (before applying the GNN model)
    X3_train,          # Updated target embeddings (after applying the GNN model)
    X4_train,          # Original target embeddings (before applying the GNN model)
    tensor_score_train, # Ground truth labels for the training set (1 for matched pairs, 0 for unmatched pairs)

    X1_val,            # Updated source embeddings for the validation set
    X2_val,            # Original source embeddings for the validation set
    X3_val,            # Updated target embeddings for the validation set
    X4_val,            # Original target embeddings for the validation set
    tensor_score_val,  # Ground truth labels for the validation set (1 for matched pairs, 0 for unmatched pairs)

    epochs=100,        # Number of epochs (iterations over the entire training dataset)
    batch_size=32,     # Number of training samples processed in one forward/backward pass
    learning_rate=0.001, # Learning rate for the optimizer (controls step size during optimization)
    weight_decay=1e-4 # Weight decay (L2 regularization) to prevent overfitting
)
```

    Epoch [1/100] Training Loss: 0.0282, F1 Score: 0.0000 | Validation Loss: 0.0277, F1 Score: 0.0000
    Epoch [2/100] Training Loss: 0.0250, F1 Score: 0.0076 | Validation Loss: 0.0246, F1 Score: 0.0056
    Epoch [3/100] Training Loss: 0.0222, F1 Score: 0.0851 | Validation Loss: 0.0215, F1 Score: 0.1072
    Epoch [4/100] Training Loss: 0.0196, F1 Score: 0.2486 | Validation Loss: 0.0192, F1 Score: 0.2906
    Epoch [5/100] Training Loss: 0.0175, F1 Score: 0.3963 | Validation Loss: 0.0169, F1 Score: 0.4415
    Epoch [6/100] Training Loss: 0.0158, F1 Score: 0.4967 | Validation Loss: 0.0160, F1 Score: 0.4685
    Epoch [7/100] Training Loss: 0.0144, F1 Score: 0.5597 | Validation Loss: 0.0144, F1 Score: 0.6449
    Epoch [8/100] Training Loss: 0.0132, F1 Score: 0.6140 | Validation Loss: 0.0140, F1 Score: 0.6985
    Epoch [9/100] Training Loss: 0.0123, F1 Score: 0.6616 | Validation Loss: 0.0121, F1 Score: 0.6667
    Epoch [10/100] Training Loss: 0.0116, F1 Score: 0.6952 | Validation Loss: 0.0114, F1 Score: 0.6865
    Epoch [11/100] Training Loss: 0.0110, F1 Score: 0.7214 | Validation Loss: 0.0111, F1 Score: 0.7207
    Epoch [12/100] Training Loss: 0.0104, F1 Score: 0.7403 | Validation Loss: 0.0103, F1 Score: 0.7276
    Epoch [13/100] Training Loss: 0.0100, F1 Score: 0.7632 | Validation Loss: 0.0099, F1 Score: 0.7299
    Epoch [14/100] Training Loss: 0.0096, F1 Score: 0.7741 | Validation Loss: 0.0098, F1 Score: 0.7344
    Epoch [15/100] Training Loss: 0.0093, F1 Score: 0.7779 | Validation Loss: 0.0094, F1 Score: 0.7456
    Epoch [16/100] Training Loss: 0.0090, F1 Score: 0.7823 | Validation Loss: 0.0088, F1 Score: 0.7884
    Epoch [17/100] Training Loss: 0.0088, F1 Score: 0.7918 | Validation Loss: 0.0090, F1 Score: 0.8442
    Epoch [18/100] Training Loss: 0.0086, F1 Score: 0.8003 | Validation Loss: 0.0084, F1 Score: 0.8114
    Epoch [19/100] Training Loss: 0.0083, F1 Score: 0.8128 | Validation Loss: 0.0083, F1 Score: 0.8114
    Epoch [20/100] Training Loss: 0.0082, F1 Score: 0.8080 | Validation Loss: 0.0083, F1 Score: 0.8460
    Epoch [21/100] Training Loss: 0.0081, F1 Score: 0.8201 | Validation Loss: 0.0079, F1 Score: 0.8087
    Epoch [22/100] Training Loss: 0.0079, F1 Score: 0.8244 | Validation Loss: 0.0081, F1 Score: 0.8094
    Epoch [23/100] Training Loss: 0.0079, F1 Score: 0.8241 | Validation Loss: 0.0079, F1 Score: 0.7966
    Epoch [24/100] Training Loss: 0.0078, F1 Score: 0.8249 | Validation Loss: 0.0080, F1 Score: 0.8047
    Epoch [25/100] Training Loss: 0.0077, F1 Score: 0.8212 | Validation Loss: 0.0075, F1 Score: 0.8423
    Epoch [26/100] Training Loss: 0.0076, F1 Score: 0.8286 | Validation Loss: 0.0079, F1 Score: 0.8662
    Epoch [27/100] Training Loss: 0.0075, F1 Score: 0.8307 | Validation Loss: 0.0075, F1 Score: 0.8366
    Epoch [28/100] Training Loss: 0.0075, F1 Score: 0.8313 | Validation Loss: 0.0072, F1 Score: 0.8460
    Epoch [29/100] Training Loss: 0.0075, F1 Score: 0.8341 | Validation Loss: 0.0073, F1 Score: 0.8347
    Epoch [30/100] Training Loss: 0.0074, F1 Score: 0.8402 | Validation Loss: 0.0073, F1 Score: 0.8385
    Epoch [31/100] Training Loss: 0.0074, F1 Score: 0.8368 | Validation Loss: 0.0072, F1 Score: 0.8608
    Epoch [32/100] Training Loss: 0.0073, F1 Score: 0.8371 | Validation Loss: 0.0070, F1 Score: 0.8516
    Epoch [33/100] Training Loss: 0.0072, F1 Score: 0.8429 | Validation Loss: 0.0072, F1 Score: 0.8212
    Epoch [34/100] Training Loss: 0.0073, F1 Score: 0.8448 | Validation Loss: 0.0071, F1 Score: 0.8385
    Epoch [35/100] Training Loss: 0.0073, F1 Score: 0.8345 | Validation Loss: 0.0072, F1 Score: 0.8366
    Epoch [36/100] Training Loss: 0.0072, F1 Score: 0.8395 | Validation Loss: 0.0070, F1 Score: 0.8442
    Epoch [37/100] Training Loss: 0.0072, F1 Score: 0.8479 | Validation Loss: 0.0078, F1 Score: 0.8186
    Epoch [38/100] Training Loss: 0.0071, F1 Score: 0.8438 | Validation Loss: 0.0076, F1 Score: 0.8047
    Epoch [39/100] Training Loss: 0.0071, F1 Score: 0.8344 | Validation Loss: 0.0068, F1 Score: 0.8498
    Epoch [40/100] Training Loss: 0.0071, F1 Score: 0.8481 | Validation Loss: 0.0071, F1 Score: 0.8460
    Epoch [41/100] Training Loss: 0.0071, F1 Score: 0.8409 | Validation Loss: 0.0069, F1 Score: 0.8680
    Epoch [42/100] Training Loss: 0.0071, F1 Score: 0.8465 | Validation Loss: 0.0072, F1 Score: 0.8309
    Epoch [43/100] Training Loss: 0.0071, F1 Score: 0.8454 | Validation Loss: 0.0071, F1 Score: 0.8644
    Epoch [44/100] Training Loss: 0.0071, F1 Score: 0.8423 | Validation Loss: 0.0069, F1 Score: 0.8590
    Epoch [45/100] Training Loss: 0.0070, F1 Score: 0.8484 | Validation Loss: 0.0068, F1 Score: 0.8535
    Epoch [46/100] Training Loss: 0.0070, F1 Score: 0.8514 | Validation Loss: 0.0068, F1 Score: 0.8626
    Epoch [47/100] Training Loss: 0.0070, F1 Score: 0.8479 | Validation Loss: 0.0070, F1 Score: 0.8347
    Epoch [48/100] Training Loss: 0.0071, F1 Score: 0.8451 | Validation Loss: 0.0069, F1 Score: 0.8738
    Epoch [49/100] Training Loss: 0.0070, F1 Score: 0.8440 | Validation Loss: 0.0073, F1 Score: 0.8231
    Epoch [50/100] Training Loss: 0.0070, F1 Score: 0.8490 | Validation Loss: 0.0070, F1 Score: 0.8328
    Epoch [51/100] Training Loss: 0.0070, F1 Score: 0.8463 | Validation Loss: 0.0067, F1 Score: 0.8571
    Epoch [52/100] Training Loss: 0.0069, F1 Score: 0.8490 | Validation Loss: 0.0077, F1 Score: 0.8074
    Epoch [53/100] Training Loss: 0.0071, F1 Score: 0.8444 | Validation Loss: 0.0068, F1 Score: 0.8479
    Epoch [54/100] Training Loss: 0.0070, F1 Score: 0.8409 | Validation Loss: 0.0069, F1 Score: 0.8479
    Epoch [55/100] Training Loss: 0.0070, F1 Score: 0.8463 | Validation Loss: 0.0066, F1 Score: 0.8752
    Epoch [56/100] Training Loss: 0.0070, F1 Score: 0.8428 | Validation Loss: 0.0068, F1 Score: 0.8553
    Epoch [57/100] Training Loss: 0.0070, F1 Score: 0.8403 | Validation Loss: 0.0068, F1 Score: 0.8553
    Epoch [58/100] Training Loss: 0.0070, F1 Score: 0.8459 | Validation Loss: 0.0068, F1 Score: 0.8442
    Epoch [59/100] Training Loss: 0.0070, F1 Score: 0.8514 | Validation Loss: 0.0071, F1 Score: 0.8251
    Epoch [60/100] Training Loss: 0.0070, F1 Score: 0.8457 | Validation Loss: 0.0067, F1 Score: 0.8698
    Epoch [61/100] Training Loss: 0.0069, F1 Score: 0.8463 | Validation Loss: 0.0067, F1 Score: 0.8680
    Epoch [62/100] Training Loss: 0.0070, F1 Score: 0.8467 | Validation Loss: 0.0067, F1 Score: 0.8752
    Epoch [63/100] Training Loss: 0.0070, F1 Score: 0.8436 | Validation Loss: 0.0068, F1 Score: 0.8460
    Epoch [64/100] Training Loss: 0.0069, F1 Score: 0.8492 | Validation Loss: 0.0073, F1 Score: 0.8251
    Epoch [65/100] Training Loss: 0.0070, F1 Score: 0.8465 | Validation Loss: 0.0066, F1 Score: 0.8787
    Epoch [66/100] Training Loss: 0.0070, F1 Score: 0.8477 | Validation Loss: 0.0068, F1 Score: 0.8662
    Epoch [67/100] Training Loss: 0.0065, F1 Score: 0.8569 | Validation Loss: 0.0063, F1 Score: 0.8680
    Epoch [68/100] Training Loss: 0.0065, F1 Score: 0.8608 | Validation Loss: 0.0064, F1 Score: 0.8698
    Epoch [69/100] Training Loss: 0.0065, F1 Score: 0.8577 | Validation Loss: 0.0064, F1 Score: 0.8716
    Epoch [70/100] Training Loss: 0.0065, F1 Score: 0.8598 | Validation Loss: 0.0065, F1 Score: 0.8571
    Epoch [71/100] Training Loss: 0.0065, F1 Score: 0.8608 | Validation Loss: 0.0064, F1 Score: 0.8680
    Epoch [72/100] Training Loss: 0.0065, F1 Score: 0.8586 | Validation Loss: 0.0064, F1 Score: 0.8680
    Epoch [73/100] Training Loss: 0.0065, F1 Score: 0.8588 | Validation Loss: 0.0065, F1 Score: 0.8716
    Epoch [74/100] Training Loss: 0.0065, F1 Score: 0.8584 | Validation Loss: 0.0064, F1 Score: 0.8805
    Epoch [75/100] Training Loss: 0.0065, F1 Score: 0.8577 | Validation Loss: 0.0064, F1 Score: 0.8571
    Epoch [76/100] Training Loss: 0.0065, F1 Score: 0.8557 | Validation Loss: 0.0064, F1 Score: 0.8716
    Epoch [77/100] Training Loss: 0.0065, F1 Score: 0.8592 | Validation Loss: 0.0064, F1 Score: 0.8734
    Epoch [78/100] Training Loss: 0.0065, F1 Score: 0.8569 | Validation Loss: 0.0064, F1 Score: 0.8716
    Epoch [79/100] Training Loss: 0.0064, F1 Score: 0.8630 | Validation Loss: 0.0064, F1 Score: 0.8680
    Epoch [80/100] Training Loss: 0.0064, F1 Score: 0.8614 | Validation Loss: 0.0064, F1 Score: 0.8698
    Epoch [81/100] Training Loss: 0.0064, F1 Score: 0.8592 | Validation Loss: 0.0064, F1 Score: 0.8698
    Epoch [82/100] Training Loss: 0.0064, F1 Score: 0.8616 | Validation Loss: 0.0064, F1 Score: 0.8698
    Epoch [83/100] Training Loss: 0.0064, F1 Score: 0.8608 | Validation Loss: 0.0064, F1 Score: 0.8698
    Epoch [84/100] Training Loss: 0.0064, F1 Score: 0.8600 | Validation Loss: 0.0064, F1 Score: 0.8698
    Epoch [85/100] Training Loss: 0.0064, F1 Score: 0.8614 | Validation Loss: 0.0064, F1 Score: 0.8698
    Epoch [86/100] Training Loss: 0.0064, F1 Score: 0.8608 | Validation Loss: 0.0064, F1 Score: 0.8698
    Epoch [87/100] Training Loss: 0.0064, F1 Score: 0.8614 | Validation Loss: 0.0064, F1 Score: 0.8698
    Epoch [88/100] Training Loss: 0.0064, F1 Score: 0.8614 | Validation Loss: 0.0064, F1 Score: 0.8698
    Epoch [89/100] Training Loss: 0.0064, F1 Score: 0.8598 | Validation Loss: 0.0064, F1 Score: 0.8698
    Epoch [90/100] Training Loss: 0.0064, F1 Score: 0.8616 | Validation Loss: 0.0064, F1 Score: 0.8698
    Epoch [91/100] Training Loss: 0.0064, F1 Score: 0.8616 | Validation Loss: 0.0064, F1 Score: 0.8698
    Epoch [92/100] Training Loss: 0.0064, F1 Score: 0.8616 | Validation Loss: 0.0064, F1 Score: 0.8698
    Epoch [93/100] Training Loss: 0.0064, F1 Score: 0.8616 | Validation Loss: 0.0064, F1 Score: 0.8698
    Epoch [94/100] Training Loss: 0.0064, F1 Score: 0.8616 | Validation Loss: 0.0064, F1 Score: 0.8698
    Epoch [95/100] Training Loss: 0.0064, F1 Score: 0.8616 | Validation Loss: 0.0064, F1 Score: 0.8698
    Epoch [96/100] Training Loss: 0.0064, F1 Score: 0.8616 | Validation Loss: 0.0064, F1 Score: 0.8698
    Epoch [97/100] Training Loss: 0.0064, F1 Score: 0.8616 | Validation Loss: 0.0064, F1 Score: 0.8698
    Epoch [98/100] Training Loss: 0.0064, F1 Score: 0.8616 | Validation Loss: 0.0064, F1 Score: 0.8698
    Epoch [99/100] Training Loss: 0.0064, F1 Score: 0.8616 | Validation Loss: 0.0064, F1 Score: 0.8698
    Epoch [100/100] Training Loss: 0.0064, F1 Score: 0.8616 | Validation Loss: 0.0064, F1 Score: 0.8698



    
![png](output_45_1.png)
    


    Training complete! Total time: 1337.70 seconds


# **Second Round Modifications**

# **Generate Embeddings**


```python
def save_gated_embeddings(gated_model, embeddings_src, x_src, embeddings_tgt, x_tgt,
                          indexed_dict_src, indexed_dict_tgt,
                          output_file_src, output_file_tgt):
    """
    Compute and save the final entity embeddings generated by the GatedCombination model
    for both source and target ontologies. Outputs include entity URIs and their final vectors.
    Measures and prints the execution time of the entire operation.

    Args:
        gated_model (nn.Module): The trained GatedCombination model.
        embeddings_src (Tensor): Structural embeddings for the source ontology.
        x_src (Tensor): Semantic embeddings for the source ontology.
        embeddings_tgt (Tensor): Structural embeddings for the target ontology.
        x_tgt (Tensor): Semantic embeddings for the target ontology.
        indexed_dict_src (dict): Index-to-URI mapping for the source ontology.
        indexed_dict_tgt (dict): Index-to-URI mapping for the target ontology.
        output_file_src (str): Path to save source embeddings (TSV).
        output_file_tgt (str): Path to save target embeddings (TSV).
    """
    import pandas as pd
    import torch
    import time

    start_time = time.time()

    # Use GPU if available
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    gated_model = gated_model.to(device)
    gated_model.eval()

    # Move inputs to the same device
    embeddings_src = embeddings_src.to(device)
    x_src = x_src.to(device)
    embeddings_tgt = embeddings_tgt.to(device)
    x_tgt = x_tgt.to(device)

    with torch.no_grad():
        # === Source ontology ===
        gate_src = torch.sigmoid(gated_model.gate_A_fc(embeddings_src))
        final_src = embeddings_src * gate_src + x_src * (1 - gate_src)
        final_src = final_src.cpu().numpy()

        # === Target ontology ===
        gate_tgt = torch.sigmoid(gated_model.gate_B_fc(embeddings_tgt))
        final_tgt = embeddings_tgt * gate_tgt + x_tgt * (1 - gate_tgt)
        final_tgt = final_tgt.cpu().numpy()

    # Create DataFrames with Concept URI and embedding values
    df_src = pd.DataFrame(final_src)
    df_src.insert(0, "Concept", [indexed_dict_src[i] for i in range(len(df_src))])

    df_tgt = pd.DataFrame(final_tgt)
    df_tgt.insert(0, "Concept", [indexed_dict_tgt[i] for i in range(len(df_tgt))])

    # Save embeddings to file
    df_src.to_csv(output_file_src, sep='\t', index=False)
    df_tgt.to_csv(output_file_tgt, sep='\t', index=False)

    elapsed_time = time.time() - start_time
    print(f"✅ Gated embeddings saved:\n- Source: {output_file_src}\n- Target: {output_file_tgt}")
    print(f"⏱️ Execution time: {elapsed_time:.2f} seconds")

```


```python
# Build an indexed dictionary for the source ontology classes
# src_class is the file path to the JSON file containing the source ontology classes
indexed_dict_src = build_indexed_dict(src_class)

# Build an indexed dictionary for the target ontology classes
# tgt_class is the file path to the JSON file containing the target ontology classes
indexed_dict_tgt = build_indexed_dict(tgt_class)
```


```python
# Define output file paths for final embeddings of source and target ontologies
output_file_src = f"{data_dir}/{src_ent}_final_embeddings.tsv"
output_file_tgt = f"{data_dir}/{tgt_ent}_final_embeddings.tsv"

# Save the final gated embeddings for all concepts in source and target ontologies
save_gated_embeddings(
    gated_model=trained_model,          # The trained GatedCombination model
    embeddings_src=embeddings_src,      # GNN-transformed embeddings for source entities
    x_src=x_src,                        # Initial semantic embeddings for source entities
    embeddings_tgt=embeddings_tgt,      # GNN-transformed embeddings for target entities
    x_tgt=x_tgt,                        # Initial semantic embeddings for target entities
    indexed_dict_src=indexed_dict_src,  # Index-to-URI mapping for source ontology
    indexed_dict_tgt=indexed_dict_tgt,  # Index-to-URI mapping for target ontology
    output_file_src=output_file_src,    # Destination file path for source embeddings
    output_file_tgt=output_file_tgt     # Destination file path for target embeddings
)

```

    ✅ Gated embeddings saved:
    - Source: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings.tsv
    - Target: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings.tsv
    ⏱️ Execution time: 53.45 seconds


# **Filter No Used Concepts**





```python
import pandas as pd

def filter_ignored_class(src_emb_path, tgt_emb_path, src_onto, tgt_onto):
    """
    Filters the source and target embedding files by removing concepts considered "ignored classes"
    (e.g., owl:Thing, deprecated entities, etc.) based on both source and target ontologies.

    Args:
        src_emb_path (str): Path to the TSV file containing source embeddings with 'Concept' column.
        tgt_emb_path (str): Path to the TSV file containing target embeddings with 'Concept' column.
        src_onto (Ontology): Source ontology object loaded with DeepOnto.
        tgt_onto (Ontology): Target ontology object loaded with DeepOnto.

    Returns:
        (str, str): Paths to the cleaned source and target embedding files.
    """

    # === Load the embedding files ===
    df_src = pd.read_csv(src_emb_path, sep='\t', dtype=str)
    print(f"🔍 Initial source file: {len(df_src)} rows")

    df_tgt = pd.read_csv(tgt_emb_path, sep='\t', dtype=str)
    print(f"🔍 Initial target file: {len(df_tgt)} rows")

    # === Step 1: Retrieve ignored classes from both ontologies ===
    ignored_class_index = get_ignored_class_index(src_onto)  # e.g., owl:Thing, non-usable classes
    ignored_class_index.update(get_ignored_class_index(tgt_onto))  # Merge with target ontology's ignored classes
    ignored_uris = set(str(uri).strip() for uri in ignored_class_index)

    # === Step 2: Remove rows where the 'Concept' column matches ignored URIs ===
    df_src_cleaned = df_src[~df_src['Concept'].isin(ignored_uris)].reset_index(drop=True)
    df_tgt_cleaned = df_tgt[~df_tgt['Concept'].isin(ignored_uris)].reset_index(drop=True)

    print(f"✅ Source after removing ignored classes: {len(df_src_cleaned)} rows")
    print(f"✅ Target after removing ignored classes: {len(df_tgt_cleaned)} rows")

    # === Step 3: Save the cleaned embedding files ===
    output_file_src = src_emb_path.replace(".tsv", "_ignored_class_cleaned.tsv")
    output_file_tgt = tgt_emb_path.replace(".tsv", "_ignored_class_cleaned.tsv")

    df_src_cleaned.to_csv(output_file_src, sep='\t', index=False)
    df_tgt_cleaned.to_csv(output_file_tgt, sep='\t', index=False)

    print(f"📁 Cleaned source file saved to: {output_file_src}")
    print(f"📁 Cleaned target file saved to: {output_file_tgt}")

    return output_file_src, output_file_tgt
```


```python
# Call the function to filter out ignored concepts (e.g., owl:Thing, deprecated, etc.)
# from the source and target ontology embeddings.

# Input:
# - src_emb_path: Path to the TSV file containing embeddings for the source ontology
# - tgt_emb_path: Path to the TSV file containing embeddings for the target ontology
# - src_onto / tgt_onto: DeepOnto ontology objects used to identify ignored concepts

# Output:
# - src_file: Path to the cleaned source embeddings (with ignored concepts removed)
# - tgt_file: Path to the cleaned target embeddings (with ignored concepts removed)

src_file, tgt_file = filter_ignored_class(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings.tsv",
    src_onto=src_onto,
    tgt_onto=tgt_onto

)

```

    🔍 Initial source file: 23107 rows
    🔍 Initial target file: 20498 rows
    ✅ Source after removing ignored classes: 11407 rows
    ✅ Target after removing ignored classes: 14207 rows
    📁 Cleaned source file saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned.tsv
    📁 Cleaned target file saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned.tsv


# **Encoders' Definitions**


```python
!pip install optuna  # Install the Optuna library, a powerful framework for hyperparameter optimization using efficient search algorithms (e.g., TPE, CMA-ES, etc.)
```

    Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)
    Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.2)
    Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)
    Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.2.6)
    Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)
    Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)
    Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)
    Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)
    Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)
    Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)
    Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)



```python
# === Data Manipulation and Warnings ===
import pandas as pd               # For handling tabular data
import numpy as np                # For numerical operations on arrays
import warnings, gc               # To suppress warnings and manage garbage collection
warnings.filterwarnings('ignore') # Ignore all warnings during execution to keep output clean

# === PyTorch and GNN Modules ===
import torch                      # Core PyTorch library for tensor operations and models
from torch import optim, Tensor   # Optimizers and Tensor object
import torch.nn as nn             # Base class for neural network modules
import torch.nn.functional as F   # Functional interface for neural network layers and loss functions

# PyTorch Geometric modules for graph neural networks
from torch_geometric.nn.conv import MessagePassing  # Base class for implementing GNN layers
from torch_geometric.data import Data               # Container for graph data
from torch_geometric.loader import DataLoader       # DataLoader for batched graph processing

# === Scikit-learn Modules ===
from sklearn.preprocessing import LabelEncoder      # Encode categorical labels as integers
from sklearn.model_selection import train_test_split # Split data into training and test sets
from sklearn.metrics import mean_squared_error, mean_absolute_error, precision_score, accuracy_score
# Common metrics for evaluating regression and classification performance

# === Hyperparameter Optimization ===
import optuna                     # For automated hyperparameter tuning using Bayesian optimization

```


```python
import torch.nn as nn

# === Simple Linear Encoder ===
class LinearEncoder(nn.Module):
    def __init__(self, embedding_dim):
        super(LinearEncoder, self).__init__()
        # A single linear transformation layer: y = Wx + b
        self.linear = nn.Linear(embedding_dim, embedding_dim)

    def forward(self, x):
        # Forward pass: apply the linear transformation
        return self.linear(x)

# === Multi-Layer Perceptron (MLP) Encoder ===
class MLPEncoder(nn.Module):
    def __init__(self, embedding_dim):
        super(MLPEncoder, self).__init__()
        # A 2-layer MLP with ReLU activation
        self.net = nn.Sequential(
            nn.Linear(embedding_dim, embedding_dim),
            nn.ReLU(),
            nn.Linear(embedding_dim, embedding_dim)
        )

    def forward(self, x):
        # Forward pass through the MLP
        return self.net(x)

# === Residual MLP Encoder ===
class ResMLPEncoder(nn.Module):
    def __init__(self, embedding_dim):
        super(ResMLPEncoder, self).__init__()
        # Two linear layers with ReLU activation
        self.linear1 = nn.Linear(embedding_dim, embedding_dim)
        self.relu = nn.ReLU()
        self.linear2 = nn.Linear(embedding_dim, embedding_dim)

    def forward(self, x):
        # Residual connection: output = x + F(x)
        out = self.linear1(x)
        out = self.relu(out)
        out = self.linear2(out)
        return x + out

# === Transformer-based Encoder ===
class TransformerEncoder(nn.Module):
    def __init__(self, embedding_dim, nhead=4, num_layers=1):
        super(TransformerEncoder, self).__init__()
        # Define a Transformer encoder layer with multi-head attention
        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=nhead)
        # Stack the encoder layer `num_layers` times
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

    def forward(self, x):
        # Add sequence dimension (required by PyTorch Transformer)
        x = x.unsqueeze(1)             # [batch_size, 1, embedding_dim]
        x = x.transpose(0, 1)          # [1, batch_size, embedding_dim] (seq_len first)
        x = self.transformer_encoder(x) # Pass through transformer
        x = x.transpose(0, 1).squeeze(1) # Remove added dimensions
        return x
```


```python
import pandas as pd
import torch

def encode_embeddings_with_concept_column(encoder_model, input_file, output_file):
    """
    Applies an encoder model to a set of embeddings (while preserving the 'Concept' column),
    and saves the encoded results in the same tabular format.

    Args:
        encoder_model: A PyTorch model (e.g., LinearEncoder, MLPEncoder, etc.)
        input_file (str): Path to the input TSV file containing 'Concept' and embedding vectors.
        output_file (str): Path to save the encoded embeddings.
    """

    # Select device (GPU if available, else CPU)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Move the encoder model to the selected device and set it to evaluation mode
    encoder_model = encoder_model.to(device)
    encoder_model.eval()

    # Load the input TSV file containing concept URIs and embeddings
    df = pd.read_csv(input_file, sep='\t')

    # Extract the 'Concept' column to preserve URIs
    concepts = df['Concept'].tolist()

    # Extract the numerical embedding values (excluding the 'Concept' column)
    embedding_values = df.drop(columns=['Concept']).values

    # Convert the embedding matrix into a PyTorch tensor and move to the device
    embeddings = torch.FloatTensor(embedding_values).to(device)

    # Pass the embeddings through the encoder model without computing gradients
    with torch.no_grad():
        encoded = encoder_model(embeddings).cpu().numpy()

    # Reconstruct a new DataFrame with the encoded vectors and corresponding URIs
    df_encoded = pd.DataFrame(encoded, columns=[f'dim_{i}' for i in range(encoded.shape[1])])
    df_encoded.insert(0, "Concept", concepts)  # Re-insert the 'Concept' column at the first position

    # Save the encoded embeddings to a TSV file
    df_encoded.to_csv(output_file, sep='\t', index=False)
    print(f"✅ Encoded embeddings saved to: {output_file}")

```

# **Similarity Metrics**

# **Euclidean Distance**


```python
import pandas as pd
import numpy as np
from sklearn.metrics import pairwise_distances
import time

def topk_euclidean_with_uris(src_emb_path, tgt_emb_path, top_k=15, output_file="topk_mappings_euclidean_final.tsv"):
    start_time = time.time()  # ⏱️ Début du chronométrage

    # Load source and target embedding files (each with a 'Concept' column and embedding columns)
    df_src = pd.read_csv(src_emb_path, sep='\t')
    df_tgt = pd.read_csv(tgt_emb_path, sep='\t')

    # Extract concept URIs from the 'Concept' column
    uris_src = df_src["Concept"].values
    uris_tgt = df_tgt["Concept"].values

    # Extract embedding vectors as numpy arrays (drop the 'Concept' column)
    src_vecs = df_src.drop(columns=["Concept"]).values
    tgt_vecs = df_tgt.drop(columns=["Concept"]).values

    # Compute Euclidean (L2) distance between all source and target vectors
    dist_matrix = pairwise_distances(src_vecs, tgt_vecs, metric='euclidean')

    # Convert distance to similarity: the smaller the distance, the higher the similarity
    sim_matrix = 1 / (1 + dist_matrix)

    # For each source entity, find the indices of the top-k most similar target entities
    topk_indices = np.argsort(-sim_matrix, axis=1)[:, :top_k]  # sort by similarity descending
    topk_scores = np.take_along_axis(sim_matrix, topk_indices, axis=1)  # retrieve top-k similarity scores

    # Construct the output DataFrame with URIs and similarity scores
    rows = []
    for i, (indices, scores) in enumerate(zip(topk_indices, topk_scores)):
        src_uri = uris_src[i]
        for tgt_idx, score in zip(indices, scores):
            tgt_uri = uris_tgt[tgt_idx]
            rows.append((src_uri, tgt_uri, score))  # store the source, target, and score

    # Create a DataFrame and save the results to a TSV file
    df_result = pd.DataFrame(rows, columns=["SrcEntity", "TgtEntity", "Score"])
    df_result.to_csv(output_file, sep='\t', index=False)

    # ⏱️ Fin du chronométrage
    elapsed_time = time.time() - start_time
    print(f"✅ Top-{top_k} Euclidean similarity results saved to: {output_file}")
    print(f"⏱️ Execution time: {elapsed_time:.2f} seconds")


```

# **FAISS Similarity**


```python
# Install the FAISS library for CPU-based approximate nearest neighbor search
!pip install faiss-cpu

# Alternatively, install the FAISS library optimized for GPU acceleration (if CUDA is available)
!pip install faiss-gpu
```

    Collecting faiss-cpu
      Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)
    Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.2.6)
    Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)
    Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m31.3/31.3 MB[0m [31m53.7 MB/s[0m eta [36m0:00:00[0m
    [?25hInstalling collected packages: faiss-cpu
    Successfully installed faiss-cpu-1.11.0
    [31mERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)[0m[31m
    [0m[31mERROR: No matching distribution found for faiss-gpu[0m[31m
    [0m


```python
import pandas as pd
import numpy as np
import faiss
import time

def load_embeddings(src_emb_path, tgt_emb_path):
    df_src = pd.read_csv(src_emb_path, sep='\t')
    df_tgt = pd.read_csv(tgt_emb_path, sep='\t')
    uris_src = df_src["Concept"].values
    uris_tgt = df_tgt["Concept"].values
    src_vecs = df_src.drop(columns=["Concept"]).values.astype('float32')
    tgt_vecs = df_tgt.drop(columns=["Concept"]).values.astype('float32')
    return uris_src, uris_tgt, src_vecs, tgt_vecs

def save_results(uris_src, uris_tgt, indices, scores, output_file, top_k):
    rows = []
    for i, (ind_row, score_row) in enumerate(zip(indices, scores)):
        src_uri = uris_src[i]
        for j, tgt_idx in enumerate(ind_row):
            tgt_uri = uris_tgt[tgt_idx]
            score = score_row[j]
            rows.append((src_uri, tgt_uri, score))
    df_result = pd.DataFrame(rows, columns=["SrcEntity", "TgtEntity", "Score"])
    df_result.to_csv(output_file, sep='\t', index=False)
    print(f"Top-{top_k} FAISS similarity results saved to: {output_file}")

def topk_faiss_l2(src_emb_path, tgt_emb_path, top_k=15, output_file="topk_l2.tsv"):
    print("🔹 Using L2 (Euclidean) distance with FAISS")
    start = time.time()

    uris_src, uris_tgt, src_vecs, tgt_vecs = load_embeddings(src_emb_path, tgt_emb_path)
    dim = src_vecs.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(tgt_vecs)
    distances, indices = index.search(src_vecs, top_k)
    similarity_scores = 1 / (1 + distances)

    save_results(uris_src, uris_tgt, indices, similarity_scores, output_file, top_k)

    print(f"⏱️ Execution time: {time.time() - start:.2f} seconds")

def topk_faiss_inner_product(src_emb_path, tgt_emb_path, top_k=15, output_file="topk_ip.tsv"):
    print("🔹 Using Inner Product (dot product) with FAISS")
    start = time.time()

    uris_src, uris_tgt, src_vecs, tgt_vecs = load_embeddings(src_emb_path, tgt_emb_path)
    dim = src_vecs.shape[1]
    index = faiss.IndexFlatIP(dim)
    index.add(tgt_vecs)
    indices_scores, indices = index.search(src_vecs, top_k)

    save_results(uris_src, uris_tgt, indices, indices_scores, output_file, top_k)

    print(f"⏱️ Execution time: {time.time() - start:.2f} seconds")

def topk_faiss_cosine(src_emb_path, tgt_emb_path, top_k=15, output_file="topk_mappings_faiss_cosine.tsv"):
    df_src = pd.read_csv(src_emb_path, sep='\t')
    df_tgt = pd.read_csv(tgt_emb_path, sep='\t')

    uris_src = df_src["Concept"].values
    uris_tgt = df_tgt["Concept"].values

    src_vecs = df_src.drop(columns=["Concept"]).values.astype('float32')
    tgt_vecs = df_tgt.drop(columns=["Concept"]).values.astype('float32')

    # Normalize for cosine similarity
    src_vecs = src_vecs / np.linalg.norm(src_vecs, axis=1, keepdims=True)
    tgt_vecs = tgt_vecs / np.linalg.norm(tgt_vecs, axis=1, keepdims=True)

    # Ensure arrays are C-contiguous
    src_vecs = np.ascontiguousarray(src_vecs)
    tgt_vecs = np.ascontiguousarray(tgt_vecs)

    # Build FAISS index
    index = faiss.IndexFlatIP(tgt_vecs.shape[1])  # inner product == cosine similarity if vectors are normalized
    index.add(tgt_vecs)

    # Perform search
    scores, indices = index.search(src_vecs, top_k)

    # Save results
    with open(output_file, "w") as f:
        f.write("SrcEntity\tTgtEntity\tScore\n")
        for i, (row_scores, row_indices) in enumerate(zip(scores, indices)):
            for score, idx in zip(row_scores, row_indices):
                f.write(f"{uris_src[i]}\t{uris_tgt[idx]}\t{score:.6f}\n")

    print(f"✅ Top-{top_k} cosine similarity results saved to: {output_file}")
```

# **ANNOY Similarity**


```python
# Install the Annoy library for approximate nearest neighbor search using tree-based indexing (good for fast read operations)
!pip install annoy
```

    Collecting annoy
      Downloading annoy-1.17.3.tar.gz (647 kB)
    [?25l     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/647.5 kB[0m [31m?[0m eta [36m-:--:--[0m[2K     [91m━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━[0m [32m256.0/647.5 kB[0m [31m7.4 MB/s[0m eta [36m0:00:01[0m[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m647.5/647.5 kB[0m [31m10.0 MB/s[0m eta [36m0:00:00[0m
    [?25h  Installing build dependencies ... [?25l[?25hdone
      Getting requirements to build wheel ... [?25l[?25hdone
      Installing backend dependencies ... [?25l[?25hdone
      Preparing metadata (pyproject.toml) ... [?25l[?25hdone
    Building wheels for collected packages: annoy
      Building wheel for annoy (pyproject.toml) ... [?25l[?25hdone
      Created wheel for annoy: filename=annoy-1.17.3-cp311-cp311-linux_x86_64.whl size=551738 sha256=91ee3563c6d2455c9eca79578df55fa93ad81c07dcaebb466299b1cd2ec93f80
      Stored in directory: /root/.cache/pip/wheels/33/e5/58/0a3e34b92bedf09b4c57e37a63ff395ade6f6c1099ba59877c
    Successfully built annoy
    Installing collected packages: annoy
    Successfully installed annoy-1.17.3



```python
import pandas as pd
import numpy as np
from annoy import AnnoyIndex
import time

def load_embeddings_annoy(src_emb_path, tgt_emb_path):
    df_src = pd.read_csv(src_emb_path, sep='\t')
    df_tgt = pd.read_csv(tgt_emb_path, sep='\t')
    uris_src = df_src["Concept"].values
    uris_tgt = df_tgt["Concept"].values
    src_vecs = df_src.drop(columns=["Concept"]).values.astype("float32")
    tgt_vecs = df_tgt.drop(columns=["Concept"]).values.astype("float32")
    return uris_src, uris_tgt, src_vecs, tgt_vecs

def save_annoy_results(uris_src, uris_tgt, all_results, output_file, top_k):
    df_result = pd.DataFrame(all_results, columns=["SrcEntity", "TgtEntity", "Score"])
    df_result.to_csv(output_file, sep='\t', index=False)
    print(f"Top-{top_k} Annoy similarity results saved to: {output_file}")

def topk_annoy(src_emb_path, tgt_emb_path, top_k=15, output_file="topk_annoy.tsv", metric='euclidean'):
    print(f"🔹 Using Annoy with metric: {metric}")
    start = time.time()

    uris_src, uris_tgt, src_vecs, tgt_vecs = load_embeddings_annoy(src_emb_path, tgt_emb_path)
    dim = tgt_vecs.shape[1]
    index = AnnoyIndex(dim, metric)

    for i, vec in enumerate(tgt_vecs):
        index.add_item(i, vec)

    index.build(n_trees=10)

    results = []
    for i, src_vec in enumerate(src_vecs):
        idxs, dists = index.get_nns_by_vector(src_vec, top_k, include_distances=True)
        for j, (idx, dist) in enumerate(zip(idxs, dists)):
            score = 1 / (1 + dist)  # Convert distance to similarity
            results.append((uris_src[i], uris_tgt[idx], score))

    save_annoy_results(uris_src, uris_tgt, results, output_file, top_k)
    print(f"⏱️ Execution time: {time.time() - start:.2f} seconds")

# Wrapper for Euclidean
def topk_annoy_euclidean(src_emb_path, tgt_emb_path, top_k=15, output_file="topk_annoy_euclidean.tsv"):
    topk_annoy(src_emb_path, tgt_emb_path, top_k, output_file, metric='euclidean')

# Wrapper for Cosine (approximate, via angular)
def topk_annoy_cosine(src_emb_path, tgt_emb_path, top_k=15, output_file="topk_annoy_cosine.tsv"):
    topk_annoy(src_emb_path, tgt_emb_path, top_k, output_file, metric='angular')

```

# **DIEM Similarity**


```python
import pandas as pd
import numpy as np
import time

def compute_diem_matrix(A, B, eps=1e-8):
    """
    Compute DIEM distance matrix between A and B:
    diem(a, b) = ||a - b||^2 / (||a|| * ||b||)
    """
    norm_A = np.linalg.norm(A, axis=1).reshape(-1, 1)
    norm_B = np.linalg.norm(B, axis=1).reshape(1, -1)

    dot_prod = np.dot(A, B.T)

    a_sq = np.sum(A**2, axis=1).reshape(-1, 1)
    b_sq = np.sum(B**2, axis=1).reshape(1, -1)
    l2_squared = a_sq + b_sq - 2 * dot_prod

    denom = norm_A * norm_B + eps
    diem = l2_squared / denom
    return diem

def topk_diem(src_emb_path, tgt_emb_path, top_k=15, output_file="topk_mappings_diem.tsv"):
    print("🔹 Using DIEM similarity measure")
    start_time = time.time()

    # Load embeddings
    df_src = pd.read_csv(src_emb_path, sep='\t')
    df_tgt = pd.read_csv(tgt_emb_path, sep='\t')

    uris_src = df_src["Concept"].values
    uris_tgt = df_tgt["Concept"].values
    src_vecs = df_src.drop(columns=["Concept"]).values.astype('float32')
    tgt_vecs = df_tgt.drop(columns=["Concept"]).values.astype('float32')

    # Compute DIEM similarity matrix
    diem_matrix = compute_diem_matrix(src_vecs, tgt_vecs)
    sim_matrix = 1 / (1 + diem_matrix)

    # Select top-k most similar target concepts
    topk_indices = np.argsort(-sim_matrix, axis=1)[:, :top_k]
    topk_scores = np.take_along_axis(sim_matrix, topk_indices, axis=1)

    rows = []
    for i, (indices, scores) in enumerate(zip(topk_indices, topk_scores)):
        src_uri = uris_src[i]
        for tgt_idx, score in zip(indices, scores):
            tgt_uri = uris_tgt[tgt_idx]
            rows.append((src_uri, tgt_uri, score))

    df_result = pd.DataFrame(rows, columns=["SrcEntity", "TgtEntity", "Score"])
    df_result.to_csv(output_file, sep='\t', index=False)
    print(f"Top-{top_k} DIEM similarity results saved to: {output_file}")

    end_time = time.time()
    print(f"⏱️ Execution time: {end_time - start_time:.2f} seconds")
```

# **Cosine From Transformer**


```python
# Install the Sentence Transformers library, which provides pre-trained models like BERT, RoBERTa, and SapBERT for computing dense vector representations of text (suitable for semantic similarity tasks)
!pip install sentence-transformers
```

    Collecting sentence-transformers
      Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)
    Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)
    Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)
    Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0)
    Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)
    Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)
    Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.31.2)
    Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)
    Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.13.2)
    Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)
    Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)
    Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)
    Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)
    Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)
    Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)
    Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)
    Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)
    Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)
    Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)
    Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)
    Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)
    Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)
    Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)
    Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)
    Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)
    Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)
    Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)
    Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)
    Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)
    Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)
    Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)
    Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)
    Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.6)
    Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)
    Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)
    Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)
    Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.0)
    Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)
    Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)
    Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)
    Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)
    Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)
    Downloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m345.7/345.7 kB[0m [31m7.0 MB/s[0m eta [36m0:00:00[0m
    [?25hInstalling collected packages: sentence-transformers
    Successfully installed sentence-transformers-4.1.0



```python
import pandas as pd
import torch
import time
from sentence_transformers import util

def topk_cosine(src_emb_path, tgt_emb_path, top_k=15, output_file="topk_mappings_cosine.tsv"):
    print("🔹 Using Cosine Similarity (Sentence-Transformers)")
    start_time = time.time()

    # Load embeddings with URIs
    df_src = pd.read_csv(src_emb_path, sep='\t')
    df_tgt = pd.read_csv(tgt_emb_path, sep='\t')

    uris_src = df_src["Concept"].values
    uris_tgt = df_tgt["Concept"].values

    src_vecs = torch.tensor(df_src.drop(columns=["Concept"]).values, dtype=torch.float32)
    tgt_vecs = torch.tensor(df_tgt.drop(columns=["Concept"]).values, dtype=torch.float32)

    # Normalize vectors for cosine similarity
    src_vecs = torch.nn.functional.normalize(src_vecs, p=2, dim=1)
    tgt_vecs = torch.nn.functional.normalize(tgt_vecs, p=2, dim=1)

    # Compute cosine similarity matrix
    sim_matrix = util.cos_sim(src_vecs, tgt_vecs)  # shape: [src x tgt]

    # Top-k per row
    top_k_scores, top_k_indices = torch.topk(sim_matrix, k=top_k, dim=1)

    rows = []
    for i, (indices_row, scores_row) in enumerate(zip(top_k_indices, top_k_scores)):
        src_uri = uris_src[i]
        for j in range(top_k):
            tgt_uri = uris_tgt[indices_row[j]]
            score = scores_row[j].item()
            rows.append((src_uri, tgt_uri, score))

    # Save results
    df_result = pd.DataFrame(rows, columns=["SrcEntity", "TgtEntity", "Score"])
    df_result.to_csv(output_file, sep='\t', index=False)
    print(f"Top-{top_k} cosine similarity results saved to: {output_file}")

    print(f"⏱️ Execution time: {time.time() - start_time:.2f} seconds")
```


```python
import pandas as pd
import numpy as np
import torch
import time

def topk_sharpened_cosine_with_uris(
    src_emb_path, tgt_emb_path, top_k=15, output_file="topk_mappings_sharpened_cosine.tsv", sharpening_power=2
):
    start_time = time.time()  # Chronométrage début

    # Chargement des embeddings source et target
    df_src = pd.read_csv(src_emb_path, sep='\t')
    df_tgt = pd.read_csv(tgt_emb_path, sep='\t')

    uris_src = df_src["Concept"].values
    uris_tgt = df_tgt["Concept"].values

    # Conversion des vecteurs en tenseurs torch
    src_vecs = torch.tensor(df_src.drop(columns=["Concept"]).values, dtype=torch.float32)
    tgt_vecs = torch.tensor(df_tgt.drop(columns=["Concept"]).values, dtype=torch.float32)

    # Normalisation des vecteurs pour produit scalaire = cosinus
    src_vecs = torch.nn.functional.normalize(src_vecs, p=2, dim=1)
    tgt_vecs = torch.nn.functional.normalize(tgt_vecs, p=2, dim=1)

    # Calcul des similarités cosinus
    sim_matrix = torch.matmul(src_vecs, tgt_vecs.T)

    # Appliquer la mise au carré pour sharpened cosine
    sim_matrix = sim_matrix ** sharpening_power

    # Sélection des top-k indices et scores
    topk_scores, topk_indices = torch.topk(sim_matrix, k=top_k, dim=1)

    # Construction des résultats
    rows = []
    for i, (indices, scores) in enumerate(zip(topk_indices, topk_scores)):
        src_uri = uris_src[i]
        for tgt_idx, score in zip(indices.tolist(), scores.tolist()):
            tgt_uri = uris_tgt[tgt_idx]
            rows.append((src_uri, tgt_uri, float(score)))

    df_result = pd.DataFrame(rows, columns=["SrcEntity", "TgtEntity", "Score"])
    df_result.to_csv(output_file, sep='\t', index=False)

    print(f"Top-{top_k} sharpened cosine similarity results saved to: {output_file}")
    print(f"⏱️ Execution time: {time.time() - start_time:.2f} seconds")

```

# **Mappings Evaluation Functions**

# **Precision, Recall, F1**


```python
def select_best_candidates_per_src_with_margin(df, score_margin=0.01):
    """
    For each SrcEntity, retain all candidate mappings whose similarity score is
    within 99% of the best score (default margin = 0.01).

    Args:
        df (pd.DataFrame): DataFrame containing columns ['SrcEntity', 'TgtEntity', 'Score'].
        score_margin (float): Score margin. 0.01 means keep scores ≥ 99% of best score.

    Returns:
        pd.DataFrame: Filtered DataFrame with multiple high-quality candidates per SrcEntity.
    """
    selected_rows = []

    for src, group in df.groupby("SrcEntity"):
        group_sorted = group.sort_values(by="Score", ascending=False)
        best_score = group_sorted.iloc[0]["Score"]
        threshold = best_score * (1 - score_margin)

        # Keep all target entities with score >= threshold
        close_matches = group_sorted[group_sorted["Score"] >= threshold]
        selected_rows.append(close_matches)

    result_df = pd.concat(selected_rows).reset_index(drop=True)
    print(f"🏆 Selected candidates within {(1 - score_margin) * 100:.1f}% of best score per SrcEntity: {len(result_df)} rows")
    return result_df

```


```python
import pandas as pd

def filter_and_evaluate_predictions_top1(
    topk_file,
    train_file,
    test_file,
    src_onto,
    tgt_onto,
    threshold=0.0  # Similarity score threshold
):
    # === Step 1: Load prediction and reference files ===
    df = pd.read_csv(topk_file, sep='\t', dtype=str)
    train_df = pd.read_csv(train_file, sep="\t", dtype=str)
    test_df = pd.read_csv(test_file, sep="\t", dtype=str)
    print(f"🔍 Initial file: {len(df)} rows")

    # === Step 2: Remove URIs only present in train set ===
    train_uris = set(train_df['SrcEntity']) | set(train_df['TgtEntity'])
    test_uris = set(test_df['SrcEntity']) | set(test_df['TgtEntity'])
    uris_to_exclude = train_uris - test_uris
    df = df[~(df['SrcEntity'].isin(uris_to_exclude) | df['TgtEntity'].isin(uris_to_exclude))].reset_index(drop=True)
    print(f"✅ After removing train-only URIs: {len(df)} rows")

    # === Step 3: Remove ignored ontology classes ===
    ignored_class_index = get_ignored_class_index(src_onto)
    ignored_class_index.update(get_ignored_class_index(tgt_onto))
    ignored_uris = {str(uri).strip() for uri in ignored_class_index}
    df = df[~(df['SrcEntity'].isin(ignored_uris) | df['TgtEntity'].isin(ignored_uris))].reset_index(drop=True)
    print(f"✅ After removing ignored classes: {len(df)} rows")

    # === Step 4: Keep only predictions with source entities from the test set ===
    test_src_entities = set(test_df['SrcEntity'])
    df = df[df['SrcEntity'].isin(test_src_entities)].reset_index(drop=True)
    print(f"✅ After keeping only test SrcEntities: {len(df)} rows")

    # === Step 5: Apply similarity threshold ===
    df['Score'] = df['Score'].astype(float)
    df = df[df['Score'] >= threshold]
    print(f"✅ After applying threshold ≥ {threshold}: {len(df)} rows")

    # === Step 6: Save all filtered predictions (before top-1 selection) ===
    output_file_all = topk_file.replace(".tsv", f"_filtered.tsv")
    df.to_csv(output_file_all, sep='\t', index=False)
    print(f"📁 Filtered predictions saved: {output_file_all}")

    # === Step 7: Select best candidates per source with relaxed top-k margin ===
    df_top1 = select_best_candidates_per_src_with_margin(df, score_margin=0.0035)

    # === Step 8: Save relaxed Top-1 filtered results ===
    output_file_top1 = topk_file.replace(".tsv", f"_filtered_top1_th{threshold}.tsv")
    df_top1.to_csv(output_file_top1, sep='\t', index=False)
    print(f"📁 Filtered Top-1 file saved: {output_file_top1}")

    # === Step 9: Evaluate Top-1 results ===
    preds = EntityMapping.read_table_mappings(output_file_top1)
    refs = ReferenceMapping.read_table_mappings(test_file)
    preds = remove_ignored_mappings(preds, ignored_class_index)

    results = AlignmentEvaluator.f1(preds, refs)

    preds2 = [p.to_tuple() for p in preds]
    refs2 = [r.to_tuple() for r in refs]
    correct = len(set(preds2).intersection(set(refs2)))

    print(f"🎯 Correct mappings (Top-1): {correct}")
    print(f"📊 Evaluation (P / R / F1): {results}")

    return output_file_top1, results, correct

```


```python
import pandas as pd

def filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file,
    train_file,
    test_file
):
    # === Step 1: Load files ===
    df = pd.read_csv(topk_file, sep='\t', dtype=str)
    train_df = pd.read_csv(train_file, sep="\t", dtype=str)
    test_df = pd.read_csv(test_file, sep="\t", dtype=str)

    print(f"🔍 Initial file: {len(df)} rows")

    # === Step 2: Identify common entities (source or target) ===
    test_entities = set(test_df['SrcEntity']).union(set(test_df['TgtEntity']))
    train_entities = set(train_df['SrcEntity']).union(set(train_df['TgtEntity']))
    common_entities = test_entities & train_entities
    print(f"❗ Common entities to remove: {len(common_entities)}")

    # === Step 3: Filter test set and keep only pure test entities ===
    filtered_test_df = test_df[
        ~(test_df['SrcEntity'].isin(common_entities) | test_df['TgtEntity'].isin(common_entities))
    ].reset_index(drop=True)

    src_entities_test = set(filtered_test_df['SrcEntity'])

    # === Step 4: Filter topk predictions to only include valid test sources ===
    df['SrcEntity'] = df['SrcEntity'].str.strip()
    df = df[df['SrcEntity'].isin(src_entities_test)].reset_index(drop=True)
    print(f"✅ After filtering to test SrcEntities only: {len(df)} rows")

    # === Step 5: Convert Score and compute Top-1 ===
    df['Score'] = df['Score'].astype(float)
    df_top1 = df.sort_values(by=["SrcEntity", "Score"], ascending=[True, False]) \
                .groupby("SrcEntity", as_index=False).first()

    # === Step 6: Add missing SrcEntities (not in df_top1) ===
    missing_srcs = src_entities_test - set(df_top1['SrcEntity'])
    if missing_srcs:
        print(f"⚠️ Adding {len(missing_srcs)} missing SrcEntities with no prediction")
        missing_rows = pd.DataFrame({
            "SrcEntity": list(missing_srcs),
            "TgtEntity": [None] * len(missing_srcs),
            "Score": [0.0] * len(missing_srcs)
        })
        df_top1 = pd.concat([df_top1, missing_rows], ignore_index=True)

    df_top1 = df_top1.sort_values(by="SrcEntity").reset_index(drop=True)
    print(f"🏆 After Top-1 selection: {len(df_top1)} rows")

    # === Step 7: Save final Top-1 predictions ===
    output_file = topk_file.replace(".tsv", f"_Top1_testclean.tsv")
    df_top1.to_csv(output_file, sep='\t', index=False)
    print(f"📁 Final Top-1 file saved: {output_file}")

    # === Step 8: Evaluate against filtered test set ===
    refs_set = set(zip(filtered_test_df['SrcEntity'], filtered_test_df['TgtEntity']))
    preds_set = set(zip(df_top1['SrcEntity'], df_top1['TgtEntity']))

    correct = len(preds_set & refs_set)
    precision = correct / len(preds_set) if preds_set else 0.0
    recall = correct / len(refs_set) if refs_set else 0.0
    f1 = (2 * precision * recall) / (precision + recall + 1e-8) if (precision + recall) > 0 else 0.0

    results = {
        "Precision": round(precision, 4),
        "Recall": round(recall, 4),
        "F1": round(f1, 4)
    }

    print(f"🎯 Correct mappings (Top-1): {correct}")
    print(f"📊 Evaluation (P / R / F1): {results}")

    return output_file, results, correct

```

# **Precision@k, Recall@k, F1@k**


```python
import pandas as pd
from collections import defaultdict

def filter_topk(
    topk_file,
    train_file,
    test_file,
    src_onto,
    tgt_onto,
    k=10,
    threshold=0.0
):
    """
    Filtre les prédictions top-k pour ne garder que les concepts source du test set,
    exclut les classes ignorées et évalue Precision@k / Recall@k / F1@k.
    """
    # === Load data ===
    df = pd.read_csv(topk_file, sep='\t', dtype=str)
    train_df = pd.read_csv(train_file, sep='\t', dtype=str)
    test_df = pd.read_csv(test_file, sep='\t', dtype=str)

    print(f"🔍 Initial candidate mappings: {len(df)}")

    # === Score conversion and threshold ===
    df['Score'] = df['Score'].apply(lambda x: float(str(x).strip('[]')) if pd.notnull(x) else 0.0)
    df = df[df['Score'] >= threshold]
    print(f"✅ After applying threshold {threshold}: {len(df)}")

    # === Remove train-only URIs ===
    train_uris = set(train_df['SrcEntity']) | set(train_df['TgtEntity'])
    test_uris = set(test_df['SrcEntity']) | set(test_df['TgtEntity'])
    uris_to_exclude = train_uris - test_uris
    df = df[~(df['SrcEntity'].isin(uris_to_exclude) | df['TgtEntity'].isin(uris_to_exclude))]
    print(f"✅ After removing train-only URIs: {len(df)}")

    # === Remove ignored classes ===
    ignored_class_index = get_ignored_class_index(src_onto)
    ignored_class_index.update(get_ignored_class_index(tgt_onto))
    ignored_uris = {str(uri).strip() for uri in ignored_class_index}
    df = df[~(df['SrcEntity'].isin(ignored_uris) | df['TgtEntity'].isin(ignored_uris))]
    print(f"✅ After removing ignored classes: {len(df)}")

    # === Keep only source entities from test set ===
    test_src_entities = set(test_df['SrcEntity'])
    df = df[df['SrcEntity'].isin(test_src_entities)]
    print(f"✅ After keeping only test SrcEntities: {len(df)}")

    # === Save filtered predictions ===
    output_file = topk_file.replace(".tsv", f"_filtered_top{k}_th{threshold}.tsv")
    df.to_csv(output_file, sep='\t', index=False)
    print(f"📁 Filtered top-k saved to: {output_file}")

    # === Evaluate Precision@k / Recall@k / F1@k ===
    results = evaluate_topk(df, test_df, k=k)
    print(f"📊 Evaluation results (Top-{k}): {results}")

    return output_file, results

```


```python
def evaluate_topk(predictions_df, reference_df, k=10):
    ref_dict = defaultdict(set)
    for _, row in reference_df.iterrows():
        ref_dict[row['SrcEntity']].add(row['TgtEntity'])

    topk_df = predictions_df.sort_values(by='Score', ascending=False).groupby('SrcEntity').head(k)

    total_tp = total_pred = total_ref = 0

    for src, group in topk_df.groupby('SrcEntity'):
        predicted = set(group['TgtEntity'])
        true = ref_dict.get(src, set())
        tp = len(predicted & true)
        total_tp += tp
        total_pred += len(predicted)
        total_ref += len(true)

    precision = total_tp / total_pred if total_pred else 0.0
    recall = total_tp / total_ref if total_ref else 0.0
    f1 = 2 * precision * recall / (precision + recall + 1e-8) if precision + recall > 0 else 0.0

    return {
        f'Precision@{k}': round(precision, 4),
        f'Recall@{k}': round(recall, 4),
        f'F1@{k}': round(f1, 4)
    }
```

# **Local MRR and Hit@k**


```python
import pandas as pd

# === Étape 1 : Charger les fichiers ===
# Chemins vers les fichiers
cands_path = candidates_Rank = f"{data_dir}/{task}_cands.csv"
src_emb_path = f"{data_dir}/{src_ent}_final_embeddings.tsv"
tgt_emb_path = f"{data_dir}/{tgt_ent}_final_embeddings.tsv"

# Lire les fichiers
df_cands = pd.read_csv(cands_path)
src_emb_df = pd.read_csv(src_emb_path, sep="\t")
tgt_emb_df = pd.read_csv(tgt_emb_path, sep="\t")

# === Étape 2 : Extraire les URI uniques des sources et des cibles ===
unique_src_df = pd.DataFrame(df_cands["SrcEntity"].unique(), columns=["Concept"])
unique_tgt_df = pd.DataFrame(df_cands["TgtEntity"].unique(), columns=["Concept"])

# === Étape 3 : Rattacher les embeddings à chaque concept ===
merged_src_df = pd.merge(unique_src_df, src_emb_df, on="Concept", how="left")
merged_tgt_df = pd.merge(unique_tgt_df, tgt_emb_df, on="Concept", how="left")

# === Étape 4 : Sauvegarder les résultats ===
merged_src_df.to_csv(f"{data_dir}/{src_ent}_cands_with_embeddings.tsv", sep="\t", index=False)
merged_tgt_df.to_csv(f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv", sep="\t", index=False)
```

# **Mappings Generation**

# **Using Euclidean**

# **K=1**


```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_1_mappings_euclidean.tsv"
)

```

    ✅ Top-1 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_euclidean.tsv
    ⏱️ Execution time: 9.83 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_euclidean_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1486
    📊 Evaluation (P / R / F1): {'Precision': 0.7456, 'Recall': 0.721, 'F1': 0.7331}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9338 rows
    ✅ After removing ignored classes: 9338 rows
    ✅ After keeping only test SrcEntities: 2401 rows
    ✅ After applying threshold ≥ 0.0: 2401 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_euclidean_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2401 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_euclidean_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1815
    📊 Evaluation (P / R / F1): {'P': 0.756, 'R': 0.682, 'F1': 0.717}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/neoplas_top_1_mappings_euclidean_filtered.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7559, 'Recall@1': 0.7071, 'F1@1': 0.7307}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_1_mappings_euclidean_mrr_hit.tsv"
)

```

    Top-1 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_euclidean_mrr_hit.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_euclidean_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6909033534555333, 'Hits@1': 0.681562147953436, 'Hits@5': 0.681562147953436, 'Hits@10': 0.681562147953436}


# **K=5**


```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=5,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_5_mappings_euclidean.tsv"
)

```

    ✅ Top-5 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_5_mappings_euclidean.tsv
    ⏱️ Execution time: 9.69 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_5_mappings_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 57035 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 9965 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_5_mappings_euclidean_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1486
    📊 Evaluation (P / R / F1): {'Precision': 0.7456, 'Recall': 0.721, 'F1': 0.7331}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_5_mappings_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 57035 rows
    ✅ After removing train-only URIs: 46753 rows
    ✅ After removing ignored classes: 46753 rows
    ✅ After keeping only test SrcEntities: 11240 rows
    ✅ After applying threshold ≥ 0.0: 11240 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_5_mappings_euclidean_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2526 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_5_mappings_euclidean_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1879
    📊 Evaluation (P / R / F1): {'P': 0.744, 'R': 0.706, 'F1': 0.724}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_5_mappings_euclidean_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7492, 'Recall@1': 0.7, 'F1@1': 0.7237}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=5,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_5_mappings_euclidean_mrr_hit.tsv"
)

```

    ✅ Top-5 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_5_mappings_euclidean_mrr_hit.tsv
    ⏱️ Execution time: 4.06 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_5_mappings_euclidean_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8825049681028113, 'Hits@1': 0.8516710476905746, 'Hits@5': 0.9121291776192264, 'Hits@10': 0.9121291776192264}


# **K=10**


```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_10_mappings_euclidean.tsv"
)

```

    ✅ Top-10 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_euclidean.tsv
    ⏱️ Execution time: 10.82 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_euclidean_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1486
    📊 Evaluation (P / R / F1): {'Precision': 0.7456, 'Recall': 0.721, 'F1': 0.7331}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 93974 rows
    ✅ After removing ignored classes: 93974 rows
    ✅ After keeping only test SrcEntities: 22353 rows
    ✅ After applying threshold ≥ 0.0: 22353 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_euclidean_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2526 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_euclidean_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1879
    📊 Evaluation (P / R / F1): {'P': 0.744, 'R': 0.706, 'F1': 0.724}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_euclidean_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7492, 'Recall@1': 0.7, 'F1@1': 0.7237}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_10_mappings_euclidean_mrr_hit.tsv"
)

```

    Top-10 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_euclidean_mrr_hit.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_euclidean_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9087012240228131, 'Hits@1': 0.8708223807735637, 'Hits@5': 0.9504318437852046, 'Hits@10': 0.9504318437852046}


# **K=30**


```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_30_mappings_euclidean.tsv"
)

```

    ✅ Top-30 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_euclidean.tsv
    ⏱️ Execution time: 12.45 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_euclidean_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1486
    📊 Evaluation (P / R / F1): {'Precision': 0.7456, 'Recall': 0.721, 'F1': 0.7331}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 284213 rows
    ✅ After removing ignored classes: 284213 rows
    ✅ After keeping only test SrcEntities: 67469 rows
    ✅ After applying threshold ≥ 0.0: 67469 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_euclidean_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2526 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_euclidean_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1879
    📊 Evaluation (P / R / F1): {'P': 0.744, 'R': 0.706, 'F1': 0.724}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_euclidean_filtered.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7492, 'Recall@1': 0.7, 'F1@1': 0.7237}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_30_mappings_euclidean_mrr_hit.tsv"
)

```

    Top-30 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_euclidean_mrr_hit.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_euclidean_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9210168581050832, 'Hits@1': 0.8768306421329328, 'Hits@5': 0.97371385655276, 'Hits@10': 0.9778445362373264}


# **K=100**


```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_100_mappings_euclidean.tsv"
)

```

    ✅ Top-100 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_euclidean.tsv
    ⏱️ Execution time: 19.06 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_euclidean_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1486
    📊 Evaluation (P / R / F1): {'Precision': 0.7456, 'Recall': 0.721, 'F1': 0.7331}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 953701 rows
    ✅ After removing ignored classes: 953701 rows
    ✅ After keeping only test SrcEntities: 226196 rows
    ✅ After applying threshold ≥ 0.0: 226196 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_euclidean_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2526 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_euclidean_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1879
    📊 Evaluation (P / R / F1): {'P': 0.744, 'R': 0.706, 'F1': 0.724}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_euclidean_filtered.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7492, 'Recall@1': 0.7, 'F1@1': 0.7237}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_100_mappings_euclidean_mrr_hit.tsv"
)

```

    Top-100 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_euclidean_mrr_hit.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_euclidean_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9249873795274303, 'Hits@1': 0.8775816748028539, 'Hits@5': 0.984228313931656, 'Hits@10': 0.9906120916259857}


# **K=200**


```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_200_mappings_euclidean.tsv"
)

```

    ✅ Top-200 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_euclidean.tsv
    ⏱️ Execution time: 28.54 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_euclidean_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1486
    📊 Evaluation (P / R / F1): {'Precision': 0.7456, 'Recall': 0.721, 'F1': 0.7331}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1912330 rows
    ✅ After removing ignored classes: 1912330 rows
    ✅ After keeping only test SrcEntities: 453381 rows
    ✅ After applying threshold ≥ 0.0: 453381 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_euclidean_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2526 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_euclidean_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1879
    📊 Evaluation (P / R / F1): {'P': 0.744, 'R': 0.706, 'F1': 0.724}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_euclidean_filtered.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7492, 'Recall@1': 0.7, 'F1@1': 0.7237}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_200_mappings_euclidean_mrr_hit.tsv"
)

```

    Top-200 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_euclidean_mrr_hit.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_euclidean_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.925705592253055, 'Hits@1': 0.8779571911378145, 'Hits@5': 0.9849793466015772, 'Hits@10': 0.9932407059707097}


# **With Encoders**

# **ResMLPEncoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = ResMLPEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_cands_with_embeddings_ResMLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_ResMLPencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using Euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the Euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_euclidean_with_uris(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_euclidean_ResMLPencoded.tsv"
)
```

    Top-1 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_euclidean_ResMLPencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_euclidean_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_euclidean_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1491
    📊 Evaluation (P / R / F1): {'Precision': 0.7481, 'Recall': 0.7234, 'F1': 0.7356}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_euclidean_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9342 rows
    ✅ After removing ignored classes: 9342 rows
    ✅ After keeping only test SrcEntities: 2399 rows
    ✅ After applying threshold ≥ 0.0: 2399 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_euclidean_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2399 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_euclidean_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1818
    📊 Evaluation (P / R / F1): {'P': 0.758, 'R': 0.683, 'F1': 0.718}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_euclidean_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7511, 'Recall@1': 0.7023, 'F1@1': 0.7259}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_1_mappings_euclidean_mrr_hit_ResMLPencoded.tsv"
)

```

    Top-1 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_euclidean_mrr_hit_ResMLPencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_euclidean_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6919970842260992, 'Hits@1': 0.6826886969583177, 'Hits@5': 0.6826886969583177, 'Hits@10': 0.6826886969583177}


# **K=10**


```python
# Compute the top-10 most similar mappings using Euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the Euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_euclidean_with_uris(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_euclidean_ResMLPencoded.tsv"
)
```

    Top-10 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_euclidean_ResMLPencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_euclidean_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_euclidean_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1491
    📊 Evaluation (P / R / F1): {'Precision': 0.7481, 'Recall': 0.7234, 'F1': 0.7356}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_euclidean_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 93920 rows
    ✅ After removing ignored classes: 93920 rows
    ✅ After keeping only test SrcEntities: 22319 rows
    ✅ After applying threshold ≥ 0.0: 22319 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_euclidean_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2524 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_euclidean_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1880
    📊 Evaluation (P / R / F1): {'P': 0.745, 'R': 0.706, 'F1': 0.725}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_euclidean_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7436, 'Recall@1': 0.6947, 'F1@1': 0.7183}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_10_mappings_euclidean_mrr_hit_ResMLPencoded.tsv"
)

```

    Top-10 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_euclidean_mrr_hit_ResMLPencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_euclidean_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9090877674247528, 'Hits@1': 0.8719489297784454, 'Hits@5': 0.950056327450244, 'Hits@10': 0.950056327450244}


# **K=30**


```python
# Compute the top-10 most similar mappings using Euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the Euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_euclidean_with_uris(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_euclidean_ResMLPencoded.tsv"
)
```

    Top-30 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_euclidean_ResMLPencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_euclidean_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_euclidean_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1491
    📊 Evaluation (P / R / F1): {'Precision': 0.7481, 'Recall': 0.7234, 'F1': 0.7356}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_euclidean_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 284018 rows
    ✅ After removing ignored classes: 284018 rows
    ✅ After keeping only test SrcEntities: 67461 rows
    ✅ After applying threshold ≥ 0.0: 67461 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_euclidean_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2524 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_euclidean_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1880
    📊 Evaluation (P / R / F1): {'P': 0.745, 'R': 0.706, 'F1': 0.725}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_euclidean_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7436, 'Recall@1': 0.6947, 'F1@1': 0.7183}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_30_mappings_euclidean_mrr_hit_ResMLPencoded.tsv"
)

```

    Top-30 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_euclidean_mrr_hit_ResMLPencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_euclidean_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9211469652236597, 'Hits@1': 0.8772061584678934, 'Hits@5': 0.97371385655276, 'Hits@10': 0.9774690199023658}


# **K=100**


```python
# Compute the top-10 most similar mappings using Euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the Euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_euclidean_with_uris(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_euclidean_ResMLPencoded.tsv"
)
```

    Top-100 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_euclidean_ResMLPencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_euclidean_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_euclidean_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1491
    📊 Evaluation (P / R / F1): {'Precision': 0.7481, 'Recall': 0.7234, 'F1': 0.7356}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_euclidean_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 953425 rows
    ✅ After removing ignored classes: 953425 rows
    ✅ After keeping only test SrcEntities: 226116 rows
    ✅ After applying threshold ≥ 0.0: 226116 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_euclidean_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2524 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_euclidean_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1880
    📊 Evaluation (P / R / F1): {'P': 0.745, 'R': 0.706, 'F1': 0.725}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_euclidean_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7436, 'Recall@1': 0.6947, 'F1@1': 0.7183}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_100_mappings_euclidean_mrr_hit_ResMLPencoded.tsv"
)

```

    Top-100 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_euclidean_mrr_hit_ResMLPencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_euclidean_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9255115896064701, 'Hits@1': 0.878332707472775, 'Hits@5': 0.984228313931656, 'Hits@10': 0.9902365752910252}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using Euclidean distance on ResMLP-encoded embeddings, then save the results.

topk_euclidean_with_uris(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_euclidean_ResMLPencoded.tsv"  # Save results to this output file
)
```

    Top-200 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_euclidean_ResMLPencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_euclidean_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_euclidean_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1491
    📊 Evaluation (P / R / F1): {'Precision': 0.7481, 'Recall': 0.7234, 'F1': 0.7356}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_euclidean_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1911373 rows
    ✅ After removing ignored classes: 1911373 rows
    ✅ After keeping only test SrcEntities: 453155 rows
    ✅ After applying threshold ≥ 0.0: 453155 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_euclidean_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2524 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_euclidean_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1880
    📊 Evaluation (P / R / F1): {'P': 0.745, 'R': 0.706, 'F1': 0.725}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_euclidean_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7436, 'Recall@1': 0.6947, 'F1@1': 0.7183}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_200_mappings_euclidean_mrr_hit_ResMLPencoded.tsv"
)

```

    Top-200 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_euclidean_mrr_hit_ResMLPencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_euclidean_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9259205080966103, 'Hits@1': 0.878332707472775, 'Hits@5': 0.9849793466015772, 'Hits@10': 0.9928651896357491}


# **MLPEncoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = MLPEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_MLPencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_MLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_cands_with_embeddings_MLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_MLPencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using Euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the Euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_euclidean_with_uris(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_euclidean_MLPencoded.tsv"
)
```

    Top-1 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_euclidean_MLPencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_euclidean_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_euclidean_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1483
    📊 Evaluation (P / R / F1): {'Precision': 0.7441, 'Recall': 0.7196, 'F1': 0.7316}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_euclidean_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9388 rows
    ✅ After removing ignored classes: 9388 rows
    ✅ After keeping only test SrcEntities: 2407 rows
    ✅ After applying threshold ≥ 0.0: 2407 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_euclidean_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2407 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_euclidean_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1804
    📊 Evaluation (P / R / F1): {'P': 0.749, 'R': 0.677, 'F1': 0.712}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_euclidean_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7405, 'Recall@1': 0.6935, 'F1@1': 0.7163}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_1_mappings_euclidean_mrr_hit_MLPencoded.tsv"
)

```

    Top-1 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_euclidean_mrr_hit_MLPencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_euclidean_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6868941643499249, 'Hits@1': 0.6774314682688697, 'Hits@5': 0.6774314682688697, 'Hits@10': 0.6774314682688697}


# **K=10**


```python
# Compute the top-10 most similar mappings using Euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the Euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_euclidean_with_uris(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_euclidean_MLPencoded.tsv"
)
```

    Top-10 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_euclidean_MLPencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_euclidean_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_euclidean_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1483
    📊 Evaluation (P / R / F1): {'Precision': 0.7441, 'Recall': 0.7196, 'F1': 0.7316}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_euclidean_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 93990 rows
    ✅ After removing ignored classes: 93990 rows
    ✅ After keeping only test SrcEntities: 22382 rows
    ✅ After applying threshold ≥ 0.0: 22382 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_euclidean_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2571 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_euclidean_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1866
    📊 Evaluation (P / R / F1): {'P': 0.726, 'R': 0.701, 'F1': 0.713}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_euclidean_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7335, 'Recall@1': 0.6853, 'F1@1': 0.7086}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_10_mappings_euclidean_mrr_hit_MLPencoded.tsv"
)

```

    Top-10 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_euclidean_mrr_hit_MLPencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_euclidean_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9090877674247528, 'Hits@1': 0.8719489297784454, 'Hits@5': 0.950056327450244, 'Hits@10': 0.950056327450244}


# **K=30**


```python
# Compute the top-10 most similar mappings using Euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the Euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_euclidean_with_uris(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_euclidean_MLPencoded.tsv"
)
```

    Top-30 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_euclidean_MLPencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_euclidean_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_euclidean_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1483
    📊 Evaluation (P / R / F1): {'Precision': 0.7441, 'Recall': 0.7196, 'F1': 0.7316}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_euclidean_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 283640 rows
    ✅ After removing ignored classes: 283640 rows
    ✅ After keeping only test SrcEntities: 67291 rows
    ✅ After applying threshold ≥ 0.0: 67291 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_euclidean_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2571 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_euclidean_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1866
    📊 Evaluation (P / R / F1): {'P': 0.726, 'R': 0.701, 'F1': 0.713}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_euclidean_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7283, 'Recall@1': 0.6804, 'F1@1': 0.7036}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_30_mappings_euclidean_mrr_hit_MLPencoded.tsv"
)

```

    Top-30 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_euclidean_mrr_hit_MLPencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_euclidean_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9176873232267122, 'Hits@1': 0.873075478783327, 'Hits@5': 0.9703342095381149, 'Hits@10': 0.97371385655276}


# **K=100**


```python
# Compute the top-10 most similar mappings using Euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the Euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_euclidean_with_uris(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_euclidean_MLPencoded.tsv"
)
```

    Top-100 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_euclidean_MLPencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_euclidean_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_euclidean_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1483
    📊 Evaluation (P / R / F1): {'Precision': 0.7441, 'Recall': 0.7196, 'F1': 0.7316}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_euclidean_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 950237 rows
    ✅ After removing ignored classes: 950237 rows
    ✅ After keeping only test SrcEntities: 225382 rows
    ✅ After applying threshold ≥ 0.0: 225382 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_euclidean_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2571 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_euclidean_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1866
    📊 Evaluation (P / R / F1): {'P': 0.726, 'R': 0.701, 'F1': 0.713}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_euclidean_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7283, 'Recall@1': 0.6804, 'F1@1': 0.7036}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_100_mappings_euclidean_mrr_hit_MLPencoded.tsv"
)

```

    Top-100 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_euclidean_mrr_hit_MLPencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_euclidean_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9221417372326226, 'Hits@1': 0.8749530604581299, 'Hits@5': 0.9800976342470897, 'Hits@10': 0.9876079609463012}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using Euclidean distance on ResMLP-encoded embeddings, then save the results.

topk_euclidean_with_uris(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_euclidean_MLPencoded.tsv"  # Save results to this output file
)
```

    Top-200 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_euclidean_MLPencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_euclidean_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_euclidean_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1483
    📊 Evaluation (P / R / F1): {'Precision': 0.7441, 'Recall': 0.7196, 'F1': 0.7316}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_euclidean_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1903564 rows
    ✅ After removing ignored classes: 1903564 rows
    ✅ After keeping only test SrcEntities: 451423 rows
    ✅ After applying threshold ≥ 0.0: 451423 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_euclidean_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2571 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_euclidean_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1866
    📊 Evaluation (P / R / F1): {'P': 0.726, 'R': 0.701, 'F1': 0.713}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_euclidean_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7283, 'Recall@1': 0.6804, 'F1@1': 0.7036}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_200_mappings_euclidean_mrr_hit_MLPencoded.tsv"
)

```

    Top-200 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_euclidean_mrr_hit_MLPencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_euclidean_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9233137232238512, 'Hits@1': 0.8757040931280511, 'Hits@5': 0.9823507322568532, 'Hits@10': 0.9898610589560646}


# **LinearEncoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = LinearEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_Linencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_Linencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_cands_with_embeddings_Linencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_Linencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using Euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the Euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_euclidean_with_uris(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_euclidean_Linencoded.tsv"
)
```

    Top-1 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_euclidean_Linencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_euclidean_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_euclidean_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1498
    📊 Evaluation (P / R / F1): {'Precision': 0.7516, 'Recall': 0.7268, 'F1': 0.739}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_euclidean_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9372 rows
    ✅ After removing ignored classes: 9372 rows
    ✅ After keeping only test SrcEntities: 2400 rows
    ✅ After applying threshold ≥ 0.0: 2400 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_euclidean_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2400 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_euclidean_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1822
    📊 Evaluation (P / R / F1): {'P': 0.759, 'R': 0.684, 'F1': 0.72}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_euclidean_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7491, 'Recall@1': 0.7005, 'F1@1': 0.724}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_1_mappings_euclidean_mrr_hit_Linencoded.tsv"
)

```

    Top-1 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_euclidean_mrr_hit_Linencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_euclidean_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6934537089338119, 'Hits@1': 0.68419076229816, 'Hits@5': 0.68419076229816, 'Hits@10': 0.68419076229816}


# **K=10**


```python
# Compute the top-10 most similar mappings using Euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the Euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_euclidean_with_uris(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_euclidean_Linencoded.tsv"
)
```

    Top-10 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_euclidean_Linencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_euclidean_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_euclidean_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1498
    📊 Evaluation (P / R / F1): {'Precision': 0.7516, 'Recall': 0.7268, 'F1': 0.739}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_euclidean_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 94044 rows
    ✅ After removing ignored classes: 94044 rows
    ✅ After keeping only test SrcEntities: 22364 rows
    ✅ After applying threshold ≥ 0.0: 22364 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_euclidean_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2532 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_euclidean_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1879
    📊 Evaluation (P / R / F1): {'P': 0.742, 'R': 0.706, 'F1': 0.723}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_euclidean_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.742, 'Recall@1': 0.6932, 'F1@1': 0.7168}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_10_mappings_euclidean_mrr_hit_Linencoded.tsv"
)

```

    Top-10 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_euclidean_mrr_hit_Linencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_euclidean_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9071688445542276, 'Hits@1': 0.8696958317686819, 'Hits@5': 0.9474277131055201, 'Hits@10': 0.9478032294404807}


# **K=30**


```python
# Compute the top-10 most similar mappings using Euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the Euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_euclidean_with_uris(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_euclidean_Linencoded.tsv"
)
```

    Top-30 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_euclidean_Linencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_euclidean_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_euclidean_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1498
    📊 Evaluation (P / R / F1): {'Precision': 0.7516, 'Recall': 0.7268, 'F1': 0.739}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_euclidean_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 284093 rows
    ✅ After removing ignored classes: 284093 rows
    ✅ After keeping only test SrcEntities: 67420 rows
    ✅ After applying threshold ≥ 0.0: 67420 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_euclidean_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2532 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_euclidean_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1879
    📊 Evaluation (P / R / F1): {'P': 0.742, 'R': 0.706, 'F1': 0.723}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_euclidean_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.742, 'Recall@1': 0.6932, 'F1@1': 0.7168}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_30_mappings_euclidean_mrr_hit_Linencoded.tsv"
)

```

    Top-30 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_euclidean_mrr_hit_Linencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_euclidean_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9222392789554695, 'Hits@1': 0.8775816748028539, 'Hits@5': 0.9744648892226812, 'Hits@10': 0.9793466015771686}


# **K=100**


```python
# Compute the top-10 most similar mappings using Euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the Euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_euclidean_with_uris(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_euclidean_Linencoded.tsv"
)
```

    Top-100 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_euclidean_Linencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_euclidean_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_euclidean_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1498
    📊 Evaluation (P / R / F1): {'Precision': 0.7516, 'Recall': 0.7268, 'F1': 0.739}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_euclidean_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 953176 rows
    ✅ After removing ignored classes: 953176 rows
    ✅ After keeping only test SrcEntities: 226206 rows
    ✅ After applying threshold ≥ 0.0: 226206 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_euclidean_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2532 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_euclidean_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1879
    📊 Evaluation (P / R / F1): {'P': 0.742, 'R': 0.706, 'F1': 0.723}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_euclidean_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.742, 'Recall@1': 0.6932, 'F1@1': 0.7168}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_100_mappings_euclidean_mrr_hit_Linencoded.tsv"
)

```

    Top-100 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_euclidean_mrr_hit_Linencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_euclidean_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9263841865020298, 'Hits@1': 0.8790837401426962, 'Hits@5': 0.9834772812617348, 'Hits@10': 0.9917386406308675}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using Euclidean distance on ResMLP-encoded embeddings, then save the results.

topk_euclidean_with_uris(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_euclidean_Linencoded.tsv"  # Save results to this output file
)
```

    Top-200 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_euclidean_Linencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_euclidean_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_euclidean_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1498
    📊 Evaluation (P / R / F1): {'Precision': 0.7516, 'Recall': 0.7268, 'F1': 0.739}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_euclidean_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1911596 rows
    ✅ After removing ignored classes: 1911596 rows
    ✅ After keeping only test SrcEntities: 453397 rows
    ✅ After applying threshold ≥ 0.0: 453397 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_euclidean_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2532 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_euclidean_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1879
    📊 Evaluation (P / R / F1): {'P': 0.742, 'R': 0.706, 'F1': 0.723}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_euclidean_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.742, 'Recall@1': 0.6932, 'F1@1': 0.7168}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_200_mappings_euclidean_mrr_hit_Linencoded.tsv"
)

```

    Top-200 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_euclidean_mrr_hit_Linencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_euclidean_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9265658185281622, 'Hits@1': 0.8790837401426962, 'Hits@5': 0.9834772812617348, 'Hits@10': 0.9932407059707097}


# **Transformer Encoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = TransformerEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_TRencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_TRencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_cands_with_embeddings_TRencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_TRencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using Euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the Euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_euclidean_with_uris(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_euclidean_TRencoded.tsv"
)
```

    Top-1 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_euclidean_TRencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_euclidean_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_euclidean_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1451
    📊 Evaluation (P / R / F1): {'Precision': 0.728, 'Recall': 0.704, 'F1': 0.7158}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_euclidean_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9486 rows
    ✅ After removing ignored classes: 9486 rows
    ✅ After keeping only test SrcEntities: 2402 rows
    ✅ After applying threshold ≥ 0.0: 2402 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_euclidean_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2402 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_euclidean_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1772
    📊 Evaluation (P / R / F1): {'P': 0.738, 'R': 0.665, 'F1': 0.7}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_euclidean_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7385, 'Recall@1': 0.691, 'F1@1': 0.714}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_1_mappings_euclidean_mrr_hit_TRencoded.tsv"
)

```

    Top-1 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_euclidean_mrr_hit_TRencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_euclidean_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6752298065301915, 'Hits@1': 0.6654149455501315, 'Hits@5': 0.6654149455501315, 'Hits@10': 0.6654149455501315}


# **K=10**


```python
# Compute the top-10 most similar mappings using Euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the Euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_euclidean_with_uris(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_euclidean_TRencoded.tsv"
)
```

    Top-10 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_euclidean_TRencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_euclidean_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_euclidean_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1451
    📊 Evaluation (P / R / F1): {'Precision': 0.728, 'Recall': 0.704, 'F1': 0.7158}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_euclidean_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 96146 rows
    ✅ After removing ignored classes: 96146 rows
    ✅ After keeping only test SrcEntities: 22638 rows
    ✅ After applying threshold ≥ 0.0: 22638 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_euclidean_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2531 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_euclidean_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1824
    📊 Evaluation (P / R / F1): {'P': 0.721, 'R': 0.685, 'F1': 0.702}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_euclidean_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7291, 'Recall@1': 0.6812, 'F1@1': 0.7043}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_10_mappings_euclidean_mrr_hit_TRencoded.tsv"
)

```

    Top-10 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_euclidean_mrr_hit_TRencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_euclidean_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8984349166358322, 'Hits@1': 0.8603079233946677, 'Hits@5': 0.9391663537363876, 'Hits@10': 0.9391663537363876}


# **K=30**


```python
# Compute the top-10 most similar mappings using Euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the Euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_euclidean_with_uris(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_euclidean_TRencoded.tsv"
)
```

    Top-30 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_euclidean_TRencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_euclidean_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_euclidean_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1451
    📊 Evaluation (P / R / F1): {'Precision': 0.728, 'Recall': 0.704, 'F1': 0.7158}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_euclidean_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 291483 rows
    ✅ After removing ignored classes: 291483 rows
    ✅ After keeping only test SrcEntities: 68488 rows
    ✅ After applying threshold ≥ 0.0: 68488 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_euclidean_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2531 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_euclidean_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1824
    📊 Evaluation (P / R / F1): {'P': 0.721, 'R': 0.685, 'F1': 0.702}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_euclidean_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7291, 'Recall@1': 0.6812, 'F1@1': 0.7043}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_30_mappings_euclidean_mrr_hit_TRencoded.tsv"
)

```

    Top-30 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_euclidean_mrr_hit_TRencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_euclidean_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9168946417504511, 'Hits@1': 0.8711978971085242, 'Hits@5': 0.9722117912129178, 'Hits@10': 0.9744648892226812}


# **K=100**


```python
# Compute the top-10 most similar mappings using Euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the Euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_euclidean_with_uris(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_euclidean_TRencoded.tsv"
)
```

    Top-100 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_euclidean_TRencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_euclidean_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_euclidean_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1451
    📊 Evaluation (P / R / F1): {'Precision': 0.728, 'Recall': 0.704, 'F1': 0.7158}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_euclidean_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 981153 rows
    ✅ After removing ignored classes: 981153 rows
    ✅ After keeping only test SrcEntities: 230587 rows
    ✅ After applying threshold ≥ 0.0: 230587 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_euclidean_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2531 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_euclidean_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1824
    📊 Evaluation (P / R / F1): {'P': 0.721, 'R': 0.685, 'F1': 0.702}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_euclidean_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7291, 'Recall@1': 0.6812, 'F1@1': 0.7043}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_100_mappings_euclidean_mrr_hit_TRencoded.tsv"
)

```

    Top-100 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_euclidean_mrr_hit_TRencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_euclidean_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9226968207979722, 'Hits@1': 0.8734509951182876, 'Hits@5': 0.9846038302666166, 'Hits@10': 0.9913631242959069}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using Euclidean distance on ResMLP-encoded embeddings, then save the results.

topk_euclidean_with_uris(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_euclidean_TRencoded.tsv"  # Save results to this output file
)
```

    Top-200 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_euclidean_TRencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_euclidean_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_euclidean_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1451
    📊 Evaluation (P / R / F1): {'Precision': 0.728, 'Recall': 0.704, 'F1': 0.7158}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_euclidean_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1970864 rows
    ✅ After removing ignored classes: 1970864 rows
    ✅ After keeping only test SrcEntities: 463457 rows
    ✅ After applying threshold ≥ 0.0: 463457 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_euclidean_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2531 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_euclidean_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1824
    📊 Evaluation (P / R / F1): {'P': 0.721, 'R': 0.685, 'F1': 0.702}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_euclidean_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7291, 'Recall@1': 0.6812, 'F1@1': 0.7043}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_200_mappings_euclidean_mrr_hit_TRencoded.tsv"
)

```

    Top-200 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_euclidean_mrr_hit_TRencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_euclidean_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9232885668953403, 'Hits@1': 0.8738265114532482, 'Hits@5': 0.9853548629365377, 'Hits@10': 0.9924896733007886}


# **Using ANNOY: topk_annoy_euclidean**

# **K=1**


```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_1_mappings_annoy_euclidean.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-1 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_euclidean.tsv
    ⏱️ Execution time: 9.20 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_annoy_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_euclidean_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1275
    📊 Evaluation (P / R / F1): {'Precision': 0.6397, 'Recall': 0.6186, 'F1': 0.629}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_annoy_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9366 rows
    ✅ After removing ignored classes: 9366 rows
    ✅ After keeping only test SrcEntities: 2373 rows
    ✅ After applying threshold ≥ 0.0: 2373 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_euclidean_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2373 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_euclidean_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1558
    📊 Evaluation (P / R / F1): {'P': 0.657, 'R': 0.585, 'F1': 0.619}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/neoplas_top_1_mappings_annoy_euclidean_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6566, 'Recall@1': 0.6139, 'F1@1': 0.6345}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_1_mappings_annoy_euclidean_mrr_hit.tsv"
)

```

    Top-1 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_euclidean_mrr_hit.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_annoy_euclidean_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6909033534555333, 'Hits@1': 0.681562147953436, 'Hits@5': 0.681562147953436, 'Hits@10': 0.681562147953436}


# **K=10**


```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_10_mappings_annoy_euclidean.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-10 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_euclidean.tsv
    ⏱️ Execution time: 8.58 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_annoy_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_euclidean_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1275
    📊 Evaluation (P / R / F1): {'Precision': 0.6397, 'Recall': 0.6186, 'F1': 0.629}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_annoy_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 94016 rows
    ✅ After removing ignored classes: 94016 rows
    ✅ After keeping only test SrcEntities: 22385 rows
    ✅ After applying threshold ≥ 0.0: 22385 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_euclidean_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2531 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_euclidean_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1597
    📊 Evaluation (P / R / F1): {'P': 0.631, 'R': 0.6, 'F1': 0.615}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_annoy_euclidean_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6387, 'Recall@1': 0.5967, 'F1@1': 0.617}



```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_10_mappings_annoy_euclidean_mrr_hit.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-10 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_euclidean_mrr_hit.tsv
    ⏱️ Execution time: 4.28 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_annoy_euclidean_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.7015951534348566, 'Hits@1': 0.6744273375891852, 'Hits@5': 0.7153586180998873, 'Hits@10': 0.7153586180998873}


# **K=30**


```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_30_mappings_annoy_euclidean.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-30 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_euclidean.tsv
    ⏱️ Execution time: 9.33 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_annoy_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_euclidean_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1275
    📊 Evaluation (P / R / F1): {'Precision': 0.6397, 'Recall': 0.6186, 'F1': 0.629}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_annoy_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 282695 rows
    ✅ After removing ignored classes: 282695 rows
    ✅ After keeping only test SrcEntities: 67026 rows
    ✅ After applying threshold ≥ 0.0: 67026 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_euclidean_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2531 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_euclidean_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1597
    📊 Evaluation (P / R / F1): {'P': 0.631, 'R': 0.6, 'F1': 0.615}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_annoy_euclidean_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6387, 'Recall@1': 0.5967, 'F1@1': 0.617}



```python
topk_euclidean_with_uris(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_30_mappings_annoy_euclidean_mrr_hit.tsv"
)

```

    Top-30 Euclidean similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_euclidean_mrr_hit.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_annoy_euclidean_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9210168581050832, 'Hits@1': 0.8768306421329328, 'Hits@5': 0.97371385655276, 'Hits@10': 0.9778445362373264}


# **K=100**


```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_100_mappings_annoy_euclidean.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-100 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_euclidean.tsv
    ⏱️ Execution time: 18.57 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_annoy_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_euclidean_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1380
    📊 Evaluation (P / R / F1): {'Precision': 0.6924, 'Recall': 0.6696, 'F1': 0.6808}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_annoy_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 946624 rows
    ✅ After removing ignored classes: 946624 rows
    ✅ After keeping only test SrcEntities: 224550 rows
    ✅ After applying threshold ≥ 0.0: 224550 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_euclidean_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2523 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_euclidean_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1739
    📊 Evaluation (P / R / F1): {'P': 0.689, 'R': 0.653, 'F1': 0.671}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6941, 'Recall@1': 0.6485, 'F1@1': 0.6705}



```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_mrr_hit.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-100 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_euclidean_mrr_hit.tsv
    ⏱️ Execution time: 6.32 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.7972239597537332, 'Hits@1': 0.763800225309801, 'Hits@5': 0.8280135185880586, 'Hits@10': 0.8310176492677431}


# **K=200**


```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_200_mappings_annoy_euclidean.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-200 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_euclidean.tsv
    ⏱️ Execution time: 28.61 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_annoy_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_euclidean_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1420
    📊 Evaluation (P / R / F1): {'Precision': 0.7125, 'Recall': 0.689, 'F1': 0.7005}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_annoy_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1898583 rows
    ✅ After removing ignored classes: 1898583 rows
    ✅ After keeping only test SrcEntities: 450298 rows
    ✅ After applying threshold ≥ 0.0: 450298 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_euclidean_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2528 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_euclidean_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1791
    📊 Evaluation (P / R / F1): {'P': 0.708, 'R': 0.673, 'F1': 0.69}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_annoy_euclidean_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7142, 'Recall@1': 0.6673, 'F1@1': 0.69}



```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_200_mappings_annoy_euclidean_mrr_hit.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-200 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_euclidean_mrr_hit.tsv
    ⏱️ Execution time: 10.20 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_annoy_euclidean_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8477243858030415, 'Hits@1': 0.8103642508449117, 'Hits@5': 0.888847164851671, 'Hits@10': 0.8926023282012767}


# **K=500**


```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=500,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_500_mappings_annoy_euclidean.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-500 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_annoy_euclidean.tsv
    ⏱️ Execution time: 60.91 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_500_mappings_annoy_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 5703500 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 996500 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_annoy_euclidean_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1451
    📊 Evaluation (P / R / F1): {'Precision': 0.728, 'Recall': 0.704, 'F1': 0.7158}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_500_mappings_annoy_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 5703500 rows
    ✅ After removing train-only URIs: 4765166 rows
    ✅ After removing ignored classes: 4765166 rows
    ✅ After keeping only test SrcEntities: 1130165 rows
    ✅ After applying threshold ≥ 0.0: 1130165 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_annoy_euclidean_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2528 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_annoy_euclidean_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1835
    📊 Evaluation (P / R / F1): {'P': 0.726, 'R': 0.689, 'F1': 0.707}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_500_mappings_annoy_euclidean_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7319, 'Recall@1': 0.6838, 'F1@1': 0.707}



```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=500,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_500_mappings_annoy_euclidean_mrr_hit.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-500 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_annoy_euclidean_mrr_hit.tsv
    ⏱️ Execution time: 15.86 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_500_mappings_annoy_euclidean_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8975030851554188, 'Hits@1': 0.8546751783702591, 'Hits@5': 0.949305294780323, 'Hits@10': 0.9556890724746526}


# **K=1000**


```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1000,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_1000_mappings_annoy_euclidean.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-1000 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_annoy_euclidean.tsv
    ⏱️ Execution time: 112.04 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1000_mappings_annoy_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407000 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993000 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_annoy_euclidean_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1478
    📊 Evaluation (P / R / F1): {'Precision': 0.7416, 'Recall': 0.7171, 'F1': 0.7292}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1000_mappings_annoy_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 11407000 rows
    ✅ After removing train-only URIs: 9564475 rows
    ✅ After removing ignored classes: 9564475 rows
    ✅ After keeping only test SrcEntities: 2268981 rows
    ✅ After applying threshold ≥ 0.0: 2268981 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_annoy_euclidean_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2526 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_annoy_euclidean_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1871
    📊 Evaluation (P / R / F1): {'P': 0.741, 'R': 0.703, 'F1': 0.721}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1000_mappings_annoy_euclidean_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.746, 'Recall@1': 0.697, 'F1@1': 0.7206}



```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1000,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_1000_mappings_annoy_euclidean_mrr_hit.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-1000 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_annoy_euclidean_mrr_hit.tsv
    ⏱️ Execution time: 26.43 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1000_mappings_annoy_euclidean_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9179797590704873, 'Hits@1': 0.872324446113406, 'Hits@5': 0.9744648892226812, 'Hits@10': 0.9819752159218926}


# **K=2000**


```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=2000,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_2000_mappings_annoy_euclidean.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-2000 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_annoy_euclidean.tsv
    ⏱️ Execution time: 220.67 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_2000_mappings_annoy_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 22814000 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 3986000 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_annoy_euclidean_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1484
    📊 Evaluation (P / R / F1): {'Precision': 0.7446, 'Recall': 0.72, 'F1': 0.7321}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_2000_mappings_annoy_euclidean.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 22814000 rows
    ✅ After removing train-only URIs: 19238783 rows
    ✅ After removing ignored classes: 19238783 rows
    ✅ After keeping only test SrcEntities: 4566194 rows
    ✅ After applying threshold ≥ 0.0: 4566194 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_annoy_euclidean_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2526 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_annoy_euclidean_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1877
    📊 Evaluation (P / R / F1): {'P': 0.743, 'R': 0.705, 'F1': 0.723}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_2000_mappings_annoy_euclidean_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7484, 'Recall@1': 0.6992, 'F1@1': 0.723}



```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=2000,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_2000_mappings_annoy_euclidean_mrr_hit.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-2000 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_annoy_euclidean_mrr_hit.tsv
    ⏱️ Execution time: 49.70 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_2000_mappings_annoy_euclidean_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9253701919282805, 'Hits@1': 0.8772061584678934, 'Hits@5': 0.9846038302666166, 'Hits@10': 0.9936162223056703}


# **With Encoders**

# **ResMLPEncoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = ResMLPEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_cands_with_embeddings_ResMLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_ResMLPencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using Euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the Euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_euclidean(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_annoy_euclidean_ResMLPencoded.tsv"
)
```

    🔹 Using Annoy with metric: euclidean
    Top-1 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_euclidean_ResMLPencoded.tsv
    ⏱️ Execution time: 8.10 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_annoy_euclidean_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_euclidean_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1286
    📊 Evaluation (P / R / F1): {'Precision': 0.6453, 'Recall': 0.624, 'F1': 0.6344}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_annoy_euclidean_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9379 rows
    ✅ After removing ignored classes: 9379 rows
    ✅ After keeping only test SrcEntities: 2379 rows
    ✅ After applying threshold ≥ 0.0: 2379 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_euclidean_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2379 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_euclidean_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1573
    📊 Evaluation (P / R / F1): {'P': 0.661, 'R': 0.591, 'F1': 0.624}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_annoy_euclidean_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6612, 'Recall@1': 0.6183, 'F1@1': 0.639}



```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_1_mappings_annoy_euclidean_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-1 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_euclidean_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 4.46 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_annoy_euclidean_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.5855596613410594, 'Hits@1': 0.573037927149831, 'Hits@5': 0.573037927149831, 'Hits@10': 0.573037927149831}


# **K=10**


```python
# Compute the top-10 most similar mappings using Euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the Euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_euclidean(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_annoy_euclidean_ResMLPencoded.tsv"
)
```

    🔹 Using Annoy with metric: euclidean
    Top-10 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_euclidean_ResMLPencoded.tsv
    ⏱️ Execution time: 8.15 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_annoy_euclidean_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_euclidean_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1286
    📊 Evaluation (P / R / F1): {'Precision': 0.6453, 'Recall': 0.624, 'F1': 0.6344}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_annoy_euclidean_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 93874 rows
    ✅ After removing ignored classes: 93874 rows
    ✅ After keeping only test SrcEntities: 22378 rows
    ✅ After applying threshold ≥ 0.0: 22378 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_euclidean_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2530 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_euclidean_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1616
    📊 Evaluation (P / R / F1): {'P': 0.639, 'R': 0.607, 'F1': 0.622}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_annoy_euclidean_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6459, 'Recall@1': 0.6035, 'F1@1': 0.624}



```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_10_mappings_annoy_euclidean_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-10 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_euclidean_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 4.64 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_annoy_euclidean_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6982667656413073, 'Hits@1': 0.6702966579046189, 'Hits@5': 0.7119789710852422, 'Hits@10': 0.7119789710852422}


# **K=30**


```python
# Compute the top-10 most similar mappings using Euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the Euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_euclidean(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_annoy_euclidean_ResMLPencoded.tsv"
)
```

    🔹 Using Annoy with metric: euclidean
    Top-30 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_euclidean_ResMLPencoded.tsv
    ⏱️ Execution time: 9.66 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_annoy_euclidean_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_euclidean_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1286
    📊 Evaluation (P / R / F1): {'Precision': 0.6453, 'Recall': 0.624, 'F1': 0.6344}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_annoy_euclidean_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 282514 rows
    ✅ After removing ignored classes: 282514 rows
    ✅ After keeping only test SrcEntities: 66953 rows
    ✅ After applying threshold ≥ 0.0: 66953 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_euclidean_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2530 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_euclidean_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1616
    📊 Evaluation (P / R / F1): {'P': 0.639, 'R': 0.607, 'F1': 0.622}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_annoy_euclidean_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6459, 'Recall@1': 0.6035, 'F1@1': 0.624}



```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_30_mappings_annoy_euclidean_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-30 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_euclidean_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 5.35 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_annoy_euclidean_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.7033237615826492, 'Hits@1': 0.673676304919264, 'Hits@5': 0.7213668794592565, 'Hits@10': 0.7213668794592565}


# **K=100**


```python
# Compute the top-10 most similar mappings using Euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the Euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_euclidean(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_ResMLPencoded.tsv"
)
```

    🔹 Using Annoy with metric: euclidean
    Top-100 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_euclidean_ResMLPencoded.tsv
    ⏱️ Execution time: 19.17 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_euclidean_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1388
    📊 Evaluation (P / R / F1): {'Precision': 0.6964, 'Recall': 0.6735, 'F1': 0.6848}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 945727 rows
    ✅ After removing ignored classes: 945727 rows
    ✅ After keeping only test SrcEntities: 224376 rows
    ✅ After applying threshold ≥ 0.0: 224376 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_euclidean_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2529 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_euclidean_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1749
    📊 Evaluation (P / R / F1): {'P': 0.692, 'R': 0.657, 'F1': 0.674}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.699, 'Recall@1': 0.653, 'F1@1': 0.6752}



```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-100 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_euclidean_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 9.46 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8084844677974998, 'Hits@1': 0.7731881336838152, 'Hits@5': 0.8419076229815997, 'Hits@10': 0.8437852046564025}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using Euclidean distance on ResMLP-encoded embeddings, then save the results.

topk_annoy_euclidean(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_annoy_euclidean_ResMLPencoded.tsv"  # Save results to this output file
)
```

    🔹 Using Annoy with metric: euclidean
    Top-200 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_euclidean_ResMLPencoded.tsv
    ⏱️ Execution time: 30.13 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_annoy_euclidean_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_euclidean_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1428
    📊 Evaluation (P / R / F1): {'Precision': 0.7165, 'Recall': 0.6929, 'F1': 0.7045}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_annoy_euclidean_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1895392 rows
    ✅ After removing ignored classes: 1895392 rows
    ✅ After keeping only test SrcEntities: 449856 rows
    ✅ After applying threshold ≥ 0.0: 449856 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_euclidean_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2527 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_euclidean_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1800
    📊 Evaluation (P / R / F1): {'P': 0.712, 'R': 0.676, 'F1': 0.694}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_annoy_euclidean_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7182, 'Recall@1': 0.671, 'F1@1': 0.6938}



```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_200_mappings_annoy_euclidean_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-200 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_euclidean_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 10.09 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_annoy_euclidean_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8510010044153624, 'Hits@1': 0.811866316184754, 'Hits@5': 0.8933533608711979, 'Hits@10': 0.8963574915508825}


# **MLPEncoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = MLPEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_MLPencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_MLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_cands_with_embeddings_MLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_MLPencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using Euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the Euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_euclidean(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_annoy_euclidean_MLPencoded.tsv"
)
```

    🔹 Using Annoy with metric: euclidean
    Top-1 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_euclidean_MLPencoded.tsv
    ⏱️ Execution time: 7.28 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_annoy_euclidean_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_euclidean_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1257
    📊 Evaluation (P / R / F1): {'Precision': 0.6307, 'Recall': 0.6099, 'F1': 0.6201}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_annoy_euclidean_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9370 rows
    ✅ After removing ignored classes: 9370 rows
    ✅ After keeping only test SrcEntities: 2363 rows
    ✅ After applying threshold ≥ 0.0: 2363 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_euclidean_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2363 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_euclidean_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1539
    📊 Evaluation (P / R / F1): {'P': 0.651, 'R': 0.578, 'F1': 0.612}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_annoy_euclidean_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6513, 'Recall@1': 0.6093, 'F1@1': 0.6296}



```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_1_mappings_annoy_euclidean_mrr_hit_MLPencoded.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-1 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_euclidean_mrr_hit_MLPencoded.tsv
    ⏱️ Execution time: 6.72 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_annoy_euclidean_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.5680684891305295, 'Hits@1': 0.5550131430717237, 'Hits@5': 0.5550131430717237, 'Hits@10': 0.5550131430717237}


# **K=10**


```python
# Compute the top-10 most similar mappings using Euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the Euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_euclidean(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_annoy_euclidean_MLPencoded.tsv"
)
```

    🔹 Using Annoy with metric: euclidean
    Top-10 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_euclidean_MLPencoded.tsv
    ⏱️ Execution time: 8.61 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_annoy_euclidean_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_euclidean_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1257
    📊 Evaluation (P / R / F1): {'Precision': 0.6307, 'Recall': 0.6099, 'F1': 0.6201}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_annoy_euclidean_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 93861 rows
    ✅ After removing ignored classes: 93861 rows
    ✅ After keeping only test SrcEntities: 22287 rows
    ✅ After applying threshold ≥ 0.0: 22287 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_euclidean_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2581 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_euclidean_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1570
    📊 Evaluation (P / R / F1): {'P': 0.608, 'R': 0.59, 'F1': 0.599}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_annoy_euclidean_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6266, 'Recall@1': 0.5854, 'F1@1': 0.6053}



```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_10_mappings_annoy_euclidean_mrr_hit_MLPencoded.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-10 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_euclidean_mrr_hit_MLPencoded.tsv
    ⏱️ Execution time: 4.77 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_annoy_euclidean_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6982667656413073, 'Hits@1': 0.6702966579046189, 'Hits@5': 0.7119789710852422, 'Hits@10': 0.7119789710852422}


# **K=30**


```python
# Compute the top-10 most similar mappings using Euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the Euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_euclidean(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_annoy_euclidean_MLPencoded.tsv"
)
```

    🔹 Using Annoy with metric: euclidean
    Top-30 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_euclidean_MLPencoded.tsv
    ⏱️ Execution time: 9.93 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_annoy_euclidean_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_euclidean_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1257
    📊 Evaluation (P / R / F1): {'Precision': 0.6307, 'Recall': 0.6099, 'F1': 0.6201}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_annoy_euclidean_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 282865 rows
    ✅ After removing ignored classes: 282865 rows
    ✅ After keeping only test SrcEntities: 67150 rows
    ✅ After applying threshold ≥ 0.0: 67150 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_euclidean_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2581 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_euclidean_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1570
    📊 Evaluation (P / R / F1): {'P': 0.608, 'R': 0.59, 'F1': 0.599}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_annoy_euclidean_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6266, 'Recall@1': 0.5854, 'F1@1': 0.6053}



```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_30_mappings_annoy_euclidean_mrr_hit_MLPencoded.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-30 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_euclidean_mrr_hit_MLPencoded.tsv
    ⏱️ Execution time: 5.24 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_annoy_euclidean_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6862085273671268, 'Hits@1': 0.6537739391663537, 'Hits@5': 0.7070972587307548, 'Hits@10': 0.707848291400676}


# **K=100**


```python
# Compute the top-10 most similar mappings using Euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the Euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_euclidean(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_MLPencoded.tsv"
)
```

    🔹 Using Annoy with metric: euclidean
    Top-100 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_euclidean_MLPencoded.tsv
    ⏱️ Execution time: 18.35 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_euclidean_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1370
    📊 Evaluation (P / R / F1): {'Precision': 0.6874, 'Recall': 0.6647, 'F1': 0.6759}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 947353 rows
    ✅ After removing ignored classes: 947353 rows
    ✅ After keeping only test SrcEntities: 224613 rows
    ✅ After applying threshold ≥ 0.0: 224613 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_euclidean_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2560 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_euclidean_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1719
    📊 Evaluation (P / R / F1): {'P': 0.671, 'R': 0.646, 'F1': 0.658}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6849, 'Recall@1': 0.6399, 'F1@1': 0.6616}



```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_mrr_hit_MLPencoded.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-100 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_euclidean_mrr_hit_MLPencoded.tsv
    ⏱️ Execution time: 9.59 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.7995274757316854, 'Hits@1': 0.7630491926398798, 'Hits@5': 0.8358993616222306, 'Hits@10': 0.838152459631994}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using Euclidean distance on ResMLP-encoded embeddings, then save the results.

topk_annoy_euclidean(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_annoy_euclidean_MLPencoded.tsv"  # Save results to this output file
)
```

    🔹 Using Annoy with metric: euclidean
    Top-200 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_euclidean_MLPencoded.tsv
    ⏱️ Execution time: 30.87 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_annoy_euclidean_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_euclidean_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1430
    📊 Evaluation (P / R / F1): {'Precision': 0.7175, 'Recall': 0.6938, 'F1': 0.7055}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_annoy_euclidean_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1900699 rows
    ✅ After removing ignored classes: 1900699 rows
    ✅ After keeping only test SrcEntities: 450907 rows
    ✅ After applying threshold ≥ 0.0: 450907 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_euclidean_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2560 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_euclidean_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1800
    📊 Evaluation (P / R / F1): {'P': 0.703, 'R': 0.676, 'F1': 0.689}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_annoy_euclidean_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7162, 'Recall@1': 0.6692, 'F1@1': 0.6919}



```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_200_mappings_annoy_euclidean_mrr_hit_MLPencoded.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-200 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_euclidean_mrr_hit_MLPencoded.tsv
    ⏱️ Execution time: 10.63 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_annoy_euclidean_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8495749014917451, 'Hits@1': 0.8084866691701089, 'Hits@5': 0.8952309425460008, 'Hits@10': 0.899361622230567}


# **LinearEncoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = LinearEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_Linencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_Linencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_cands_with_embeddings_Linencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_Linencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using Euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the Euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_euclidean(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_annoy_euclidean_Linencoded.tsv"
)
```

    🔹 Using Annoy with metric: euclidean
    Top-1 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_euclidean_Linencoded.tsv
    ⏱️ Execution time: 6.58 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_annoy_euclidean_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_euclidean_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1224
    📊 Evaluation (P / R / F1): {'Precision': 0.6141, 'Recall': 0.5939, 'F1': 0.6038}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_annoy_euclidean_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9344 rows
    ✅ After removing ignored classes: 9344 rows
    ✅ After keeping only test SrcEntities: 2368 rows
    ✅ After applying threshold ≥ 0.0: 2368 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_euclidean_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2368 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_euclidean_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1513
    📊 Evaluation (P / R / F1): {'P': 0.639, 'R': 0.568, 'F1': 0.601}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_annoy_euclidean_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6389, 'Recall@1': 0.5973, 'F1@1': 0.6174}



```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_1_mappings_euclidean_mrr_hit_Linencoded.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-1 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_euclidean_mrr_hit_Linencoded.tsv
    ⏱️ Execution time: 4.30 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_annoy_euclidean_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.5516596463725412, 'Hits@1': 0.5381149079984979, 'Hits@5': 0.5381149079984979, 'Hits@10': 0.5381149079984979}


# **K=10**


```python
# Compute the top-10 most similar mappings using Euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the Euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_euclidean(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_annoy_euclidean_Linencoded.tsv"
)
```

    🔹 Using Annoy with metric: euclidean
    Top-10 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_euclidean_Linencoded.tsv
    ⏱️ Execution time: 8.23 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_annoy_euclidean_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_euclidean_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1224
    📊 Evaluation (P / R / F1): {'Precision': 0.6141, 'Recall': 0.5939, 'F1': 0.6038}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_annoy_euclidean_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 94032 rows
    ✅ After removing ignored classes: 94032 rows
    ✅ After keeping only test SrcEntities: 22301 rows
    ✅ After applying threshold ≥ 0.0: 22301 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_euclidean_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2536 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_euclidean_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1544
    📊 Evaluation (P / R / F1): {'P': 0.609, 'R': 0.58, 'F1': 0.594}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_annoy_euclidean_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6174, 'Recall@1': 0.5768, 'F1@1': 0.5964}



```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_10_mappings_annoy_euclidean_mrr_hit_Linencoded.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-10 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_euclidean_mrr_hit_Linencoded.tsv
    ⏱️ Execution time: 5.02 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_annoy_euclidean_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.7011835582079946, 'Hits@1': 0.6755538865940668, 'Hits@5': 0.7127300037551634, 'Hits@10': 0.7127300037551634}


# **K=30**


```python
# Compute the top-10 most similar mappings using annoy_euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the annoy_euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_euclidean(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_annoy_euclidean_Linencoded.tsv"
)
```

    🔹 Using Annoy with metric: euclidean
    Top-30 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_euclidean_Linencoded.tsv
    ⏱️ Execution time: 10.01 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_annoy_euclidean_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_euclidean_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1224
    📊 Evaluation (P / R / F1): {'Precision': 0.6141, 'Recall': 0.5939, 'F1': 0.6038}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_annoy_euclidean_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 283174 rows
    ✅ After removing ignored classes: 283174 rows
    ✅ After keeping only test SrcEntities: 67055 rows
    ✅ After applying threshold ≥ 0.0: 67055 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_euclidean_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2536 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_euclidean_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1545
    📊 Evaluation (P / R / F1): {'P': 0.609, 'R': 0.58, 'F1': 0.594}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_annoy_euclidean_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6178, 'Recall@1': 0.5772, 'F1@1': 0.5968}



```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_30_mappings_annoy_euclidean_mrr_hit_Linencoded.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-30 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_euclidean_mrr_hit_Linencoded.tsv
    ⏱️ Execution time: 5.40 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_annoy_euclidean_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.7059450771558113, 'Hits@1': 0.6785580172737514, 'Hits@5': 0.7213668794592565, 'Hits@10': 0.7213668794592565}


# **K=100**


```python
# Compute the top-10 most similar mappings using annoy_euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the annoy_euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_euclidean(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_Linencoded.tsv"
)
```

    🔹 Using Annoy with metric: euclidean
    Top-100 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_euclidean_Linencoded.tsv
    ⏱️ Execution time: 18.76 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_euclidean_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1370
    📊 Evaluation (P / R / F1): {'Precision': 0.6874, 'Recall': 0.6647, 'F1': 0.6759}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 947297 rows
    ✅ After removing ignored classes: 947297 rows
    ✅ After keeping only test SrcEntities: 224403 rows
    ✅ After applying threshold ≥ 0.0: 224403 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_euclidean_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2533 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_euclidean_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1718
    📊 Evaluation (P / R / F1): {'P': 0.678, 'R': 0.645, 'F1': 0.661}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6869, 'Recall@1': 0.6418, 'F1@1': 0.6636}



```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_mrr_hit_Linencoded.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-100 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_euclidean_mrr_hit_Linencoded.tsv
    ⏱️ Execution time: 7.04 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.7999311126553633, 'Hits@1': 0.7649267743146827, 'Hits@5': 0.8325197146075854, 'Hits@10': 0.8343972962823882}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using annoy_euclidean distance on ResMLP-encoded embeddings, then save the results.

topk_annoy_euclidean(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_annoy_euclidean_Linencoded.tsv"  # Save results to this output file
)
```

    🔹 Using Annoy with metric: euclidean
    Top-200 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_euclidean_Linencoded.tsv
    ⏱️ Execution time: 29.22 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_annoy_euclidean_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_euclidean_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1422
    📊 Evaluation (P / R / F1): {'Precision': 0.7135, 'Recall': 0.69, 'F1': 0.7015}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_annoy_euclidean_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1900089 rows
    ✅ After removing ignored classes: 1900089 rows
    ✅ After keeping only test SrcEntities: 449769 rows
    ✅ After applying threshold ≥ 0.0: 449769 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_euclidean_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2530 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_euclidean_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1782
    📊 Evaluation (P / R / F1): {'P': 0.704, 'R': 0.669, 'F1': 0.686}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_annoy_euclidean_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7122, 'Recall@1': 0.6654, 'F1@1': 0.688}



```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on annoy_euclidean distance
    output_file=f"{results_dir}/{task}_top_200_mappings_annoy_euclidean_mrr_hit_Linencoded.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-200 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_euclidean_mrr_hit_Linencoded.tsv
    ⏱️ Execution time: 10.59 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_annoy_euclidean_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8447749019974694, 'Hits@1': 0.8051070221554638, 'Hits@5': 0.8869695831768682, 'Hits@10': 0.8911002628614345}


# **Transformer Encoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = TransformerEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_TRencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv"
)
```


```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```


```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

# **K=1**


```python
# Compute the top-10 most similar mappings using Euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the Euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_euclidean(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_annoy_euclidean_TRencoded.tsv"
)
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_annoy_euclidean_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_annoy_euclidean_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```


```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_annoy_euclidean_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```


```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_1_mappings_annoy_euclidean_mrr_hit_TRencoded.tsv"
)

```


```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_annoy_euclidean_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.5822873047078311, 'Hits@1': 0.5696582801351859, 'Hits@5': 0.5696582801351859, 'Hits@10': 0.5696582801351859}


# **K=10**


```python
# Compute the top-10 most similar mappings using annoy_euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the annoy_euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_euclidean(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_annoy_euclidean_TRencoded.tsv"
)
```

    🔹 Using Annoy with metric: euclidean
    Top-10 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_euclidean_TRencoded.tsv
    ⏱️ Execution time: 8.25 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_annoy_euclidean_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_euclidean_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1268
    📊 Evaluation (P / R / F1): {'Precision': 0.6362, 'Recall': 0.6152, 'F1': 0.6256}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_annoy_euclidean_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 95659 rows
    ✅ After removing ignored classes: 95659 rows
    ✅ After keeping only test SrcEntities: 22583 rows
    ✅ After applying threshold ≥ 0.0: 22583 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_euclidean_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2525 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_euclidean_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1583
    📊 Evaluation (P / R / F1): {'P': 0.627, 'R': 0.594, 'F1': 0.61}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_annoy_euclidean_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6346, 'Recall@1': 0.5929, 'F1@1': 0.6131}



```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_10_mappings_annoy_euclidean_mrr_hit_TRencoded.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-10 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_euclidean_mrr_hit_TRencoded.tsv
    ⏱️ Execution time: 4.68 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_annoy_euclidean_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.7089664020246961, 'Hits@1': 0.6811866316184754, 'Hits@5': 0.7232444611340594, 'Hits@10': 0.7232444611340594}


# **K=30**


```python
# Compute the top-10 most similar mappings using annoy_euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the annoy_euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_euclidean(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_annoy_euclidean_TRencoded.tsv"
)
```

    🔹 Using Annoy with metric: euclidean
    Top-30 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_euclidean_TRencoded.tsv
    ⏱️ Execution time: 9.89 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_annoy_euclidean_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_euclidean_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1268
    📊 Evaluation (P / R / F1): {'Precision': 0.6362, 'Recall': 0.6152, 'F1': 0.6256}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_annoy_euclidean_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 288281 rows
    ✅ After removing ignored classes: 288281 rows
    ✅ After keeping only test SrcEntities: 67758 rows
    ✅ After applying threshold ≥ 0.0: 67758 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_euclidean_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2525 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_euclidean_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1583
    📊 Evaluation (P / R / F1): {'P': 0.627, 'R': 0.594, 'F1': 0.61}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_annoy_euclidean_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6346, 'Recall@1': 0.5929, 'F1@1': 0.6131}



```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_30_mappings_annoy_euclidean_mrr_hit_TRencoded.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-30 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_euclidean_mrr_hit_TRencoded.tsv
    ⏱️ Execution time: 5.12 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_annoy_euclidean_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.7137923720356095, 'Hits@1': 0.6834397296282388, 'Hits@5': 0.7337589185129553, 'Hits@10': 0.7337589185129553}


# **K=100**


```python
# Compute the top-10 most similar mappings using annoy_euclidean distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the annoy_euclidean distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_euclidean(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_TRencoded.tsv"
)
```

    🔹 Using Annoy with metric: euclidean
    Top-100 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_euclidean_TRencoded.tsv
    ⏱️ Execution time: 18.52 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_euclidean_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1361
    📊 Evaluation (P / R / F1): {'Precision': 0.6829, 'Recall': 0.6604, 'F1': 0.6714}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 966776 rows
    ✅ After removing ignored classes: 966776 rows
    ✅ After keeping only test SrcEntities: 227498 rows
    ✅ After applying threshold ≥ 0.0: 227498 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_euclidean_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2516 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_euclidean_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1697
    📊 Evaluation (P / R / F1): {'P': 0.674, 'R': 0.637, 'F1': 0.655}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6801, 'Recall@1': 0.6354, 'F1@1': 0.657}



```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_mrr_hit_TRencoded.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-100 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_euclidean_mrr_hit_TRencoded.tsv
    ⏱️ Execution time: 8.94 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


    ---------------------------------------------------------------------------

    NameError                                 Traceback (most recent call last)

    <ipython-input-1-861c01d37a1c> in <cell line: 0>()
          1 # Compute MRR and Hits@k metrics
          2 # This function evaluates the predicted rankings against the reference mappings
    ----> 3 results = compute_mrr_and_hits(
          4     reference_file=test_cands,             # Reference file with true ranks
          5     predicted_file=f"{results_dir}/{task}_top_100_mappings_annoy_euclidean_mrr_hit_TRencoded.tsv",             # File containing predicted rankings


    NameError: name 'compute_mrr_and_hits' is not defined



```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.7990284128731151, 'Hits@1': 0.7604205782951559, 'Hits@5': 0.8362748779571911, 'Hits@10': 0.838152459631994}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using annoy_euclidean distance on ResMLP-encoded embeddings, then save the results.

topk_annoy_euclidean(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_annoy_euclidean_TRencoded.tsv"  # Save results to this output file
)
```

    🔹 Using Annoy with metric: euclidean
    Top-200 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_euclidean_TRencoded.tsv
    ⏱️ Execution time: 35.44 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_annoy_euclidean_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_euclidean_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1410
    📊 Evaluation (P / R / F1): {'Precision': 0.7075, 'Recall': 0.6841, 'F1': 0.6956}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_annoy_euclidean_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1942766 rows
    ✅ After removing ignored classes: 1942766 rows
    ✅ After keeping only test SrcEntities: 457382 rows
    ✅ After applying threshold ≥ 0.0: 457382 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_euclidean_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2516 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_euclidean_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1762
    📊 Evaluation (P / R / F1): {'P': 0.7, 'R': 0.662, 'F1': 0.68}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_annoy_euclidean_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7058, 'Recall@1': 0.6594, 'F1@1': 0.6818}



```python
topk_annoy_euclidean(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on Euclidean distance
    output_file=f"{results_dir}/{task}_top_200_mappings_annoy_euclidean_mrr_hit_TRencoded.tsv"
)

```

    🔹 Using Annoy with metric: euclidean
    Top-200 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_euclidean_mrr_hit_TRencoded.tsv
    ⏱️ Execution time: 10.20 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_annoy_euclidean_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8461573976048632, 'Hits@1': 0.8039804731505821, 'Hits@5': 0.8911002628614345, 'Hits@10': 0.8937288772061585}


# **USING ANNOY COSINE**

# **K=1**


```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_1_mappings_annoy_cosine.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-1 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_cosine.tsv
    ⏱️ Execution time: 10.30 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_annoy_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_cosine_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1238
    📊 Evaluation (P / R / F1): {'Precision': 0.6212, 'Recall': 0.6007, 'F1': 0.6108}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_annoy_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9550 rows
    ✅ After removing ignored classes: 9550 rows
    ✅ After keeping only test SrcEntities: 2369 rows
    ✅ After applying threshold ≥ 0.0: 2369 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_cosine_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2369 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_cosine_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1508
    📊 Evaluation (P / R / F1): {'P': 0.637, 'R': 0.566, 'F1': 0.599}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/neoplas_top_1_mappings_annoy_cosine_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6366, 'Recall@1': 0.5958, 'F1@1': 0.6155}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_1_mappings_annoy_cosine_mrr_hit.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-1 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_cosine_mrr_hit.tsv
    ⏱️ Execution time: 4.32 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_annoy_cosine_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.5673395456567827, 'Hits@1': 0.5542621104018025, 'Hits@5': 0.5542621104018025, 'Hits@10': 0.5542621104018025}


# **K=10**


```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_10_mappings_annoy_cosine.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-10 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_cosine.tsv
    ⏱️ Execution time: 8.19 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_annoy_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_cosine_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1238
    📊 Evaluation (P / R / F1): {'Precision': 0.6212, 'Recall': 0.6007, 'F1': 0.6108}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_annoy_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 96252 rows
    ✅ After removing ignored classes: 96252 rows
    ✅ After keeping only test SrcEntities: 22573 rows
    ✅ After applying threshold ≥ 0.0: 22573 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_cosine_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2612 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_cosine_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1542
    📊 Evaluation (P / R / F1): {'P': 0.59, 'R': 0.579, 'F1': 0.585}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_annoy_cosine_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6145, 'Recall@1': 0.5742, 'F1@1': 0.5937}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_10_mappings_annoy_cosine_mrr_hit.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-10 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_cosine_mrr_hit.tsv
    ⏱️ Execution time: 4.81 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_annoy_cosine_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6872488379636772, 'Hits@1': 0.6597822005257229, 'Hits@5': 0.7003379647014645, 'Hits@10': 0.7003379647014645}


# **K=30**


```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_30_mappings_annoy_cosine.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-30 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_cosine.tsv
    ⏱️ Execution time: 10.24 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_annoy_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_cosine_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1245
    📊 Evaluation (P / R / F1): {'Precision': 0.6247, 'Recall': 0.6041, 'F1': 0.6142}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_annoy_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 290010 rows
    ✅ After removing ignored classes: 290010 rows
    ✅ After keeping only test SrcEntities: 67920 rows
    ✅ After applying threshold ≥ 0.0: 67920 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_cosine_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2609 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_cosine_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1552
    📊 Evaluation (P / R / F1): {'P': 0.595, 'R': 0.583, 'F1': 0.589}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_annoy_cosine_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6182, 'Recall@1': 0.5775, 'F1@1': 0.5972}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_30_mappings_annoy_cosine_mrr_hit.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-30 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_cosine_mrr_hit.tsv
    ⏱️ Execution time: 5.58 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_annoy_cosine_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6963353037877898, 'Hits@1': 0.6661659782200525, 'Hits@5': 0.7153586180998873, 'Hits@10': 0.7157341344348479}


# **K=100**


```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_100_mappings_annoy_cosine.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-100 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_cosine.tsv
    ⏱️ Execution time: 19.14 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_annoy_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_cosine_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1336
    📊 Evaluation (P / R / F1): {'Precision': 0.6703, 'Recall': 0.6482, 'F1': 0.6591}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_annoy_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 972423 rows
    ✅ After removing ignored classes: 972423 rows
    ✅ After keeping only test SrcEntities: 227772 rows
    ✅ After applying threshold ≥ 0.0: 227772 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_cosine_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2588 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_cosine_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1682
    📊 Evaluation (P / R / F1): {'P': 0.65, 'R': 0.632, 'F1': 0.641}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_annoy_cosine_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6692, 'Recall@1': 0.6252, 'F1@1': 0.6465}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_100_mappings_annoy_cosine_mrr_hit.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-100 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_cosine_mrr_hit.tsv
    ⏱️ Execution time: 9.31 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_annoy_cosine_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.7860011735885412, 'Hits@1': 0.7495306045812993, 'Hits@5': 0.8193766428839655, 'Hits@10': 0.8212542245587683}


# **K=200**


```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_200_mappings_annoy_cosine.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-200 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_cosine.tsv
    ⏱️ Execution time: 28.34 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_annoy_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_cosine_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1390
    📊 Evaluation (P / R / F1): {'Precision': 0.6974, 'Recall': 0.6744, 'F1': 0.6857}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_annoy_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1952461 rows
    ✅ After removing ignored classes: 1952461 rows
    ✅ After keeping only test SrcEntities: 457977 rows
    ✅ After applying threshold ≥ 0.0: 457977 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_cosine_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2588 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_cosine_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1752
    📊 Evaluation (P / R / F1): {'P': 0.677, 'R': 0.658, 'F1': 0.667}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_annoy_cosine_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6957, 'Recall@1': 0.65, 'F1@1': 0.6721}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_200_mappings_annoy_cosine_mrr_hit.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-200 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_cosine_mrr_hit.tsv
    ⏱️ Execution time: 8.93 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_annoy_cosine_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8354774203163613, 'Hits@1': 0.7938415321066467, 'Hits@5': 0.8809613218174991, 'Hits@10': 0.8839654524971836}


# **K=500**


```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=500,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_500_mappings_annoy_cosine.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-500 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_annoy_cosine.tsv
    ⏱️ Execution time: 63.34 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_500_mappings_annoy_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 5703500 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 996500 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_annoy_cosine_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1428
    📊 Evaluation (P / R / F1): {'Precision': 0.7165, 'Recall': 0.6929, 'F1': 0.7045}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_500_mappings_annoy_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 5703500 rows
    ✅ After removing train-only URIs: 4914884 rows
    ✅ After removing ignored classes: 4914884 rows
    ✅ After keeping only test SrcEntities: 1155179 rows
    ✅ After applying threshold ≥ 0.0: 1155179 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_annoy_cosine_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2573 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_annoy_cosine_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1806
    📊 Evaluation (P / R / F1): {'P': 0.702, 'R': 0.678, 'F1': 0.69}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_500_mappings_annoy_cosine_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.717, 'Recall@1': 0.6699, 'F1@1': 0.6927}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=500,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_500_mappings_annoy_cosine_mrr_hit.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-500 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_annoy_cosine_mrr_hit.tsv
    ⏱️ Execution time: 16.90 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_500_mappings_annoy_cosine_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8773990419302761, 'Hits@1': 0.8313931656027037, 'Hits@5': 0.9305294780322944, 'Hits@10': 0.9354111903867818}


# **K=1000**


```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1000,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_1000_mappings_annoy_cosine.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-1000 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_annoy_cosine.tsv
    ⏱️ Execution time: 112.49 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1000_mappings_annoy_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407000 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993000 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_annoy_cosine_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1444
    📊 Evaluation (P / R / F1): {'Precision': 0.7245, 'Recall': 0.7006, 'F1': 0.7124}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1000_mappings_annoy_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 11407000 rows
    ✅ After removing train-only URIs: 9891996 rows
    ✅ After removing ignored classes: 9891996 rows
    ✅ After keeping only test SrcEntities: 2330013 rows
    ✅ After applying threshold ≥ 0.0: 2330013 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_annoy_cosine_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2576 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_annoy_cosine_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1828
    📊 Evaluation (P / R / F1): {'P': 0.71, 'R': 0.686, 'F1': 0.698}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1000_mappings_annoy_cosine_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7251, 'Recall@1': 0.6774, 'F1@1': 0.7004}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1000,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_1000_mappings_annoy_cosine_mrr_hit.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-1000 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_annoy_cosine_mrr_hit.tsv
    ⏱️ Execution time: 27.09 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1000_mappings_annoy_cosine_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9047100607124008, 'Hits@1': 0.8558017273751408, 'Hits@5': 0.9635749155088247, 'Hits@10': 0.9699586932031543}


# **K=2000**


```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=2000,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_2000_mappings_annoy_cosine.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-2000 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_annoy_cosine.tsv
    ⏱️ Execution time: 210.92 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_2000_mappings_annoy_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 22814000 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 3986000 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_annoy_cosine_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1456
    📊 Evaluation (P / R / F1): {'Precision': 0.7306, 'Recall': 0.7065, 'F1': 0.7183}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_2000_mappings_annoy_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 22814000 rows
    ✅ After removing train-only URIs: 19888129 rows
    ✅ After removing ignored classes: 19888129 rows
    ✅ After keeping only test SrcEntities: 4698277 rows
    ✅ After applying threshold ≥ 0.0: 4698277 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_annoy_cosine_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2574 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_annoy_cosine_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1844
    📊 Evaluation (P / R / F1): {'P': 0.716, 'R': 0.692, 'F1': 0.704}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_2000_mappings_annoy_cosine_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7311, 'Recall@1': 0.6831, 'F1@1': 0.7063}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=2000,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_2000_mappings_annoy_cosine_mrr_hit.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-2000 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_annoy_cosine_mrr_hit.tsv
    ⏱️ Execution time: 57.65 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_2000_mappings_annoy_cosine_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9152261956897951, 'Hits@1': 0.8640630867442733, 'Hits@5': 0.9778445362373264, 'Hits@10': 0.9864814119414195}


# **With Encoders**

# **ResMLPEncoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = ResMLPEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_cands_with_embeddings_ResMLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_ResMLPencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_annoy_cosine_ResMLPencoded.tsv"
)
```

    🔹 Using Annoy with metric: angular
    Top-1 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_cosine_ResMLPencoded.tsv
    ⏱️ Execution time: 6.53 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_annoy_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_cosine_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1235
    📊 Evaluation (P / R / F1): {'Precision': 0.6197, 'Recall': 0.5992, 'F1': 0.6093}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_annoy_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9537 rows
    ✅ After removing ignored classes: 9537 rows
    ✅ After keeping only test SrcEntities: 2374 rows
    ✅ After applying threshold ≥ 0.0: 2374 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_cosine_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2374 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_cosine_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1506
    📊 Evaluation (P / R / F1): {'P': 0.634, 'R': 0.566, 'F1': 0.598}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_annoy_cosine_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6344, 'Recall@1': 0.5936, 'F1@1': 0.6133}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_1_mappings_annoy_cosine_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-1 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_cosine_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 6.50 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_annoy_cosine_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.5826476741654151, 'Hits@1': 0.5700337964701464, 'Hits@5': 0.5700337964701464, 'Hits@10': 0.5700337964701464}


# **K=10**


```python
# Compute the top-10 most similar mappings using cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_annoy_cosine_ResMLPencoded.tsv"
)
```

    🔹 Using Annoy with metric: angular
    Top-10 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_cosine_ResMLPencoded.tsv
    ⏱️ Execution time: 7.66 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_annoy_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_cosine_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1235
    📊 Evaluation (P / R / F1): {'Precision': 0.6197, 'Recall': 0.5992, 'F1': 0.6093}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_annoy_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 96220 rows
    ✅ After removing ignored classes: 96220 rows
    ✅ After keeping only test SrcEntities: 22566 rows
    ✅ After applying threshold ≥ 0.0: 22566 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_cosine_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2582 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_cosine_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1544
    📊 Evaluation (P / R / F1): {'P': 0.598, 'R': 0.58, 'F1': 0.589}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_annoy_cosine_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6145, 'Recall@1': 0.5742, 'F1@1': 0.5937}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_10_mappings_annoy_cosine_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-10 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_cosine_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 4.48 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_annoy_cosine_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.7074817898471346, 'Hits@1': 0.6804355989485542, 'Hits@5': 0.7206158467893353, 'Hits@10': 0.7206158467893353}


# **K=30**


```python
# Compute the top-10 most similar mappings using cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_annoy_cosine_ResMLPencoded.tsv"
)
```

    🔹 Using Annoy with metric: angular
    Top-30 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_cosine_ResMLPencoded.tsv
    ⏱️ Execution time: 9.26 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_annoy_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_cosine_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1241
    📊 Evaluation (P / R / F1): {'Precision': 0.6227, 'Recall': 0.6021, 'F1': 0.6122}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_annoy_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 289888 rows
    ✅ After removing ignored classes: 289888 rows
    ✅ After keeping only test SrcEntities: 67936 rows
    ✅ After applying threshold ≥ 0.0: 67936 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_cosine_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2582 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_cosine_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1552
    📊 Evaluation (P / R / F1): {'P': 0.601, 'R': 0.583, 'F1': 0.592}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_annoy_cosine_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6178, 'Recall@1': 0.5772, 'F1@1': 0.5968}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_30_mappings_annoy_cosine_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-30 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_cosine_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 5.20 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_annoy_cosine_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.716443494477051, 'Hits@1': 0.6871948929778445, 'Hits@5': 0.734885467517837, 'Hits@10': 0.734885467517837}


# **K=100**


```python
# Compute the top-10 most similar mappings using cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_annoy_cosine_ResMLPencoded.tsv"
)
```

    🔹 Using Annoy with metric: angular
    Top-100 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_cosine_ResMLPencoded.tsv
    ⏱️ Execution time: 17.09 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_annoy_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_cosine_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1332
    📊 Evaluation (P / R / F1): {'Precision': 0.6683, 'Recall': 0.6463, 'F1': 0.6571}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_annoy_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 972006 rows
    ✅ After removing ignored classes: 972006 rows
    ✅ After keeping only test SrcEntities: 227814 rows
    ✅ After applying threshold ≥ 0.0: 227814 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_cosine_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2586 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_cosine_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1678
    📊 Evaluation (P / R / F1): {'P': 0.649, 'R': 0.63, 'F1': 0.639}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_annoy_cosine_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6648, 'Recall@1': 0.6211, 'F1@1': 0.6422}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_100_mappings_annoy_cosine_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-100 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_cosine_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 6.68 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_annoy_cosine_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.7864795177545006, 'Hits@1': 0.7510326699211416, 'Hits@5': 0.8193766428839655, 'Hits@10': 0.8201276755538865}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using cosine distance on ResMLP-encoded embeddings, then save the results.

topk_annoy_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_annoy_cosine_ResMLPencoded.tsv"  # Save results to this output file
)
```

    🔹 Using Annoy with metric: angular
    Top-200 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_cosine_ResMLPencoded.tsv
    ⏱️ Execution time: 27.58 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_annoy_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_cosine_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1391
    📊 Evaluation (P / R / F1): {'Precision': 0.6979, 'Recall': 0.6749, 'F1': 0.6862}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_annoy_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1953132 rows
    ✅ After removing ignored classes: 1953132 rows
    ✅ After keeping only test SrcEntities: 458208 rows
    ✅ After applying threshold ≥ 0.0: 458208 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_cosine_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2577 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_cosine_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1755
    📊 Evaluation (P / R / F1): {'P': 0.681, 'R': 0.659, 'F1': 0.67}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_annoy_cosine_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6949, 'Recall@1': 0.6493, 'F1@1': 0.6713}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_200_mappings_annoy_cosine_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-200 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_cosine_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 9.68 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_annoy_cosine_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8344448823337184, 'Hits@1': 0.79609463011641, 'Hits@5': 0.8749530604581299, 'Hits@10': 0.8768306421329328}


# **MLPEncoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = MLPEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_MLPencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_MLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_cands_with_embeddings_MLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_MLPencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_annoy_cosine_MLPencoded.tsv"
)
```

    🔹 Using Annoy with metric: angular
    Top-1 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_cosine_MLPencoded.tsv
    ⏱️ Execution time: 9.51 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_annoy_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_cosine_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1184
    📊 Evaluation (P / R / F1): {'Precision': 0.5941, 'Recall': 0.5745, 'F1': 0.5841}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_annoy_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9589 rows
    ✅ After removing ignored classes: 9589 rows
    ✅ After keeping only test SrcEntities: 2363 rows
    ✅ After applying threshold ≥ 0.0: 2363 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_cosine_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2363 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_cosine_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1443
    📊 Evaluation (P / R / F1): {'P': 0.611, 'R': 0.542, 'F1': 0.574}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_annoy_cosine_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6107, 'Recall@1': 0.5717, 'F1@1': 0.5905}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_1_mappings_annoy_cosine_mrr_hit_MLPencoded.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-1 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_cosine_mrr_hit_MLPencoded.tsv
    ⏱️ Execution time: 4.09 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_annoy_cosine_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.5498461645266358, 'Hits@1': 0.536237326323695, 'Hits@5': 0.536237326323695, 'Hits@10': 0.536237326323695}


# **K=10**


```python
# Compute the top-10 most similar mappings using cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_annoy_cosine_MLPencoded.tsv"
)
```

    🔹 Using Annoy with metric: angular
    Top-10 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_cosine_MLPencoded.tsv
    ⏱️ Execution time: 7.74 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_annoy_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_cosine_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1184
    📊 Evaluation (P / R / F1): {'Precision': 0.5941, 'Recall': 0.5745, 'F1': 0.5841}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_annoy_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 96615 rows
    ✅ After removing ignored classes: 96615 rows
    ✅ After keeping only test SrcEntities: 22635 rows
    ✅ After applying threshold ≥ 0.0: 22635 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_cosine_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2631 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_cosine_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1484
    📊 Evaluation (P / R / F1): {'P': 0.564, 'R': 0.557, 'F1': 0.561}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_annoy_cosine_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.5896, 'Recall@1': 0.5509, 'F1@1': 0.5696}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_10_mappings_annoy_cosine_mrr_hit_MLPencoded.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-10 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_cosine_mrr_hit_MLPencoded.tsv
    ⏱️ Execution time: 4.43 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_annoy_cosine_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.7074817898471346, 'Hits@1': 0.6804355989485542, 'Hits@5': 0.7206158467893353, 'Hits@10': 0.7206158467893353}


# **K=30**


```python
# Compute the top-10 most similar mappings using cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_annoy_cosine_MLPencoded.tsv"
)
```

    🔹 Using Annoy with metric: angular
    Top-30 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_cosine_MLPencoded.tsv
    ⏱️ Execution time: 9.37 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_annoy_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_cosine_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1190
    📊 Evaluation (P / R / F1): {'Precision': 0.5971, 'Recall': 0.5774, 'F1': 0.5871}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_annoy_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 290774 rows
    ✅ After removing ignored classes: 290774 rows
    ✅ After keeping only test SrcEntities: 68001 rows
    ✅ After applying threshold ≥ 0.0: 68001 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_cosine_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2629 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_cosine_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1491
    📊 Evaluation (P / R / F1): {'P': 0.567, 'R': 0.56, 'F1': 0.563}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_annoy_cosine_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.5924, 'Recall@1': 0.5535, 'F1@1': 0.5723}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_30_mappings_annoy_cosine_mrr_hit_MLPencoded.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-30 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_cosine_mrr_hit_MLPencoded.tsv
    ⏱️ Execution time: 5.22 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_annoy_cosine_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6782611479920697, 'Hits@1': 0.6488922268118663, 'Hits@5': 0.6950807360120165, 'Hits@10': 0.6958317686819376}


# **K=100**


```python
# Compute the top-10 most similar mappings using cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_annoy_cosine_MLPencoded.tsv"
)
```

    🔹 Using Annoy with metric: angular
    Top-100 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_cosine_MLPencoded.tsv
    ⏱️ Execution time: 17.43 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_annoy_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_cosine_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1325
    📊 Evaluation (P / R / F1): {'Precision': 0.6648, 'Recall': 0.6429, 'F1': 0.6537}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_annoy_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 975112 rows
    ✅ After removing ignored classes: 975112 rows
    ✅ After keeping only test SrcEntities: 228788 rows
    ✅ After applying threshold ≥ 0.0: 228788 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_cosine_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2615 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_cosine_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1668
    📊 Evaluation (P / R / F1): {'P': 0.638, 'R': 0.626, 'F1': 0.632}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_annoy_cosine_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6616, 'Recall@1': 0.6181, 'F1@1': 0.6391}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_100_mappings_annoy_cosine_mrr_hit_MLPencoded.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-100 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_cosine_mrr_hit_MLPencoded.tsv
    ⏱️ Execution time: 6.56 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_annoy_cosine_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.7748548094147608, 'Hits@1': 0.7405182125422456, 'Hits@5': 0.8069846038302666, 'Hits@10': 0.8081111528351483}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using cosine distance on ResMLP-encoded embeddings, then save the results.

topk_annoy_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_annoy_cosine_MLPencoded.tsv"  # Save results to this output file
)
```

    🔹 Using Annoy with metric: angular
    Top-200 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_cosine_MLPencoded.tsv
    ⏱️ Execution time: 28.71 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_annoy_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_cosine_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1376
    📊 Evaluation (P / R / F1): {'Precision': 0.6904, 'Recall': 0.6676, 'F1': 0.6788}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_annoy_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1957903 rows
    ✅ After removing ignored classes: 1957903 rows
    ✅ After keeping only test SrcEntities: 459927 rows
    ✅ After applying threshold ≥ 0.0: 459927 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_cosine_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2613 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_cosine_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1739
    📊 Evaluation (P / R / F1): {'P': 0.666, 'R': 0.653, 'F1': 0.659}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_annoy_cosine_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6885, 'Recall@1': 0.6433, 'F1@1': 0.6651}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_200_mappings_annoy_cosine_mrr_hit_MLPencoded.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-200 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_cosine_mrr_hit_MLPencoded.tsv
    ⏱️ Execution time: 10.23 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_annoy_cosine_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8375040146871465, 'Hits@1': 0.7994742771310552, 'Hits@5': 0.8794592564776568, 'Hits@10': 0.8832144198272625}


# **LinearEncoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = LinearEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_Linencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_Linencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_cands_with_embeddings_Linencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_Linencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_annoy_cosine_Linencoded.tsv"
)
```

    🔹 Using Annoy with metric: angular
    Top-1 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_cosine_Linencoded.tsv
    ⏱️ Execution time: 6.58 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_annoy_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_cosine_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1210
    📊 Evaluation (P / R / F1): {'Precision': 0.6071, 'Recall': 0.5871, 'F1': 0.5969}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_annoy_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9590 rows
    ✅ After removing ignored classes: 9590 rows
    ✅ After keeping only test SrcEntities: 2363 rows
    ✅ After applying threshold ≥ 0.0: 2363 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_cosine_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2363 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_cosine_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1481
    📊 Evaluation (P / R / F1): {'P': 0.627, 'R': 0.556, 'F1': 0.589}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_annoy_cosine_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6267, 'Recall@1': 0.5854, 'F1@1': 0.6054}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_1_mappings_cosine_mrr_hit_Linencoded.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-1 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_cosine_mrr_hit_Linencoded.tsv
    ⏱️ Execution time: 4.14 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_annoy_cosine_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.5607834722322942, 'Hits@1': 0.5475028163725122, 'Hits@5': 0.5475028163725122, 'Hits@10': 0.5475028163725122}


# **K=10**


```python
# Compute the top-10 most similar mappings using cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_annoy_cosine_Linencoded.tsv"
)
```

    🔹 Using Annoy with metric: angular
    Top-10 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_cosine_Linencoded.tsv
    ⏱️ Execution time: 7.57 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_annoy_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_cosine_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1210
    📊 Evaluation (P / R / F1): {'Precision': 0.6071, 'Recall': 0.5871, 'F1': 0.5969}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_annoy_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 96810 rows
    ✅ After removing ignored classes: 96810 rows
    ✅ After keeping only test SrcEntities: 22650 rows
    ✅ After applying threshold ≥ 0.0: 22650 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_cosine_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2596 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_cosine_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1524
    📊 Evaluation (P / R / F1): {'P': 0.587, 'R': 0.572, 'F1': 0.58}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_annoy_cosine_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6077, 'Recall@1': 0.5678, 'F1@1': 0.5871}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_10_mappings_annoy_cosine_mrr_hit_Linencoded.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-10 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_cosine_mrr_hit_Linencoded.tsv
    ⏱️ Execution time: 4.33 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_annoy_cosine_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6815111781374843, 'Hits@1': 0.6541494555013143, 'Hits@5': 0.6935786706721743, 'Hits@10': 0.6935786706721743}


# **K=30**


```python
# Compute the top-10 most similar mappings using annoy_cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the annoy_cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_annoy_cosine_Linencoded.tsv"
)
```

    🔹 Using Annoy with metric: angular
    Top-30 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_cosine_Linencoded.tsv
    ⏱️ Execution time: 9.29 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_annoy_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_cosine_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1217
    📊 Evaluation (P / R / F1): {'Precision': 0.6106, 'Recall': 0.5905, 'F1': 0.6004}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_annoy_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 291570 rows
    ✅ After removing ignored classes: 291570 rows
    ✅ After keeping only test SrcEntities: 68047 rows
    ✅ After applying threshold ≥ 0.0: 68047 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_cosine_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2596 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_cosine_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1532
    📊 Evaluation (P / R / F1): {'P': 0.59, 'R': 0.575, 'F1': 0.583}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_annoy_cosine_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6105, 'Recall@1': 0.5704, 'F1@1': 0.5898}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_30_mappings_annoy_cosine_mrr_hit_Linencoded.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-30 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_cosine_mrr_hit_Linencoded.tsv
    ⏱️ Execution time: 5.44 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_annoy_cosine_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6890210675998, 'Hits@1': 0.6594066841907623, 'Hits@5': 0.7055951933909125, 'Hits@10': 0.7055951933909125}


# **K=100**


```python
# Compute the top-10 most similar mappings using annoy_cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the annoy_cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_annoy_cosine_Linencoded.tsv"
)
```

    🔹 Using Annoy with metric: angular
    Top-100 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_cosine_Linencoded.tsv
    ⏱️ Execution time: 17.00 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_annoy_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_cosine_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1323
    📊 Evaluation (P / R / F1): {'Precision': 0.6638, 'Recall': 0.6419, 'F1': 0.6527}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_annoy_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 977457 rows
    ✅ After removing ignored classes: 977457 rows
    ✅ After keeping only test SrcEntities: 228604 rows
    ✅ After applying threshold ≥ 0.0: 228604 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_cosine_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2584 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_cosine_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1672
    📊 Evaluation (P / R / F1): {'P': 0.647, 'R': 0.628, 'F1': 0.637}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_annoy_cosine_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6652, 'Recall@1': 0.6215, 'F1@1': 0.6426}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_100_mappings_annoy_cosine_mrr_hit_Linencoded.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-100 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_cosine_mrr_hit_Linencoded.tsv
    ⏱️ Execution time: 6.49 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_annoy_cosine_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.7729545085769328, 'Hits@1': 0.7393916635373639, 'Hits@5': 0.8017273751408186, 'Hits@10': 0.8021028914757792}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using annoy_cosine distance on ResMLP-encoded embeddings, then save the results.

topk_annoy_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_annoy_cosine_Linencoded.tsv"  # Save results to this output file
)
```

    🔹 Using Annoy with metric: angular
    Top-200 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_cosine_Linencoded.tsv
    ⏱️ Execution time: 27.70 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_annoy_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_cosine_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1381
    📊 Evaluation (P / R / F1): {'Precision': 0.6929, 'Recall': 0.6701, 'F1': 0.6813}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_annoy_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1961653 rows
    ✅ After removing ignored classes: 1961653 rows
    ✅ After keeping only test SrcEntities: 459654 rows
    ✅ After applying threshold ≥ 0.0: 459654 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_cosine_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2583 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_cosine_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1745
    📊 Evaluation (P / R / F1): {'P': 0.676, 'R': 0.655, 'F1': 0.665}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_annoy_cosine_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6933, 'Recall@1': 0.6478, 'F1@1': 0.6698}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on annoy_cosine distance
    output_file=f"{results_dir}/{task}_top_200_mappings_annoy_cosine_mrr_hit_Linencoded.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-200 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_cosine_mrr_hit_Linencoded.tsv
    ⏱️ Execution time: 9.58 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_annoy_cosine_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8252375117786712, 'Hits@1': 0.7874577544123169, 'Hits@5': 0.8633120540743522, 'Hits@10': 0.8648141194141945}


# **Transformer Encoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = TransformerEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_TRencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_TRencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_cands_with_embeddings_TRencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_TRencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_annoy_cosine_TRencoded.tsv"
)
```

    🔹 Using Annoy with metric: angular
    Top-1 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_cosine_TRencoded.tsv
    ⏱️ Execution time: 6.51 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_annoy_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_cosine_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1202
    📊 Evaluation (P / R / F1): {'Precision': 0.6031, 'Recall': 0.5832, 'F1': 0.593}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_annoy_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9566 rows
    ✅ After removing ignored classes: 9566 rows
    ✅ After keeping only test SrcEntities: 2382 rows
    ✅ After applying threshold ≥ 0.0: 2382 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_cosine_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2382 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_cosine_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1476
    📊 Evaluation (P / R / F1): {'P': 0.62, 'R': 0.554, 'F1': 0.585}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_annoy_cosine_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6196, 'Recall@1': 0.5793, 'F1@1': 0.5988}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_1_mappings_annoy_cosine_mrr_hit_TRencoded.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-1 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_annoy_cosine_mrr_hit_TRencoded.tsv
    ⏱️ Execution time: 3.82 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_annoy_cosine_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.5666121799827623, 'Hits@1': 0.5535110777318814, 'Hits@5': 0.5535110777318814, 'Hits@10': 0.5535110777318814}


# **K=10**


```python
# Compute the top-10 most similar mappings using annoy_cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the annoy_cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_annoy_cosine_TRencoded.tsv"
)
```

    🔹 Using Annoy with metric: angular
    Top-10 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_cosine_TRencoded.tsv
    ⏱️ Execution time: 8.37 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_annoy_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_cosine_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1202
    📊 Evaluation (P / R / F1): {'Precision': 0.6031, 'Recall': 0.5832, 'F1': 0.593}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_annoy_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 96533 rows
    ✅ After removing ignored classes: 96533 rows
    ✅ After keeping only test SrcEntities: 22565 rows
    ✅ After applying threshold ≥ 0.0: 22565 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_cosine_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2596 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_cosine_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1508
    📊 Evaluation (P / R / F1): {'P': 0.581, 'R': 0.566, 'F1': 0.573}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_annoy_cosine_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6029, 'Recall@1': 0.5633, 'F1@1': 0.5824}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_10_mappings_annoy_cosine_mrr_hit_TRencoded.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-10 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_annoy_cosine_mrr_hit_TRencoded.tsv
    ⏱️ Execution time: 4.40 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_annoy_cosine_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6890542329874946, 'Hits@1': 0.6635373638753286, 'Hits@5': 0.6999624483665039, 'Hits@10': 0.6999624483665039}


# **K=30**


```python
# Compute the top-10 most similar mappings using annoy_cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the annoy_cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_annoy_cosine_TRencoded.tsv"
)
```

    🔹 Using Annoy with metric: angular
    Top-30 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_cosine_TRencoded.tsv
    ⏱️ Execution time: 9.30 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_annoy_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_cosine_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1210
    📊 Evaluation (P / R / F1): {'Precision': 0.6071, 'Recall': 0.5871, 'F1': 0.5969}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_annoy_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 290724 rows
    ✅ After removing ignored classes: 290724 rows
    ✅ After keeping only test SrcEntities: 67939 rows
    ✅ After applying threshold ≥ 0.0: 67939 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_cosine_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2592 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_cosine_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1517
    📊 Evaluation (P / R / F1): {'P': 0.585, 'R': 0.57, 'F1': 0.577}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_annoy_cosine_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6065, 'Recall@1': 0.5667, 'F1@1': 0.5859}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_30_mappings_annoy_cosine_mrr_hit_TRencoded.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-30 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_annoy_cosine_mrr_hit_TRencoded.tsv
    ⏱️ Execution time: 5.57 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_annoy_cosine_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6995934229594335, 'Hits@1': 0.67104769057454, 'Hits@5': 0.7164851671047691, 'Hits@10': 0.7164851671047691}


# **K=100**


```python
# Compute the top-10 most similar mappings using annoy_cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the annoy_cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_annoy_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_annoy_cosine_TRencoded.tsv"
)
```

    🔹 Using Annoy with metric: angular
    Top-100 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_cosine_TRencoded.tsv
    ⏱️ Execution time: 17.12 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_annoy_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_cosine_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1300
    📊 Evaluation (P / R / F1): {'Precision': 0.6523, 'Recall': 0.6308, 'F1': 0.6413}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_annoy_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 975472 rows
    ✅ After removing ignored classes: 975472 rows
    ✅ After keeping only test SrcEntities: 228173 rows
    ✅ After applying threshold ≥ 0.0: 228173 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_cosine_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2590 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_cosine_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1641
    📊 Evaluation (P / R / F1): {'P': 0.634, 'R': 0.616, 'F1': 0.625}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_annoy_cosine_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6559, 'Recall@1': 0.6128, 'F1@1': 0.6337}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_100_mappings_annoy_cosine_mrr_hit_TRencoded.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-100 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_annoy_cosine_mrr_hit_TRencoded.tsv
    ⏱️ Execution time: 6.57 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_annoy_cosine_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.7857644659682671, 'Hits@1': 0.7517837025910628, 'Hits@5': 0.8163725122042809, 'Hits@10': 0.8174990612091626}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using annoy_cosine distance on ResMLP-encoded embeddings, then save the results.

topk_annoy_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_annoy_cosine_TRencoded.tsv"  # Save results to this output file
)
```

    🔹 Using Annoy with metric: angular
    Top-200 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_cosine_TRencoded.tsv
    ⏱️ Execution time: 27.86 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_annoy_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_cosine_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1369
    📊 Evaluation (P / R / F1): {'Precision': 0.6869, 'Recall': 0.6642, 'F1': 0.6754}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_annoy_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1958646 rows
    ✅ After removing ignored classes: 1958646 rows
    ✅ After keeping only test SrcEntities: 458790 rows
    ✅ After applying threshold ≥ 0.0: 458790 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_cosine_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2581 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_cosine_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1728
    📊 Evaluation (P / R / F1): {'P': 0.67, 'R': 0.649, 'F1': 0.659}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_annoy_cosine_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6893, 'Recall@1': 0.644, 'F1@1': 0.6659}



```python
topk_annoy_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_200_mappings_annoy_cosine_mrr_hit_TRencoded.tsv"
)

```

    🔹 Using Annoy with metric: angular
    Top-200 Annoy similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_annoy_cosine_mrr_hit_TRencoded.tsv
    ⏱️ Execution time: 10.18 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_annoy_cosine_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8358681277218727, 'Hits@1': 0.7983477281261735, 'Hits@5': 0.8753285767930905, 'Hits@10': 0.8775816748028539}



```python

```


```python

```

# **Using faiss: topk_faiss_cosine**

# **K=1**


```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_cosine.tsv"
)

```

    ✅ Top-1 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_cosine.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_cosine_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1456
    📊 Evaluation (P / R / F1): {'Precision': 0.7306, 'Recall': 0.7065, 'F1': 0.7183}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9455 rows
    ✅ After removing ignored classes: 9455 rows
    ✅ After keeping only test SrcEntities: 2398 rows
    ✅ After applying threshold ≥ 0.0: 2398 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_cosine_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2398 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_cosine_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1776
    📊 Evaluation (P / R / F1): {'P': 0.741, 'R': 0.667, 'F1': 0.702}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/neoplas_top_1_mappings_faiss_cosine_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7406, 'Recall@1': 0.6929, 'F1@1': 0.716}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_cosine_mrr_hit.tsv"
)

```

    ✅ Top-1 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_cosine_mrr_hit.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_faiss_cosine_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6766873779177401, 'Hits@1': 0.6669170108899737, 'Hits@5': 0.6669170108899737, 'Hits@10': 0.6669170108899737}


# **K=10**


```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_cosine.tsv"
)

```

    ✅ Top-10 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_cosine.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_cosine_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1456
    📊 Evaluation (P / R / F1): {'Precision': 0.7306, 'Recall': 0.7065, 'F1': 0.7183}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 95994 rows
    ✅ After removing ignored classes: 95994 rows
    ✅ After keeping only test SrcEntities: 22625 rows
    ✅ After applying threshold ≥ 0.0: 22625 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_cosine_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2560 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_cosine_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1845
    📊 Evaluation (P / R / F1): {'P': 0.721, 'R': 0.693, 'F1': 0.706}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_faiss_cosine_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7315, 'Recall@1': 0.6834, 'F1@1': 0.7067}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_cosine_mrr_hit.tsv"
)

```

    ✅ Top-10 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_cosine_mrr_hit.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_faiss_cosine_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.899072574208393, 'Hits@1': 0.8588058580548253, 'Hits@5': 0.9421704844160721, 'Hits@10': 0.9421704844160721}


# **K=30**


```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_cosine.tsv"
)

```

    ✅ Top-30 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_cosine.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_cosine_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1456
    📊 Evaluation (P / R / F1): {'Precision': 0.7306, 'Recall': 0.7065, 'F1': 0.7183}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 290963 rows
    ✅ After removing ignored classes: 290963 rows
    ✅ After keeping only test SrcEntities: 68393 rows
    ✅ After applying threshold ≥ 0.0: 68393 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_cosine_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2560 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_cosine_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1845
    📊 Evaluation (P / R / F1): {'P': 0.721, 'R': 0.693, 'F1': 0.706}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_faiss_cosine_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7315, 'Recall@1': 0.6834, 'F1@1': 0.7067}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_cosine_mrr_hit.tsv"
)

```

    ✅ Top-30 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_cosine_mrr_hit.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_faiss_cosine_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9166463442993463, 'Hits@1': 0.8689447990987608, 'Hits@5': 0.9733383402177995, 'Hits@10': 0.9767179872324446}


# **K=100**


```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_cosine.tsv"
)

```

    ✅ Top-100 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_cosine.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_cosine_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1456
    📊 Evaluation (P / R / F1): {'Precision': 0.7306, 'Recall': 0.7065, 'F1': 0.7183}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 979790 rows
    ✅ After removing ignored classes: 979790 rows
    ✅ After keeping only test SrcEntities: 230431 rows
    ✅ After applying threshold ≥ 0.0: 230431 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_cosine_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2560 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_cosine_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1845
    📊 Evaluation (P / R / F1): {'P': 0.721, 'R': 0.693, 'F1': 0.706}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_faiss_cosine_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7315, 'Recall@1': 0.6834, 'F1@1': 0.7067}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_cosine_mrr_hit.tsv"
)

```

    ✅ Top-100 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_cosine_mrr_hit.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_faiss_cosine_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9213368575296516, 'Hits@1': 0.8700713481036425, 'Hits@5': 0.9846038302666166, 'Hits@10': 0.992114156965828}


# **K=200**


```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_cosine.tsv"
)

```

    ✅ Top-200 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_cosine.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_cosine_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1456
    📊 Evaluation (P / R / F1): {'Precision': 0.7306, 'Recall': 0.7065, 'F1': 0.7183}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1967850 rows
    ✅ After removing ignored classes: 1967850 rows
    ✅ After keeping only test SrcEntities: 462857 rows
    ✅ After applying threshold ≥ 0.0: 462857 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_cosine_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2560 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_cosine_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1845
    📊 Evaluation (P / R / F1): {'P': 0.721, 'R': 0.693, 'F1': 0.706}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_faiss_cosine_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7315, 'Recall@1': 0.6834, 'F1@1': 0.7067}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_cosine_mrr_hit.tsv"
)

```

    ✅ Top-200 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_cosine_mrr_hit.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_faiss_cosine_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9218645742472368, 'Hits@1': 0.8700713481036425, 'Hits@5': 0.9857303792714983, 'Hits@10': 0.9936162223056703}


# **K=500**


```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=500,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_500_mappings_faiss_cosine.tsv"
)

```

    ✅ Top-500 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_faiss_cosine.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_500_mappings_faiss_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 5703500 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 996500 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_faiss_cosine_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1456
    📊 Evaluation (P / R / F1): {'Precision': 0.7306, 'Recall': 0.7065, 'F1': 0.7183}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_500_mappings_faiss_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 5703500 rows
    ✅ After removing train-only URIs: 4943894 rows
    ✅ After removing ignored classes: 4943894 rows
    ✅ After keeping only test SrcEntities: 1164359 rows
    ✅ After applying threshold ≥ 0.0: 1164359 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_faiss_cosine_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2560 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_faiss_cosine_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1845
    📊 Evaluation (P / R / F1): {'P': 0.721, 'R': 0.693, 'F1': 0.706}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_500_mappings_faiss_cosine_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7315, 'Recall@1': 0.6834, 'F1@1': 0.7067}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=500,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_500_mappings_faiss_cosine_mrr_hit.tsv"
)

```

    ✅ Top-500 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_faiss_cosine_mrr_hit.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_500_mappings_faiss_cosine_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9223080312033205, 'Hits@1': 0.8700713481036425, 'Hits@5': 0.98685692827638, 'Hits@10': 0.9951182876455126}


# **K=1000**


```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1000,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_1000_mappings_faiss_cosine.tsv"
)

```

    ✅ Top-1000 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_faiss_cosine.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1000_mappings_faiss_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407000 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993000 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_faiss_cosine_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1456
    📊 Evaluation (P / R / F1): {'Precision': 0.7306, 'Recall': 0.7065, 'F1': 0.7183}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1000_mappings_faiss_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 11407000 rows
    ✅ After removing train-only URIs: 9925625 rows
    ✅ After removing ignored classes: 9925625 rows
    ✅ After keeping only test SrcEntities: 2341451 rows
    ✅ After applying threshold ≥ 0.0: 2341451 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_faiss_cosine_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2560 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_faiss_cosine_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1845
    📊 Evaluation (P / R / F1): {'P': 0.721, 'R': 0.693, 'F1': 0.706}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1000_mappings_faiss_cosine_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7315, 'Recall@1': 0.6834, 'F1@1': 0.7067}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1000,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_1000_mappings_faiss_cosine_mrr_hit.tsv"
)

```

    ✅ Top-1000 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_faiss_cosine_mrr_hit.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1000_mappings_faiss_cosine_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9223326529703382, 'Hits@1': 0.8700713481036425, 'Hits@5': 0.98685692827638, 'Hits@10': 0.9954938039804732}


# **K=2000**


```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=2000,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_2000_mappings_faiss_cosine.tsv"
)

```

    ✅ Top-2000 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_faiss_cosine.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_2000_mappings_faiss_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 22814000 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 3986000 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_faiss_cosine_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1456
    📊 Evaluation (P / R / F1): {'Precision': 0.7306, 'Recall': 0.7065, 'F1': 0.7183}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_2000_mappings_faiss_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 22814000 rows
    ✅ After removing train-only URIs: 19917638 rows
    ✅ After removing ignored classes: 19917638 rows
    ✅ After keeping only test SrcEntities: 4707389 rows
    ✅ After applying threshold ≥ 0.0: 4707389 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_faiss_cosine_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2560 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_faiss_cosine_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1845
    📊 Evaluation (P / R / F1): {'P': 0.721, 'R': 0.693, 'F1': 0.706}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_2000_mappings_faiss_cosine_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7315, 'Recall@1': 0.6834, 'F1@1': 0.7067}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=2000,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_2000_mappings_faiss_cosine_mrr_hit.tsv"
)

```

    ✅ Top-2000 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_faiss_cosine_mrr_hit.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_2000_mappings_faiss_cosine_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9225189724532561, 'Hits@1': 0.8700713481036425, 'Hits@5': 0.98685692827638, 'Hits@10': 0.997371385655276}


# **With Encoders**

# **ResMLPEncoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = ResMLPEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_cands_with_embeddings_ResMLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_ResMLPencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_cosine_ResMLPencoded.tsv"
)
```

    ✅ Top-1 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_cosine_ResMLPencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_cosine_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1457
    📊 Evaluation (P / R / F1): {'Precision': 0.7311, 'Recall': 0.7069, 'F1': 0.7188}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9457 rows
    ✅ After removing ignored classes: 9457 rows
    ✅ After keeping only test SrcEntities: 2399 rows
    ✅ After applying threshold ≥ 0.0: 2399 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_cosine_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2399 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_cosine_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1776
    📊 Evaluation (P / R / F1): {'P': 0.74, 'R': 0.667, 'F1': 0.702}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_faiss_cosine_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7403, 'Recall@1': 0.6921, 'F1@1': 0.7154}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_cosine_mrr_hit_ResMLPencoded.tsv"
)

```

    ✅ Top-1 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_cosine_mrr_hit_ResMLPencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_faiss_cosine_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6766873779177401, 'Hits@1': 0.6669170108899737, 'Hits@5': 0.6669170108899737, 'Hits@10': 0.6669170108899737}


# **K=10**


```python
# Compute the top-10 most similar mappings using cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_cosine_ResMLPencoded.tsv"
)
```

    ✅ Top-10 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_cosine_ResMLPencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_cosine_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1457
    📊 Evaluation (P / R / F1): {'Precision': 0.7311, 'Recall': 0.7069, 'F1': 0.7188}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 95948 rows
    ✅ After removing ignored classes: 95948 rows
    ✅ After keeping only test SrcEntities: 22620 rows
    ✅ After applying threshold ≥ 0.0: 22620 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_cosine_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2564 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_cosine_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1844
    📊 Evaluation (P / R / F1): {'P': 0.719, 'R': 0.692, 'F1': 0.706}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_faiss_cosine_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7303, 'Recall@1': 0.6823, 'F1@1': 0.7055}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_cosine_mrr_hit_ResMLPencoded.tsv"
)

```

    ✅ Top-10 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_cosine_mrr_hit_ResMLPencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_faiss_cosine_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8990013817999427, 'Hits@1': 0.8595568907247465, 'Hits@5': 0.9414194517461509, 'Hits@10': 0.9414194517461509}


# **K=30**


```python
# Compute the top-10 most similar mappings using cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_cosine_ResMLPencoded.tsv"
)
```

    ✅ Top-30 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_cosine_ResMLPencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_cosine_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1457
    📊 Evaluation (P / R / F1): {'Precision': 0.7311, 'Recall': 0.7069, 'F1': 0.7188}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 291017 rows
    ✅ After removing ignored classes: 291017 rows
    ✅ After keeping only test SrcEntities: 68440 rows
    ✅ After applying threshold ≥ 0.0: 68440 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_cosine_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2564 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_cosine_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1844
    📊 Evaluation (P / R / F1): {'P': 0.719, 'R': 0.692, 'F1': 0.706}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_faiss_cosine_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7303, 'Recall@1': 0.6823, 'F1@1': 0.7055}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_cosine_mrr_hit_ResMLPencoded.tsv"
)

```

    ✅ Top-30 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_cosine_mrr_hit_ResMLPencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_faiss_cosine_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9169714140545961, 'Hits@1': 0.8704468644386031, 'Hits@5': 0.9725873075478784, 'Hits@10': 0.9752159218926023}


# **K=100**


```python
# Compute the top-10 most similar mappings using cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_cosine_ResMLPencoded.tsv"
)
```

    ✅ Top-100 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_cosine_ResMLPencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_cosine_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1457
    📊 Evaluation (P / R / F1): {'Precision': 0.7311, 'Recall': 0.7069, 'F1': 0.7188}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 979502 rows
    ✅ After removing ignored classes: 979502 rows
    ✅ After keeping only test SrcEntities: 230342 rows
    ✅ After applying threshold ≥ 0.0: 230342 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_cosine_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2564 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_cosine_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1844
    📊 Evaluation (P / R / F1): {'P': 0.719, 'R': 0.692, 'F1': 0.706}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_faiss_cosine_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7303, 'Recall@1': 0.6823, 'F1@1': 0.7055}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_cosine_mrr_hit_ResMLPencoded.tsv"
)

```

    ✅ Top-100 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_cosine_mrr_hit_ResMLPencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_faiss_cosine_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9225781361522407, 'Hits@1': 0.872324446113406, 'Hits@5': 0.9846038302666166, 'Hits@10': 0.9913631242959069}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using cosine distance on ResMLP-encoded embeddings, then save the results.

topk_faiss_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_cosine_ResMLPencoded.tsv"  # Save results to this output file
)
```

    ✅ Top-200 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_cosine_ResMLPencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_cosine_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1457
    📊 Evaluation (P / R / F1): {'Precision': 0.7311, 'Recall': 0.7069, 'F1': 0.7188}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1967499 rows
    ✅ After removing ignored classes: 1967499 rows
    ✅ After keeping only test SrcEntities: 462786 rows
    ✅ After applying threshold ≥ 0.0: 462786 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_cosine_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2564 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_cosine_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1844
    📊 Evaluation (P / R / F1): {'P': 0.719, 'R': 0.692, 'F1': 0.706}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_faiss_cosine_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7303, 'Recall@1': 0.6823, 'F1@1': 0.7055}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_cosine_mrr_hit_ResMLPencoded.tsv"
)

```

    ✅ Top-200 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_cosine_mrr_hit_ResMLPencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_faiss_cosine_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9230419870959452, 'Hits@1': 0.872324446113406, 'Hits@5': 0.9853548629365377, 'Hits@10': 0.9928651896357491}


# **MLPEncoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = MLPEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_MLPencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_MLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_cands_with_embeddings_MLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_MLPencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_cosine_MLPencoded.tsv"
)
```

    ✅ Top-1 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_cosine_MLPencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_cosine_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1445
    📊 Evaluation (P / R / F1): {'Precision': 0.725, 'Recall': 0.7011, 'F1': 0.7129}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9518 rows
    ✅ After removing ignored classes: 9518 rows
    ✅ After keeping only test SrcEntities: 2401 rows
    ✅ After applying threshold ≥ 0.0: 2401 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_cosine_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2401 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_cosine_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1762
    📊 Evaluation (P / R / F1): {'P': 0.734, 'R': 0.662, 'F1': 0.696}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_faiss_cosine_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7339, 'Recall@1': 0.6869, 'F1@1': 0.7096}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_cosine_mrr_hit_MLPencoded.tsv"
)

```

    ✅ Top-1 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_cosine_mrr_hit_MLPencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_faiss_cosine_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6715860358412924, 'Hits@1': 0.6616597822005257, 'Hits@5': 0.6616597822005257, 'Hits@10': 0.6616597822005257}


# **K=10**


```python
# Compute the top-10 most similar mappings using cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_cosine_MLPencoded.tsv"
)
```

    ✅ Top-10 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_cosine_MLPencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_cosine_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1445
    📊 Evaluation (P / R / F1): {'Precision': 0.725, 'Recall': 0.7011, 'F1': 0.7129}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 95954 rows
    ✅ After removing ignored classes: 95954 rows
    ✅ After keeping only test SrcEntities: 22647 rows
    ✅ After applying threshold ≥ 0.0: 22647 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_cosine_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2596 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_cosine_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1835
    📊 Evaluation (P / R / F1): {'P': 0.707, 'R': 0.689, 'F1': 0.698}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_faiss_cosine_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7247, 'Recall@1': 0.6771, 'F1@1': 0.7001}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_cosine_mrr_hit_MLPencoded.tsv"
)

```

    ✅ Top-10 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_cosine_mrr_hit_MLPencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_faiss_cosine_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8990013817999427, 'Hits@1': 0.8595568907247465, 'Hits@5': 0.9414194517461509, 'Hits@10': 0.9414194517461509}


# **K=30**


```python
# Compute the top-10 most similar mappings using cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_cosine_MLPencoded.tsv"
)
```

    ✅ Top-30 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_cosine_MLPencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_cosine_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1445
    📊 Evaluation (P / R / F1): {'Precision': 0.725, 'Recall': 0.7011, 'F1': 0.7129}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 290775 rows
    ✅ After removing ignored classes: 290775 rows
    ✅ After keeping only test SrcEntities: 68428 rows
    ✅ After applying threshold ≥ 0.0: 68428 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_cosine_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2596 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_cosine_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1835
    📊 Evaluation (P / R / F1): {'P': 0.707, 'R': 0.689, 'F1': 0.698}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_faiss_cosine_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7247, 'Recall@1': 0.6771, 'F1@1': 0.7001}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_cosine_mrr_hit_MLPencoded.tsv"
)

```

    ✅ Top-30 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_cosine_mrr_hit_MLPencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_faiss_cosine_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9086804193020087, 'Hits@1': 0.8621855050694706, 'Hits@5': 0.9650769808486669, 'Hits@10': 0.9677055951933909}


# **K=100**


```python
# Compute the top-10 most similar mappings using cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_cosine_MLPencoded.tsv"
)
```

    ✅ Top-100 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_cosine_MLPencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_cosine_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1445
    📊 Evaluation (P / R / F1): {'Precision': 0.725, 'Recall': 0.7011, 'F1': 0.7129}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 978050 rows
    ✅ After removing ignored classes: 978050 rows
    ✅ After keeping only test SrcEntities: 230444 rows
    ✅ After applying threshold ≥ 0.0: 230444 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_cosine_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2596 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_cosine_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1835
    📊 Evaluation (P / R / F1): {'P': 0.707, 'R': 0.689, 'F1': 0.698}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_faiss_cosine_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7247, 'Recall@1': 0.6771, 'F1@1': 0.7001}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_cosine_mrr_hit_MLPencoded.tsv"
)

```

    ✅ Top-100 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_cosine_mrr_hit_MLPencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_faiss_cosine_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9143861821360183, 'Hits@1': 0.8633120540743522, 'Hits@5': 0.9774690199023658, 'Hits@10': 0.9864814119414195}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using cosine distance on ResMLP-encoded embeddings, then save the results.

topk_faiss_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_cosine_MLPencoded.tsv"  # Save results to this output file
)
```

    ✅ Top-200 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_cosine_MLPencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_cosine_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1445
    📊 Evaluation (P / R / F1): {'Precision': 0.725, 'Recall': 0.7011, 'F1': 0.7129}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1964463 rows
    ✅ After removing ignored classes: 1964463 rows
    ✅ After keeping only test SrcEntities: 462881 rows
    ✅ After applying threshold ≥ 0.0: 462881 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_cosine_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2596 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_cosine_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1835
    📊 Evaluation (P / R / F1): {'P': 0.707, 'R': 0.689, 'F1': 0.698}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_faiss_cosine_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7247, 'Recall@1': 0.6771, 'F1@1': 0.7001}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_cosine_mrr_hit_MLPencoded.tsv"
)

```

    ✅ Top-200 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_cosine_mrr_hit_MLPencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_faiss_cosine_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9154687329302721, 'Hits@1': 0.8633120540743522, 'Hits@5': 0.9797221179121292, 'Hits@10': 0.989485542621104}


# **LinearEncoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = LinearEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_Linencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_Linencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_cands_with_embeddings_Linencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_Linencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_cosine_Linencoded.tsv"
)
```

    ✅ Top-1 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_cosine_Linencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_cosine_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1431
    📊 Evaluation (P / R / F1): {'Precision': 0.718, 'Recall': 0.6943, 'F1': 0.706}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9463 rows
    ✅ After removing ignored classes: 9463 rows
    ✅ After keeping only test SrcEntities: 2399 rows
    ✅ After applying threshold ≥ 0.0: 2399 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_cosine_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2399 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_cosine_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1754
    📊 Evaluation (P / R / F1): {'P': 0.731, 'R': 0.659, 'F1': 0.693}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_faiss_cosine_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7311, 'Recall@1': 0.6836, 'F1@1': 0.7065}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_cosine_mrr_hit_Linencoded.tsv"
)

```

    ✅ Top-1 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_cosine_mrr_hit_Linencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_faiss_cosine_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6686686841465777, 'Hits@1': 0.6586556515208412, 'Hits@5': 0.6586556515208412, 'Hits@10': 0.6586556515208412}


# **K=10**


```python
# Compute the top-10 most similar mappings using cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_cosine_Linencoded.tsv"
)
```

    ✅ Top-10 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_cosine_Linencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_cosine_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1431
    📊 Evaluation (P / R / F1): {'Precision': 0.718, 'Recall': 0.6943, 'F1': 0.706}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 95783 rows
    ✅ After removing ignored classes: 95783 rows
    ✅ After keeping only test SrcEntities: 22563 rows
    ✅ After applying threshold ≥ 0.0: 22563 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_cosine_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2558 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_cosine_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1826
    📊 Evaluation (P / R / F1): {'P': 0.714, 'R': 0.686, 'F1': 0.699}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_faiss_cosine_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7215, 'Recall@1': 0.6741, 'F1@1': 0.697}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_cosine_mrr_hit_Linencoded.tsv"
)

```

    ✅ Top-10 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_cosine_mrr_hit_Linencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_faiss_cosine_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8923751515741968, 'Hits@1': 0.8516710476905746, 'Hits@5': 0.9361622230567029, 'Hits@10': 0.9361622230567029}


# **K=30**


```python
# Compute the top-10 most similar mappings using faiss_cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the faiss_cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_cosine_Linencoded.tsv"
)
```

    ✅ Top-30 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_cosine_Linencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_cosine_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1431
    📊 Evaluation (P / R / F1): {'Precision': 0.718, 'Recall': 0.6943, 'F1': 0.706}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 290661 rows
    ✅ After removing ignored classes: 290661 rows
    ✅ After keeping only test SrcEntities: 68379 rows
    ✅ After applying threshold ≥ 0.0: 68379 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_cosine_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2558 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_cosine_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1826
    📊 Evaluation (P / R / F1): {'P': 0.714, 'R': 0.686, 'F1': 0.699}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_faiss_cosine_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7215, 'Recall@1': 0.6741, 'F1@1': 0.697}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_cosine_mrr_hit_Linencoded.tsv"
)

```

    ✅ Top-30 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_cosine_mrr_hit_Linencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_faiss_cosine_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9113772343157598, 'Hits@1': 0.8633120540743522, 'Hits@5': 0.9692076605332332, 'Hits@10': 0.9733383402177995}


# **K=100**


```python
# Compute the top-10 most similar mappings using faiss_cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the faiss_cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_cosine_Linencoded.tsv"
)
```

    ✅ Top-100 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_cosine_Linencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_cosine_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1431
    📊 Evaluation (P / R / F1): {'Precision': 0.718, 'Recall': 0.6943, 'F1': 0.706}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 977811 rows
    ✅ After removing ignored classes: 977811 rows
    ✅ After keeping only test SrcEntities: 229901 rows
    ✅ After applying threshold ≥ 0.0: 229901 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_cosine_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2558 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_cosine_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1826
    📊 Evaluation (P / R / F1): {'P': 0.714, 'R': 0.686, 'F1': 0.699}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_faiss_cosine_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7215, 'Recall@1': 0.6741, 'F1@1': 0.697}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_cosine_mrr_hit_Linencoded.tsv"
)

```

    ✅ Top-100 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_cosine_mrr_hit_Linencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_faiss_cosine_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.916907430941842, 'Hits@1': 0.8651896357491551, 'Hits@5': 0.9819752159218926, 'Hits@10': 0.989485542621104}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using faiss_cosine distance on ResMLP-encoded embeddings, then save the results.

topk_faiss_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_cosine_Linencoded.tsv"  # Save results to this output file
)
```

    ✅ Top-200 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_cosine_Linencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_cosine_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1431
    📊 Evaluation (P / R / F1): {'Precision': 0.718, 'Recall': 0.6943, 'F1': 0.706}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1963547 rows
    ✅ After removing ignored classes: 1963547 rows
    ✅ After keeping only test SrcEntities: 461638 rows
    ✅ After applying threshold ≥ 0.0: 461638 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_cosine_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2558 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_cosine_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1826
    📊 Evaluation (P / R / F1): {'P': 0.714, 'R': 0.686, 'F1': 0.699}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_faiss_cosine_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7215, 'Recall@1': 0.6741, 'F1@1': 0.697}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on faiss_cosine distance
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_cosine_mrr_hit_Linencoded.tsv"
)

```

    ✅ Top-200 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_cosine_mrr_hit_Linencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_faiss_cosine_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.91807850822531, 'Hits@1': 0.8659406684190762, 'Hits@5': 0.9831017649267744, 'Hits@10': 0.9928651896357491}


# **Transformer Encoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = TransformerEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_TRencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_TRencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_cands_with_embeddings_TRencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_TRencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_cosine_TRencoded.tsv"
)
```

    ✅ Top-1 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_cosine_TRencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_cosine_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1457
    📊 Evaluation (P / R / F1): {'Precision': 0.7311, 'Recall': 0.7069, 'F1': 0.7188}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9463 rows
    ✅ After removing ignored classes: 9463 rows
    ✅ After keeping only test SrcEntities: 2395 rows
    ✅ After applying threshold ≥ 0.0: 2395 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_cosine_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2395 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_cosine_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1777
    📊 Evaluation (P / R / F1): {'P': 0.742, 'R': 0.667, 'F1': 0.703}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_faiss_cosine_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.742, 'Recall@1': 0.6939, 'F1@1': 0.7171}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_cosine_mrr_hit_TRencoded.tsv"
)

```

    ✅ Top-1 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_cosine_mrr_hit_TRencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_faiss_cosine_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6770527963344496, 'Hits@1': 0.6672925272249343, 'Hits@5': 0.6672925272249343, 'Hits@10': 0.6672925272249343}


# **K=10**


```python
# Compute the top-10 most similar mappings using faiss_cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the faiss_cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_cosine_TRencoded.tsv"
)
```

    ✅ Top-10 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_cosine_TRencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_cosine_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1457
    📊 Evaluation (P / R / F1): {'Precision': 0.7311, 'Recall': 0.7069, 'F1': 0.7188}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 96024 rows
    ✅ After removing ignored classes: 96024 rows
    ✅ After keeping only test SrcEntities: 22639 rows
    ✅ After applying threshold ≥ 0.0: 22639 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_cosine_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2571 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_cosine_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1843
    📊 Evaluation (P / R / F1): {'P': 0.717, 'R': 0.692, 'F1': 0.704}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_faiss_cosine_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7311, 'Recall@1': 0.6831, 'F1@1': 0.7063}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_cosine_mrr_hit_TRencoded.tsv"
)

```

    ✅ Top-10 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_cosine_mrr_hit_TRencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_faiss_cosine_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.897721381584642, 'Hits@1': 0.8588058580548253, 'Hits@5': 0.9395418700713482, 'Hits@10': 0.9395418700713482}


# **K=30**


```python
# Compute the top-10 most similar mappings using faiss_cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the faiss_cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_cosine_TRencoded.tsv"
)
```

    ✅ Top-30 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_cosine_TRencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_cosine_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1457
    📊 Evaluation (P / R / F1): {'Precision': 0.7311, 'Recall': 0.7069, 'F1': 0.7188}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 291264 rows
    ✅ After removing ignored classes: 291264 rows
    ✅ After keeping only test SrcEntities: 68501 rows
    ✅ After applying threshold ≥ 0.0: 68501 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_cosine_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2571 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_cosine_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1843
    📊 Evaluation (P / R / F1): {'P': 0.717, 'R': 0.692, 'F1': 0.704}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_faiss_cosine_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7311, 'Recall@1': 0.6831, 'F1@1': 0.7063}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_cosine_mrr_hit_TRencoded.tsv"
)

```

    ✅ Top-30 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_cosine_mrr_hit_TRencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_faiss_cosine_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9154906743159793, 'Hits@1': 0.8693203154337213, 'Hits@5': 0.9703342095381149, 'Hits@10': 0.97371385655276}


# **K=100**


```python
# Compute the top-10 most similar mappings using faiss_cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the faiss_cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_cosine_TRencoded.tsv"
)
```

    ✅ Top-100 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_cosine_TRencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_cosine_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1457
    📊 Evaluation (P / R / F1): {'Precision': 0.7311, 'Recall': 0.7069, 'F1': 0.7188}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 980686 rows
    ✅ After removing ignored classes: 980686 rows
    ✅ After keeping only test SrcEntities: 230640 rows
    ✅ After applying threshold ≥ 0.0: 230640 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_cosine_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2571 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_cosine_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1843
    📊 Evaluation (P / R / F1): {'P': 0.717, 'R': 0.692, 'F1': 0.704}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_faiss_cosine_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7311, 'Recall@1': 0.6831, 'F1@1': 0.7063}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_cosine_mrr_hit_TRencoded.tsv"
)

```

    ✅ Top-100 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_cosine_mrr_hit_TRencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_faiss_cosine_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9214331839333023, 'Hits@1': 0.8711978971085242, 'Hits@5': 0.9838527975966954, 'Hits@10': 0.9913631242959069}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using faiss_cosine distance on ResMLP-encoded embeddings, then save the results.

topk_faiss_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_cosine_TRencoded.tsv"  # Save results to this output file
)
```

    ✅ Top-200 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_cosine_TRencoded.tsv



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_cosine_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1457
    📊 Evaluation (P / R / F1): {'Precision': 0.7311, 'Recall': 0.7069, 'F1': 0.7188}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1969084 rows
    ✅ After removing ignored classes: 1969084 rows
    ✅ After keeping only test SrcEntities: 463250 rows
    ✅ After applying threshold ≥ 0.0: 463250 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_cosine_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2571 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_cosine_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1843
    📊 Evaluation (P / R / F1): {'P': 0.717, 'R': 0.692, 'F1': 0.704}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_faiss_cosine_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7311, 'Recall@1': 0.6831, 'F1@1': 0.7063}



```python
topk_faiss_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_cosine_mrr_hit_TRencoded.tsv"
)

```

    ✅ Top-200 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_cosine_mrr_hit_TRencoded.tsv



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_faiss_cosine_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9221363841644391, 'Hits@1': 0.8715734134434848, 'Hits@5': 0.9849793466015772, 'Hits@10': 0.9924896733007886}


# **Using faiss: topk_faiss_l2**

# **K=1**


```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_l2.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-1 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_l2.tsv
    ⏱️ Execution time: 5.08 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_l2.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_l2_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1486
    📊 Evaluation (P / R / F1): {'Precision': 0.7456, 'Recall': 0.721, 'F1': 0.7331}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_l2.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9338 rows
    ✅ After removing ignored classes: 9338 rows
    ✅ After keeping only test SrcEntities: 2401 rows
    ✅ After applying threshold ≥ 0.0: 2401 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_l2_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2401 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_l2_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1815
    📊 Evaluation (P / R / F1): {'P': 0.756, 'R': 0.682, 'F1': 0.717}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/neoplas_top_1_mappings_faiss_l2_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7559, 'Recall@1': 0.7071, 'F1@1': 0.7307}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_l2_mrr_hit.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-1 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_l2_mrr_hit.tsv
    ⏱️ Execution time: 3.01 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_faiss_l2_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6909033534555333, 'Hits@1': 0.681562147953436, 'Hits@5': 0.681562147953436, 'Hits@10': 0.681562147953436}


# **K=10**


```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_l2.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-10 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_l2.tsv
    ⏱️ Execution time: 5.58 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_l2.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_l2_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1486
    📊 Evaluation (P / R / F1): {'Precision': 0.7456, 'Recall': 0.721, 'F1': 0.7331}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_l2.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 93974 rows
    ✅ After removing ignored classes: 93974 rows
    ✅ After keeping only test SrcEntities: 22353 rows
    ✅ After applying threshold ≥ 0.0: 22353 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_l2_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2500 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_l2_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1871
    📊 Evaluation (P / R / F1): {'P': 0.748, 'R': 0.703, 'F1': 0.725}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_faiss_l2_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7492, 'Recall@1': 0.7, 'F1@1': 0.7237}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_l2_mrr_hit.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-10 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_l2_mrr_hit.tsv
    ⏱️ Execution time: 3.10 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_faiss_l2_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9087012240228131, 'Hits@1': 0.8708223807735637, 'Hits@5': 0.9504318437852046, 'Hits@10': 0.9504318437852046}


# **K=30**


```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_l2.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-30 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_l2.tsv
    ⏱️ Execution time: 7.97 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_l2.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_l2_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1486
    📊 Evaluation (P / R / F1): {'Precision': 0.7456, 'Recall': 0.721, 'F1': 0.7331}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_l2.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 284213 rows
    ✅ After removing ignored classes: 284213 rows
    ✅ After keeping only test SrcEntities: 67469 rows
    ✅ After applying threshold ≥ 0.0: 67469 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_l2_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2500 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_l2_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1871
    📊 Evaluation (P / R / F1): {'P': 0.748, 'R': 0.703, 'F1': 0.725}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_faiss_l2_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7492, 'Recall@1': 0.7, 'F1@1': 0.7237}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_l2_mrr_hit.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-30 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_l2_mrr_hit.tsv
    ⏱️ Execution time: 3.42 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_faiss_l2_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9210168581050832, 'Hits@1': 0.8768306421329328, 'Hits@5': 0.97371385655276, 'Hits@10': 0.9778445362373264}


# **K=100**


```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_l2.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-100 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_l2.tsv
    ⏱️ Execution time: 12.61 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_l2.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_l2_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1486
    📊 Evaluation (P / R / F1): {'Precision': 0.7456, 'Recall': 0.721, 'F1': 0.7331}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_l2.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 953701 rows
    ✅ After removing ignored classes: 953701 rows
    ✅ After keeping only test SrcEntities: 226196 rows
    ✅ After applying threshold ≥ 0.0: 226196 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_l2_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2500 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_l2_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1871
    📊 Evaluation (P / R / F1): {'P': 0.748, 'R': 0.703, 'F1': 0.725}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_faiss_l2_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7492, 'Recall@1': 0.7, 'F1@1': 0.7237}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_l2_mrr_hit.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-100 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_l2_mrr_hit.tsv
    ⏱️ Execution time: 4.16 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_faiss_l2_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9249873795274303, 'Hits@1': 0.8775816748028539, 'Hits@5': 0.984228313931656, 'Hits@10': 0.9906120916259857}


# **K=200**


```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_l2.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-200 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_l2.tsv
    ⏱️ Execution time: 18.85 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_l2.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_l2_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1486
    📊 Evaluation (P / R / F1): {'Precision': 0.7456, 'Recall': 0.721, 'F1': 0.7331}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_l2.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1912330 rows
    ✅ After removing ignored classes: 1912330 rows
    ✅ After keeping only test SrcEntities: 453381 rows
    ✅ After applying threshold ≥ 0.0: 453381 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_l2_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2500 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_l2_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1871
    📊 Evaluation (P / R / F1): {'P': 0.748, 'R': 0.703, 'F1': 0.725}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_faiss_l2_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7492, 'Recall@1': 0.7, 'F1@1': 0.7237}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_l2_mrr_hit.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-200 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_l2_mrr_hit.tsv
    ⏱️ Execution time: 6.19 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_faiss_l2_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.925705592253055, 'Hits@1': 0.8779571911378145, 'Hits@5': 0.9849793466015772, 'Hits@10': 0.9932407059707097}


# **K=500**


```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=500,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_500_mappings_faiss_l2.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-500 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_faiss_l2.tsv
    ⏱️ Execution time: 39.43 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_500_mappings_faiss_l2.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 5703500 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 996500 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_faiss_l2_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1486
    📊 Evaluation (P / R / F1): {'Precision': 0.7456, 'Recall': 0.721, 'F1': 0.7331}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_500_mappings_faiss_l2.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 5703500 rows
    ✅ After removing train-only URIs: 4791553 rows
    ✅ After removing ignored classes: 4791553 rows
    ✅ After keeping only test SrcEntities: 1135511 rows
    ✅ After applying threshold ≥ 0.0: 1135511 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_faiss_l2_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2500 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_faiss_l2_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1871
    📊 Evaluation (P / R / F1): {'P': 0.748, 'R': 0.703, 'F1': 0.725}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_500_mappings_faiss_l2_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7492, 'Recall@1': 0.7, 'F1@1': 0.7237}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=500,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_500_mappings_faiss_l2_mrr_hit.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-500 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_faiss_l2_mrr_hit.tsv
    ⏱️ Execution time: 10.32 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_500_mappings_faiss_l2_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9260917581785761, 'Hits@1': 0.8779571911378145, 'Hits@5': 0.9853548629365377, 'Hits@10': 0.9951182876455126}


# **K=1000**


```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1000,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_1000_mappings_faiss_l2.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-1000 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_faiss_l2.tsv
    ⏱️ Execution time: 74.68 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1000_mappings_faiss_l2.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407000 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993000 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_faiss_l2_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1486
    📊 Evaluation (P / R / F1): {'Precision': 0.7456, 'Recall': 0.721, 'F1': 0.7331}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1000_mappings_faiss_l2.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 11407000 rows
    ✅ After removing train-only URIs: 9607031 rows
    ✅ After removing ignored classes: 9607031 rows
    ✅ After keeping only test SrcEntities: 2277216 rows
    ✅ After applying threshold ≥ 0.0: 2277216 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_faiss_l2_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2500 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_faiss_l2_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1871
    📊 Evaluation (P / R / F1): {'P': 0.748, 'R': 0.703, 'F1': 0.725}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1000_mappings_faiss_l2_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7492, 'Recall@1': 0.7, 'F1@1': 0.7237}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1000,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_1000_mappings_faiss_l2_mrr_hit.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-1000 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_faiss_l2_mrr_hit.tsv
    ⏱️ Execution time: 17.33 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1000_mappings_faiss_l2_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.926348935066863, 'Hits@1': 0.8779571911378145, 'Hits@5': 0.9857303792714983, 'Hits@10': 0.9962448366503943}


# **K=2000**


```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=2000,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_2000_mappings_faiss_l2.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-2000 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_faiss_l2.tsv
    ⏱️ Execution time: 139.75 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_2000_mappings_faiss_l2.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 22814000 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 3986000 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_faiss_l2_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1486
    📊 Evaluation (P / R / F1): {'Precision': 0.7456, 'Recall': 0.721, 'F1': 0.7331}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_2000_mappings_faiss_l2.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 22814000 rows
    ✅ After removing train-only URIs: 19293601 rows
    ✅ After removing ignored classes: 19293601 rows
    ✅ After keeping only test SrcEntities: 4576412 rows
    ✅ After applying threshold ≥ 0.0: 4576412 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_faiss_l2_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2500 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_faiss_l2_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1871
    📊 Evaluation (P / R / F1): {'P': 0.748, 'R': 0.703, 'F1': 0.725}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_2000_mappings_faiss_l2_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7492, 'Recall@1': 0.7, 'F1@1': 0.7237}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=2000,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_2000_mappings_faiss_l2_mrr_hit.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-2000 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_faiss_l2_mrr_hit.tsv
    ⏱️ Execution time: 33.66 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_2000_mappings_faiss_l2_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.926392698248757, 'Hits@1': 0.8779571911378145, 'Hits@5': 0.9857303792714983, 'Hits@10': 0.9966203529853549}


# **With Encoders**

# **ResMLPEncoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = ResMLPEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_cands_with_embeddings_ResMLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_ResMLPencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using l2 distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the l2 distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_l2(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_l2_ResMLPencoded.tsv"
)
```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-1 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_l2_ResMLPencoded.tsv
    ⏱️ Execution time: 5.19 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_l2_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_l2_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1486
    📊 Evaluation (P / R / F1): {'Precision': 0.7456, 'Recall': 0.721, 'F1': 0.7331}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_l2_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9334 rows
    ✅ After removing ignored classes: 9334 rows
    ✅ After keeping only test SrcEntities: 2400 rows
    ✅ After applying threshold ≥ 0.0: 2400 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_l2_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2400 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_l2_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1812
    📊 Evaluation (P / R / F1): {'P': 0.755, 'R': 0.68, 'F1': 0.716}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_faiss_l2_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.755, 'Recall@1': 0.7062, 'F1@1': 0.7298}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_l2_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-1 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_l2_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 5.12 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_faiss_l2_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6898089915650769, 'Hits@1': 0.6804355989485542, 'Hits@5': 0.6804355989485542, 'Hits@10': 0.6804355989485542}


# **K=10**


```python
# Compute the top-10 most similar mappings using l2 distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the l2 distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_l2(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_l2_ResMLPencoded.tsv"
)
```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-10 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_l2_ResMLPencoded.tsv
    ⏱️ Execution time: 6.38 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_l2_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_l2_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1486
    📊 Evaluation (P / R / F1): {'Precision': 0.7456, 'Recall': 0.721, 'F1': 0.7331}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_l2_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 94032 rows
    ✅ After removing ignored classes: 94032 rows
    ✅ After keeping only test SrcEntities: 22357 rows
    ✅ After applying threshold ≥ 0.0: 22357 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_l2_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2504 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_l2_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1870
    📊 Evaluation (P / R / F1): {'P': 0.747, 'R': 0.702, 'F1': 0.724}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_faiss_l2_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7484, 'Recall@1': 0.6992, 'F1@1': 0.723}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_l2_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-10 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_l2_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 3.21 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_faiss_l2_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9080009263796606, 'Hits@1': 0.8704468644386031, 'Hits@5': 0.9485542621104018, 'Hits@10': 0.9489297784453624}


# **K=30**


```python
# Compute the top-10 most similar mappings using l2 distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the l2 distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_l2(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_l2_ResMLPencoded.tsv"
)
```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-30 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_l2_ResMLPencoded.tsv
    ⏱️ Execution time: 7.13 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_l2_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_l2_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1486
    📊 Evaluation (P / R / F1): {'Precision': 0.7456, 'Recall': 0.721, 'F1': 0.7331}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_l2_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 284200 rows
    ✅ After removing ignored classes: 284200 rows
    ✅ After keeping only test SrcEntities: 67484 rows
    ✅ After applying threshold ≥ 0.0: 67484 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_l2_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2504 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_l2_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1870
    📊 Evaluation (P / R / F1): {'P': 0.747, 'R': 0.702, 'F1': 0.724}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_faiss_l2_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7484, 'Recall@1': 0.6992, 'F1@1': 0.723}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_l2_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-30 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_l2_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 3.57 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_faiss_l2_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9214169618191189, 'Hits@1': 0.8768306421329328, 'Hits@5': 0.9744648892226812, 'Hits@10': 0.9785955689072474}


# **K=100**


```python
# Compute the top-10 most similar mappings using l2 distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the l2 distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_l2(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_l2_ResMLPencoded.tsv"
)
```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-100 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_l2_ResMLPencoded.tsv
    ⏱️ Execution time: 11.81 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_l2_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_l2_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1486
    📊 Evaluation (P / R / F1): {'Precision': 0.7456, 'Recall': 0.721, 'F1': 0.7331}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_l2_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 953449 rows
    ✅ After removing ignored classes: 953449 rows
    ✅ After keeping only test SrcEntities: 226178 rows
    ✅ After applying threshold ≥ 0.0: 226178 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_l2_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2504 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_l2_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1870
    📊 Evaluation (P / R / F1): {'P': 0.747, 'R': 0.702, 'F1': 0.724}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_faiss_l2_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7484, 'Recall@1': 0.6992, 'F1@1': 0.723}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_l2_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-100 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_l2_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 4.70 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_faiss_l2_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9247532954068625, 'Hits@1': 0.8772061584678934, 'Hits@5': 0.9834772812617348, 'Hits@10': 0.9906120916259857}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using l2 distance on ResMLP-encoded embeddings, then save the results.

topk_faiss_l2(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_l2_ResMLPencoded.tsv"  # Save results to this output file
)
```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-200 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_l2_ResMLPencoded.tsv
    ⏱️ Execution time: 20.25 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_l2_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_l2_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1486
    📊 Evaluation (P / R / F1): {'Precision': 0.7456, 'Recall': 0.721, 'F1': 0.7331}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_l2_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1911786 rows
    ✅ After removing ignored classes: 1911786 rows
    ✅ After keeping only test SrcEntities: 453296 rows
    ✅ After applying threshold ≥ 0.0: 453296 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_l2_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2504 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_l2_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1870
    📊 Evaluation (P / R / F1): {'P': 0.747, 'R': 0.702, 'F1': 0.724}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_faiss_l2_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7484, 'Recall@1': 0.6992, 'F1@1': 0.723}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_l2_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-200 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_l2_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 6.08 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_faiss_l2_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.925244479008162, 'Hits@1': 0.8772061584678934, 'Hits@5': 0.9846038302666166, 'Hits@10': 0.9928651896357491}


# **MLPEncoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = MLPEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_MLPencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_MLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_cands_with_embeddings_MLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_MLPencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using l2 distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the l2 distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_l2(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_l2_MLPencoded.tsv"
)
```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-1 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_l2_MLPencoded.tsv
    ⏱️ Execution time: 4.96 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_l2_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_l2_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1470
    📊 Evaluation (P / R / F1): {'Precision': 0.7376, 'Recall': 0.7132, 'F1': 0.7252}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_l2_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9388 rows
    ✅ After removing ignored classes: 9388 rows
    ✅ After keeping only test SrcEntities: 2392 rows
    ✅ After applying threshold ≥ 0.0: 2392 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_l2_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2392 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_l2_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1785
    📊 Evaluation (P / R / F1): {'P': 0.746, 'R': 0.67, 'F1': 0.706}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_faiss_l2_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7462, 'Recall@1': 0.6981, 'F1@1': 0.7214}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_l2_mrr_hit_MLPencoded.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-1 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_l2_mrr_hit_MLPencoded.tsv
    ⏱️ Execution time: 5.24 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_faiss_l2_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6799660457498751, 'Hits@1': 0.6702966579046189, 'Hits@5': 0.6702966579046189, 'Hits@10': 0.6702966579046189}


# **K=10**


```python
# Compute the top-10 most similar mappings using l2 distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the l2 distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_l2(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_l2_MLPencoded.tsv"
)
```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-10 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_l2_MLPencoded.tsv
    ⏱️ Execution time: 6.45 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_l2_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_l2_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1470
    📊 Evaluation (P / R / F1): {'Precision': 0.7376, 'Recall': 0.7132, 'F1': 0.7252}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_l2_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 93890 rows
    ✅ After removing ignored classes: 93890 rows
    ✅ After keeping only test SrcEntities: 22307 rows
    ✅ After applying threshold ≥ 0.0: 22307 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_l2_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2527 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_l2_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1844
    📊 Evaluation (P / R / F1): {'P': 0.73, 'R': 0.692, 'F1': 0.711}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_faiss_l2_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7375, 'Recall@1': 0.6891, 'F1@1': 0.7125}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_l2_mrr_hit_MLPencoded.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-10 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_l2_mrr_hit_MLPencoded.tsv
    ⏱️ Execution time: 2.69 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_faiss_l2_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9080009263796606, 'Hits@1': 0.8704468644386031, 'Hits@5': 0.9485542621104018, 'Hits@10': 0.9489297784453624}


# **K=30**


```python
# Compute the top-10 most similar mappings using l2 distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the l2 distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_l2(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_l2_MLPencoded.tsv"
)
```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-30 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_l2_MLPencoded.tsv
    ⏱️ Execution time: 7.06 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_l2_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_l2_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1470
    📊 Evaluation (P / R / F1): {'Precision': 0.7376, 'Recall': 0.7132, 'F1': 0.7252}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_l2_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 283510 rows
    ✅ After removing ignored classes: 283510 rows
    ✅ After keeping only test SrcEntities: 67232 rows
    ✅ After applying threshold ≥ 0.0: 67232 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_l2_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2527 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_l2_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1844
    📊 Evaluation (P / R / F1): {'P': 0.73, 'R': 0.692, 'F1': 0.711}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_faiss_l2_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7375, 'Recall@1': 0.6891, 'F1@1': 0.7125}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_l2_mrr_hit_MLPencoded.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-30 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_l2_mrr_hit_MLPencoded.tsv
    ⏱️ Execution time: 3.45 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_faiss_l2_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9134069854408421, 'Hits@1': 0.8648141194141945, 'Hits@5': 0.972962823882839, 'Hits@10': 0.9759669545625235}


# **K=100**


```python
# Compute the top-10 most similar mappings using l2 distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the l2 distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_l2(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_l2_MLPencoded.tsv"
)
```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-100 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_l2_MLPencoded.tsv
    ⏱️ Execution time: 12.29 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_l2_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_l2_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1470
    📊 Evaluation (P / R / F1): {'Precision': 0.7376, 'Recall': 0.7132, 'F1': 0.7252}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_l2_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 950097 rows
    ✅ After removing ignored classes: 950097 rows
    ✅ After keeping only test SrcEntities: 225061 rows
    ✅ After applying threshold ≥ 0.0: 225061 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_l2_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2527 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_l2_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1844
    📊 Evaluation (P / R / F1): {'P': 0.73, 'R': 0.692, 'F1': 0.711}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_faiss_l2_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7375, 'Recall@1': 0.6891, 'F1@1': 0.7125}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_l2_mrr_hit_MLPencoded.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-100 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_l2_mrr_hit_MLPencoded.tsv
    ⏱️ Execution time: 4.61 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_faiss_l2_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9173304171319376, 'Hits@1': 0.8659406684190762, 'Hits@5': 0.9812241832519715, 'Hits@10': 0.9887345099511828}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using l2 distance on ResMLP-encoded embeddings, then save the results.

topk_faiss_l2(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_l2_MLPencoded.tsv"  # Save results to this output file
)
```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-200 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_l2_MLPencoded.tsv
    ⏱️ Execution time: 19.58 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_l2_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_l2_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1470
    📊 Evaluation (P / R / F1): {'Precision': 0.7376, 'Recall': 0.7132, 'F1': 0.7252}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_l2_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1903317 rows
    ✅ After removing ignored classes: 1903317 rows
    ✅ After keeping only test SrcEntities: 450881 rows
    ✅ After applying threshold ≥ 0.0: 450881 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_l2_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2527 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_l2_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1844
    📊 Evaluation (P / R / F1): {'P': 0.73, 'R': 0.692, 'F1': 0.711}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_faiss_l2_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7375, 'Recall@1': 0.6891, 'F1@1': 0.7125}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_l2_mrr_hit_MLPencoded.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-200 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_l2_mrr_hit_MLPencoded.tsv
    ⏱️ Execution time: 5.98 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_faiss_l2_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9179299338803377, 'Hits@1': 0.8659406684190762, 'Hits@5': 0.9827262485918138, 'Hits@10': 0.9913631242959069}


# **LinearEncoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = LinearEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_Linencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_Linencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_cands_with_embeddings_Linencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_Linencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using l2 distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the l2 distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_l2(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_l2_Linencoded.tsv"
)
```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-1 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_l2_Linencoded.tsv
    ⏱️ Execution time: 4.86 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_l2_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_l2_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1507
    📊 Evaluation (P / R / F1): {'Precision': 0.7561, 'Recall': 0.7312, 'F1': 0.7435}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_l2_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9355 rows
    ✅ After removing ignored classes: 9355 rows
    ✅ After keeping only test SrcEntities: 2403 rows
    ✅ After applying threshold ≥ 0.0: 2403 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_l2_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2403 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_l2_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1839
    📊 Evaluation (P / R / F1): {'P': 0.765, 'R': 0.691, 'F1': 0.726}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_faiss_l2_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7653, 'Recall@1': 0.7156, 'F1@1': 0.7396}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_1_mappings_l2_mrr_hit_Linencoded.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-1 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_l2_mrr_hit_Linencoded.tsv
    ⏱️ Execution time: 2.98 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_l2_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6996506751404974, 'Hits@1': 0.6905745399924896, 'Hits@5': 0.6905745399924896, 'Hits@10': 0.6905745399924896}


# **K=5**


```python
# Compute the top-10 most similar mappings using l2 distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the l2 distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_l2(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=5,
    output_file=f"{results_dir}/{task}_top_5_mappings_faiss_l2_Linencoded.tsv"
)
```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-5 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_5_mappings_faiss_l2_Linencoded.tsv
    ⏱️ Execution time: 6.46 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_5_mappings_faiss_l2_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 57035 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 9965 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_5_mappings_faiss_l2_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1477
    📊 Evaluation (P / R / F1): {'Precision': 0.7411, 'Recall': 0.7166, 'F1': 0.7287}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_5_mappings_faiss_l2_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 57035 rows
    ✅ After removing train-only URIs: 46750 rows
    ✅ After removing ignored classes: 46750 rows
    ✅ After keeping only test SrcEntities: 11239 rows
    ✅ After applying threshold ≥ 0.0: 11239 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_5_mappings_faiss_l2_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2508 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_5_mappings_faiss_l2_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1850
    📊 Evaluation (P / R / F1): {'P': 0.738, 'R': 0.695, 'F1': 0.716}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_faiss_l2_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7653, 'Recall@1': 0.7156, 'F1@1': 0.7396}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_1_mappings_l2_mrr_hit_Linencoded.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-1 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_l2_mrr_hit_Linencoded.tsv
    ⏱️ Execution time: 2.98 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_l2_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6996506751404974, 'Hits@1': 0.6905745399924896, 'Hits@5': 0.6905745399924896, 'Hits@10': 0.6905745399924896}


# **K=10**


```python
# Compute the top-10 most similar mappings using l2 distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the l2 distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_l2(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_l2_Linencoded.tsv"
)
```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-10 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_l2_Linencoded.tsv
    ⏱️ Execution time: 6.45 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_l2_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_l2_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1507
    📊 Evaluation (P / R / F1): {'Precision': 0.7561, 'Recall': 0.7312, 'F1': 0.7435}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_l2_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 93795 rows
    ✅ After removing ignored classes: 93795 rows
    ✅ After keeping only test SrcEntities: 22311 rows
    ✅ After applying threshold ≥ 0.0: 22311 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_l2_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2504 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_l2_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1890
    📊 Evaluation (P / R / F1): {'P': 0.755, 'R': 0.71, 'F1': 0.732}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_faiss_l2_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7576, 'Recall@1': 0.7078, 'F1@1': 0.7319}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_l2_mrr_hit_Linencoded.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-10 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_l2_mrr_hit_Linencoded.tsv
    ⏱️ Execution time: 2.87 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_faiss_l2_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9103672402853702, 'Hits@1': 0.8745775441231693, 'Hits@5': 0.9504318437852046, 'Hits@10': 0.9504318437852046}


# **K=30**


```python
# Compute the top-10 most similar mappings using faiss_l2 distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the faiss_l2 distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_l2(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_l2_Linencoded.tsv"
)
```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-30 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_l2_Linencoded.tsv
    ⏱️ Execution time: 7.77 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_l2_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_l2_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1507
    📊 Evaluation (P / R / F1): {'Precision': 0.7561, 'Recall': 0.7312, 'F1': 0.7435}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_l2_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 283680 rows
    ✅ After removing ignored classes: 283680 rows
    ✅ After keeping only test SrcEntities: 67347 rows
    ✅ After applying threshold ≥ 0.0: 67347 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_l2_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2504 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_l2_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1890
    📊 Evaluation (P / R / F1): {'P': 0.755, 'R': 0.71, 'F1': 0.732}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_faiss_l2_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7576, 'Recall@1': 0.7078, 'F1@1': 0.7319}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_l2_mrr_hit_Linencoded.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-30 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_l2_mrr_hit_Linencoded.tsv
    ⏱️ Execution time: 3.07 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_faiss_l2_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9226893995024598, 'Hits@1': 0.8805858054825385, 'Hits@5': 0.9748404055576417, 'Hits@10': 0.9770935035674052}


# **K=100**


```python
# Compute the top-10 most similar mappings using faiss_l2 distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the faiss_l2 distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_l2(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_l2_Linencoded.tsv"
)
```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-100 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_l2_Linencoded.tsv
    ⏱️ Execution time: 12.46 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_l2_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_l2_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1507
    📊 Evaluation (P / R / F1): {'Precision': 0.7561, 'Recall': 0.7312, 'F1': 0.7435}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_l2_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 952518 rows
    ✅ After removing ignored classes: 952518 rows
    ✅ After keeping only test SrcEntities: 225857 rows
    ✅ After applying threshold ≥ 0.0: 225857 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_l2_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2504 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_l2_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1890
    📊 Evaluation (P / R / F1): {'P': 0.755, 'R': 0.71, 'F1': 0.732}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_faiss_l2_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7576, 'Recall@1': 0.7078, 'F1@1': 0.7319}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_l2_mrr_hit_Linencoded.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-100 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_l2_mrr_hit_Linencoded.tsv
    ⏱️ Execution time: 4.32 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_faiss_l2_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9270179455446481, 'Hits@1': 0.8820878708223808, 'Hits@5': 0.9853548629365377, 'Hits@10': 0.9906120916259857}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using faiss_l2 distance on ResMLP-encoded embeddings, then save the results.

topk_faiss_l2(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_l2_Linencoded.tsv"  # Save results to this output file
)
```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-200 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_l2_Linencoded.tsv
    ⏱️ Execution time: 18.97 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_l2_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_l2_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1507
    📊 Evaluation (P / R / F1): {'Precision': 0.7561, 'Recall': 0.7312, 'F1': 0.7435}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_l2_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1909239 rows
    ✅ After removing ignored classes: 1909239 rows
    ✅ After keeping only test SrcEntities: 452431 rows
    ✅ After applying threshold ≥ 0.0: 452431 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_l2_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2504 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_l2_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1890
    📊 Evaluation (P / R / F1): {'P': 0.755, 'R': 0.71, 'F1': 0.732}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_faiss_l2_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7576, 'Recall@1': 0.7078, 'F1@1': 0.7319}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on faiss_l2 distance
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_l2_mrr_hit_Linencoded.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-200 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_l2_mrr_hit_Linencoded.tsv
    ⏱️ Execution time: 5.78 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_faiss_l2_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9272521559404618, 'Hits@1': 0.8820878708223808, 'Hits@5': 0.9857303792714983, 'Hits@10': 0.9924896733007886}


# **Transformer Encoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = TransformerEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_TRencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_TRencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_cands_with_embeddings_TRencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_TRencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using l2 distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the l2 distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_l2(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_l2_TRencoded.tsv"
)
```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-1 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_l2_TRencoded.tsv
    ⏱️ Execution time: 5.30 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_l2_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_l2_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1453
    📊 Evaluation (P / R / F1): {'Precision': 0.7291, 'Recall': 0.705, 'F1': 0.7168}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_l2_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9453 rows
    ✅ After removing ignored classes: 9453 rows
    ✅ After keeping only test SrcEntities: 2393 rows
    ✅ After applying threshold ≥ 0.0: 2393 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_l2_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2393 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_l2_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1779
    📊 Evaluation (P / R / F1): {'P': 0.743, 'R': 0.668, 'F1': 0.704}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_faiss_l2_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7434, 'Recall@1': 0.6955, 'F1@1': 0.7186}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_l2_mrr_hit_TRencoded.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-1 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_l2_mrr_hit_TRencoded.tsv
    ⏱️ Execution time: 5.00 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_faiss_l2_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6777814242482513, 'Hits@1': 0.6680435598948554, 'Hits@5': 0.6680435598948554, 'Hits@10': 0.6680435598948554}


# **K=10**


```python
# Compute the top-10 most similar mappings using faiss_l2 distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the faiss_l2 distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_l2(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_l2_TRencoded.tsv"
)
```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-10 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_l2_TRencoded.tsv
    ⏱️ Execution time: 6.22 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_l2_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_l2_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1453
    📊 Evaluation (P / R / F1): {'Precision': 0.7291, 'Recall': 0.705, 'F1': 0.7168}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_l2_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 95903 rows
    ✅ After removing ignored classes: 95903 rows
    ✅ After keeping only test SrcEntities: 22612 rows
    ✅ After applying threshold ≥ 0.0: 22612 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_l2_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2506 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_l2_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1823
    📊 Evaluation (P / R / F1): {'P': 0.727, 'R': 0.685, 'F1': 0.705}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_faiss_l2_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7315, 'Recall@1': 0.6834, 'F1@1': 0.7067}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_l2_mrr_hit_TRencoded.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-10 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_l2_mrr_hit_TRencoded.tsv
    ⏱️ Execution time: 3.07 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_faiss_l2_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8967946170873795, 'Hits@1': 0.8580548253849043, 'Hits@5': 0.938790837401427, 'Hits@10': 0.938790837401427}


# **K=30**


```python
# Compute the top-10 most similar mappings using faiss_l2 distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the faiss_l2 distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_l2(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_l2_TRencoded.tsv"
)
```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-30 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_l2_TRencoded.tsv
    ⏱️ Execution time: 7.76 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_l2_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_l2_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1453
    📊 Evaluation (P / R / F1): {'Precision': 0.7291, 'Recall': 0.705, 'F1': 0.7168}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_l2_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 290798 rows
    ✅ After removing ignored classes: 290798 rows
    ✅ After keeping only test SrcEntities: 68332 rows
    ✅ After applying threshold ≥ 0.0: 68332 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_l2_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2506 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_l2_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1823
    📊 Evaluation (P / R / F1): {'P': 0.727, 'R': 0.685, 'F1': 0.705}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_faiss_l2_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7315, 'Recall@1': 0.6834, 'F1@1': 0.7067}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_l2_mrr_hit_TRencoded.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-30 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_l2_mrr_hit_TRencoded.tsv
    ⏱️ Execution time: 3.66 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_faiss_l2_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9142168560797507, 'Hits@1': 0.8689447990987608, 'Hits@5': 0.9680811115283515, 'Hits@10': 0.9714607585429966}


# **K=100**


```python
# Compute the top-10 most similar mappings using faiss_l2 distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the faiss_l2 distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_l2(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_l2_TRencoded.tsv"
)
```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-100 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_l2_TRencoded.tsv
    ⏱️ Execution time: 12.75 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_l2_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_l2_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1453
    📊 Evaluation (P / R / F1): {'Precision': 0.7291, 'Recall': 0.705, 'F1': 0.7168}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_l2_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 978885 rows
    ✅ After removing ignored classes: 978885 rows
    ✅ After keeping only test SrcEntities: 230060 rows
    ✅ After applying threshold ≥ 0.0: 230060 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_l2_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2506 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_l2_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1823
    📊 Evaluation (P / R / F1): {'P': 0.727, 'R': 0.685, 'F1': 0.705}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_faiss_l2_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7315, 'Recall@1': 0.6834, 'F1@1': 0.7067}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_l2_mrr_hit_TRencoded.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-100 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_l2_mrr_hit_TRencoded.tsv
    ⏱️ Execution time: 4.45 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_faiss_l2_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9206826946966872, 'Hits@1': 0.8719489297784454, 'Hits@5': 0.9808486669170109, 'Hits@10': 0.9883589936162223}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using faiss_l2 distance on ResMLP-encoded embeddings, then save the results.

topk_faiss_l2(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_l2_TRencoded.tsv"  # Save results to this output file
)
```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-200 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_l2_TRencoded.tsv
    ⏱️ Execution time: 19.60 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_l2_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_l2_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1453
    📊 Evaluation (P / R / F1): {'Precision': 0.7291, 'Recall': 0.705, 'F1': 0.7168}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_l2_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1966091 rows
    ✅ After removing ignored classes: 1966091 rows
    ✅ After keeping only test SrcEntities: 462227 rows
    ✅ After applying threshold ≥ 0.0: 462227 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_l2_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2506 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_l2_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1823
    📊 Evaluation (P / R / F1): {'P': 0.727, 'R': 0.685, 'F1': 0.705}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_faiss_l2_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7315, 'Recall@1': 0.6834, 'F1@1': 0.7067}



```python
topk_faiss_l2(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on l2 distance
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_l2_mrr_hit_TRencoded.tsv"
)

```

    🔹 Using L2 (Euclidean) distance with FAISS
    Top-200 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_l2_mrr_hit_TRencoded.tsv
    ⏱️ Execution time: 6.28 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_faiss_l2_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9216429019703534, 'Hits@1': 0.872324446113406, 'Hits@5': 0.9823507322568532, 'Hits@10': 0.9924896733007886}



```python

```


```python

```

# **Using faiss: topk_faiss_inner_product**

# **K=1**


```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_inner_product.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-1 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_inner_product.tsv
    ⏱️ Execution time: 5.19 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_inner_product.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_inner_product_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1262
    📊 Evaluation (P / R / F1): {'Precision': 0.6332, 'Recall': 0.6123, 'F1': 0.6226}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_inner_product.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9748 rows
    ✅ After removing ignored classes: 9748 rows
    ✅ After keeping only test SrcEntities: 2399 rows
    ✅ After applying threshold ≥ 0.0: 2399 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_inner_product_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2399 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_inner_product_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1534
    📊 Evaluation (P / R / F1): {'P': 0.639, 'R': 0.576, 'F1': 0.606}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/neoplas_top_1_mappings_faiss_inner_product_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6394, 'Recall@1': 0.5978, 'F1@1': 0.6179}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_inner_product_mrr_hit.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-1 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_inner_product_mrr_hit.tsv
    ⏱️ Execution time: 2.61 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_faiss_inner_product_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.5884842709145166, 'Hits@1': 0.5760420578295156, 'Hits@5': 0.5760420578295156, 'Hits@10': 0.5760420578295156}


# **K=10**


```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_inner_product.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-10 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_inner_product.tsv
    ⏱️ Execution time: 6.61 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_inner_product.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_inner_product_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1262
    📊 Evaluation (P / R / F1): {'Precision': 0.6332, 'Recall': 0.6123, 'F1': 0.6226}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_inner_product.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 98509 rows
    ✅ After removing ignored classes: 98509 rows
    ✅ After keeping only test SrcEntities: 23139 rows
    ✅ After applying threshold ≥ 0.0: 23139 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_inner_product_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2585 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_inner_product_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1589
    📊 Evaluation (P / R / F1): {'P': 0.615, 'R': 0.597, 'F1': 0.606}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_faiss_inner_product_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6286, 'Recall@1': 0.5873, 'F1@1': 0.6073}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_inner_product_mrr_hit.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-10 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_inner_product_mrr_hit.tsv
    ⏱️ Execution time: 3.41 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_faiss_inner_product_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8466162113795246, 'Hits@1': 0.7990987607960947, 'Hits@5': 0.896733007885843, 'Hits@10': 0.8978595568907247}


# **K=30**


```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_inner_product.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-30 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_inner_product.tsv
    ⏱️ Execution time: 7.55 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_inner_product.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_inner_product_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1262
    📊 Evaluation (P / R / F1): {'Precision': 0.6332, 'Recall': 0.6123, 'F1': 0.6226}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_inner_product.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 297991 rows
    ✅ After removing ignored classes: 297991 rows
    ✅ After keeping only test SrcEntities: 69921 rows
    ✅ After applying threshold ≥ 0.0: 69921 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_inner_product_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2585 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_inner_product_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1589
    📊 Evaluation (P / R / F1): {'P': 0.615, 'R': 0.597, 'F1': 0.606}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_faiss_inner_product_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6286, 'Recall@1': 0.5873, 'F1@1': 0.6073}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_inner_product_mrr_hit.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-30 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_inner_product_mrr_hit.tsv
    ⏱️ Execution time: 3.36 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_faiss_inner_product_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8831730515483852, 'Hits@1': 0.8238828389034923, 'Hits@5': 0.955313556139692, 'Hits@10': 0.9586932031543373}


# **K=100**


```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_inner_product.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-100 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_inner_product.tsv
    ⏱️ Execution time: 12.84 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_inner_product.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_inner_product_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1262
    📊 Evaluation (P / R / F1): {'Precision': 0.6332, 'Recall': 0.6123, 'F1': 0.6226}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_inner_product.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 1000304 rows
    ✅ After removing ignored classes: 1000304 rows
    ✅ After keeping only test SrcEntities: 235235 rows
    ✅ After applying threshold ≥ 0.0: 235235 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_inner_product_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2585 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_inner_product_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1589
    📊 Evaluation (P / R / F1): {'P': 0.615, 'R': 0.597, 'F1': 0.606}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_faiss_inner_product_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6286, 'Recall@1': 0.5873, 'F1@1': 0.6073}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_inner_product_mrr_hit.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-100 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_inner_product_mrr_hit.tsv
    ⏱️ Execution time: 4.91 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_faiss_inner_product_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8936496525951082, 'Hits@1': 0.8287645512579798, 'Hits@5': 0.9748404055576417, 'Hits@10': 0.9846038302666166}


# **K=200**


```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_inner_product.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-200 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_inner_product.tsv
    ⏱️ Execution time: 20.22 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_inner_product.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_inner_product_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1262
    📊 Evaluation (P / R / F1): {'Precision': 0.6332, 'Recall': 0.6123, 'F1': 0.6226}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_inner_product.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 2006426 rows
    ✅ After removing ignored classes: 2006426 rows
    ✅ After keeping only test SrcEntities: 472235 rows
    ✅ After applying threshold ≥ 0.0: 472235 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_inner_product_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2585 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_inner_product_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1589
    📊 Evaluation (P / R / F1): {'P': 0.615, 'R': 0.597, 'F1': 0.606}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_faiss_inner_product_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6286, 'Recall@1': 0.5873, 'F1@1': 0.6073}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_inner_product_mrr_hit.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-200 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_inner_product_mrr_hit.tsv
    ⏱️ Execution time: 9.71 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_faiss_inner_product_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8955138733932495, 'Hits@1': 0.8287645512579798, 'Hits@5': 0.9797221179121292, 'Hits@10': 0.992114156965828}


# **K=500**


```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=500,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_500_mappings_faiss_inner_product.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-500 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_faiss_inner_product.tsv
    ⏱️ Execution time: 42.12 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_500_mappings_faiss_inner_product.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 5703500 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 996500 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_faiss_inner_product_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1262
    📊 Evaluation (P / R / F1): {'Precision': 0.6332, 'Recall': 0.6123, 'F1': 0.6226}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_500_mappings_faiss_inner_product.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 5703500 rows
    ✅ After removing train-only URIs: 5030995 rows
    ✅ After removing ignored classes: 5030995 rows
    ✅ After keeping only test SrcEntities: 1186335 rows
    ✅ After applying threshold ≥ 0.0: 1186335 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_faiss_inner_product_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2585 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_faiss_inner_product_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1589
    📊 Evaluation (P / R / F1): {'P': 0.615, 'R': 0.597, 'F1': 0.606}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_500_mappings_faiss_inner_product_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6286, 'Recall@1': 0.5873, 'F1@1': 0.6073}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=500,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_500_mappings_faiss_inner_product_mrr_hit.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-500 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_faiss_inner_product_mrr_hit.tsv
    ⏱️ Execution time: 11.85 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_500_mappings_faiss_inner_product_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8962798453918591, 'Hits@1': 0.8291400675929403, 'Hits@5': 0.9808486669170109, 'Hits@10': 0.9943672549755914}


# **K=1000**


```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1000,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_1000_mappings_faiss_inner_product.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-1000 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_faiss_inner_product.tsv
    ⏱️ Execution time: 83.84 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1000_mappings_faiss_inner_product.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407000 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993000 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_faiss_inner_product_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1262
    📊 Evaluation (P / R / F1): {'Precision': 0.6332, 'Recall': 0.6123, 'F1': 0.6226}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1000_mappings_faiss_inner_product.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 11407000 rows
    ✅ After removing train-only URIs: 10078399 rows
    ✅ After removing ignored classes: 10078399 rows
    ✅ After keeping only test SrcEntities: 2381130 rows
    ✅ After applying threshold ≥ 0.0: 2381130 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_faiss_inner_product_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2585 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_faiss_inner_product_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1589
    📊 Evaluation (P / R / F1): {'P': 0.615, 'R': 0.597, 'F1': 0.606}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1000_mappings_faiss_inner_product_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6286, 'Recall@1': 0.5873, 'F1@1': 0.6073}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1000,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_1000_mappings_faiss_inner_product_mrr_hit.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-1000 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_faiss_inner_product_mrr_hit.tsv
    ⏱️ Execution time: 17.35 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1000_mappings_faiss_inner_product_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8963189197942609, 'Hits@1': 0.8291400675929403, 'Hits@5': 0.9808486669170109, 'Hits@10': 0.994742771310552}


# **K=2000**


```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=2000,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_2000_mappings_faiss_inner_product.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-2000 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_faiss_inner_product.tsv
    ⏱️ Execution time: 140.45 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_2000_mappings_faiss_inner_product.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 22814000 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 3986000 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_faiss_inner_product_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1262
    📊 Evaluation (P / R / F1): {'Precision': 0.6332, 'Recall': 0.6123, 'F1': 0.6226}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_2000_mappings_faiss_inner_product.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 22814000 rows
    ✅ After removing train-only URIs: 20163955 rows
    ✅ After removing ignored classes: 20163955 rows
    ✅ After keeping only test SrcEntities: 4771027 rows
    ✅ After applying threshold ≥ 0.0: 4771027 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_faiss_inner_product_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2585 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_faiss_inner_product_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1589
    📊 Evaluation (P / R / F1): {'P': 0.615, 'R': 0.597, 'F1': 0.606}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_2000_mappings_faiss_inner_product_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6286, 'Recall@1': 0.5873, 'F1@1': 0.6073}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=2000,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_2000_mappings_faiss_inner_product_mrr_hit.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-2000 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_faiss_inner_product_mrr_hit.tsv
    ⏱️ Execution time: 31.73 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_2000_mappings_faiss_inner_product_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8964221400568136, 'Hits@1': 0.8291400675929403, 'Hits@5': 0.9808486669170109, 'Hits@10': 0.9958693203154337}


# **With Encoders**

# **ResMLPEncoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = ResMLPEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_cands_with_embeddings_ResMLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_ResMLPencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_inner_product(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_inner_product_ResMLPencoded.tsv"
)
```

    🔹 Using Inner Product (dot product) with FAISS
    Top-1 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_inner_product_ResMLPencoded.tsv
    ⏱️ Execution time: 4.83 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_inner_product_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_inner_product_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1256
    📊 Evaluation (P / R / F1): {'Precision': 0.6302, 'Recall': 0.6094, 'F1': 0.6196}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_inner_product_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9779 rows
    ✅ After removing ignored classes: 9779 rows
    ✅ After keeping only test SrcEntities: 2405 rows
    ✅ After applying threshold ≥ 0.0: 2405 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_inner_product_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2405 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_inner_product_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1524
    📊 Evaluation (P / R / F1): {'P': 0.634, 'R': 0.572, 'F1': 0.601}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_faiss_inner_product_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6337, 'Recall@1': 0.5935, 'F1@1': 0.6129}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_inner_product_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-1 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_inner_product_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 4.94 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_faiss_inner_product_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.5848379757460549, 'Hits@1': 0.5722868944799099, 'Hits@5': 0.5722868944799099, 'Hits@10': 0.5722868944799099}


# **K=10**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_inner_product(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_inner_product_ResMLPencoded.tsv"
)
```

    🔹 Using Inner Product (dot product) with FAISS
    Top-10 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_inner_product_ResMLPencoded.tsv
    ⏱️ Execution time: 5.75 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_inner_product_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_inner_product_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1256
    📊 Evaluation (P / R / F1): {'Precision': 0.6302, 'Recall': 0.6094, 'F1': 0.6196}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_inner_product_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 98750 rows
    ✅ After removing ignored classes: 98750 rows
    ✅ After keeping only test SrcEntities: 23187 rows
    ✅ After applying threshold ≥ 0.0: 23187 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_inner_product_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2604 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_inner_product_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1580
    📊 Evaluation (P / R / F1): {'P': 0.607, 'R': 0.593, 'F1': 0.6}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_faiss_inner_product_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6254, 'Recall@1': 0.5843, 'F1@1': 0.6042}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_inner_product_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-10 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_inner_product_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 2.91 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_faiss_inner_product_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8457790848592895, 'Hits@1': 0.7975966954562523, 'Hits@5': 0.8978595568907247, 'Hits@10': 0.8989861058956065}


# **K=30**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_inner_product(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_inner_product_ResMLPencoded.tsv"
)
```

    🔹 Using Inner Product (dot product) with FAISS
    Top-30 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_inner_product_ResMLPencoded.tsv
    ⏱️ Execution time: 7.40 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_inner_product_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_inner_product_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1256
    📊 Evaluation (P / R / F1): {'Precision': 0.6302, 'Recall': 0.6094, 'F1': 0.6196}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_inner_product_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 298695 rows
    ✅ After removing ignored classes: 298695 rows
    ✅ After keeping only test SrcEntities: 70042 rows
    ✅ After applying threshold ≥ 0.0: 70042 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_inner_product_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2604 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_inner_product_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1580
    📊 Evaluation (P / R / F1): {'P': 0.607, 'R': 0.593, 'F1': 0.6}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_faiss_inner_product_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6254, 'Recall@1': 0.5843, 'F1@1': 0.6042}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_inner_product_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-30 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_inner_product_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 3.34 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_faiss_inner_product_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8819902169634306, 'Hits@1': 0.8220052572286894, 'Hits@5': 0.955313556139692, 'Hits@10': 0.9586932031543373}


# **K=100**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_inner_product(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_inner_product_ResMLPencoded.tsv"
)
```

    🔹 Using Inner Product (dot product) with FAISS
    Top-100 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_inner_product_ResMLPencoded.tsv
    ⏱️ Execution time: 11.56 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_inner_product_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_inner_product_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1256
    📊 Evaluation (P / R / F1): {'Precision': 0.6302, 'Recall': 0.6094, 'F1': 0.6196}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_inner_product_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 1001885 rows
    ✅ After removing ignored classes: 1001885 rows
    ✅ After keeping only test SrcEntities: 235569 rows
    ✅ After applying threshold ≥ 0.0: 235569 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_inner_product_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2604 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_inner_product_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1580
    📊 Evaluation (P / R / F1): {'P': 0.607, 'R': 0.593, 'F1': 0.6}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_faiss_inner_product_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6254, 'Recall@1': 0.5843, 'F1@1': 0.6042}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_inner_product_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-100 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_inner_product_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 4.43 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_faiss_inner_product_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8920469937990735, 'Hits@1': 0.8268869695831769, 'Hits@5': 0.9748404055576417, 'Hits@10': 0.9834772812617348}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using inner_product distance on ResMLP-encoded embeddings, then save the results.

topk_faiss_inner_product(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_inner_product_ResMLPencoded.tsv"  # Save results to this output file
)
```

    🔹 Using Inner Product (dot product) with FAISS
    Top-200 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_inner_product_ResMLPencoded.tsv
    ⏱️ Execution time: 18.36 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_inner_product_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_inner_product_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1256
    📊 Evaluation (P / R / F1): {'Precision': 0.6302, 'Recall': 0.6094, 'F1': 0.6196}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_inner_product_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 2009966 rows
    ✅ After removing ignored classes: 2009966 rows
    ✅ After keeping only test SrcEntities: 473060 rows
    ✅ After applying threshold ≥ 0.0: 473060 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_inner_product_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2604 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_inner_product_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1580
    📊 Evaluation (P / R / F1): {'P': 0.607, 'R': 0.593, 'F1': 0.6}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_faiss_inner_product_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6254, 'Recall@1': 0.5843, 'F1@1': 0.6042}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_inner_product_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-200 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_inner_product_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 5.91 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_faiss_inner_product_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8945893711479325, 'Hits@1': 0.8272624859181374, 'Hits@5': 0.9804731505820503, 'Hits@10': 0.9917386406308675}


# **MLPEncoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = MLPEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_MLPencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_MLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_cands_with_embeddings_MLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_MLPencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_inner_product(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_inner_product_MLPencoded.tsv"
)
```

    🔹 Using Inner Product (dot product) with FAISS
    Top-1 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_inner_product_MLPencoded.tsv
    ⏱️ Execution time: 5.34 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_inner_product_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_inner_product_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1039
    📊 Evaluation (P / R / F1): {'Precision': 0.5213, 'Recall': 0.5041, 'F1': 0.5126}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_inner_product_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 10039 rows
    ✅ After removing ignored classes: 10039 rows
    ✅ After keeping only test SrcEntities: 2416 rows
    ✅ After applying threshold ≥ 0.0: 2416 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_inner_product_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2416 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_inner_product_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1255
    📊 Evaluation (P / R / F1): {'P': 0.519, 'R': 0.471, 'F1': 0.494}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_faiss_inner_product_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.5195, 'Recall@1': 0.4859, 'F1@1': 0.5021}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_inner_product_mrr_hit_MLPencoded.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-1 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_inner_product_mrr_hit_MLPencoded.tsv
    ⏱️ Execution time: 4.74 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_faiss_inner_product_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.4867928696074652, 'Hits@1': 0.47127300037551634, 'Hits@5': 0.47127300037551634, 'Hits@10': 0.47127300037551634}


# **K=10**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_inner_product(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_inner_product_MLPencoded.tsv"
)
```

    🔹 Using Inner Product (dot product) with FAISS
    Top-10 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_inner_product_MLPencoded.tsv
    ⏱️ Execution time: 6.52 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_inner_product_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_inner_product_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1039
    📊 Evaluation (P / R / F1): {'Precision': 0.5213, 'Recall': 0.5041, 'F1': 0.5126}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_inner_product_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 100791 rows
    ✅ After removing ignored classes: 100791 rows
    ✅ After keeping only test SrcEntities: 23716 rows
    ✅ After applying threshold ≥ 0.0: 23716 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_inner_product_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2659 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_inner_product_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1310
    📊 Evaluation (P / R / F1): {'P': 0.493, 'R': 0.492, 'F1': 0.492}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_faiss_inner_product_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.5129, 'Recall@1': 0.4792, 'F1@1': 0.4954}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_inner_product_mrr_hit_MLPencoded.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-10 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_inner_product_mrr_hit_MLPencoded.tsv
    ⏱️ Execution time: 2.99 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_faiss_inner_product_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8457790848592895, 'Hits@1': 0.7975966954562523, 'Hits@5': 0.8978595568907247, 'Hits@10': 0.8989861058956065}


# **K=30**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_inner_product(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_inner_product_MLPencoded.tsv"
)
```

    🔹 Using Inner Product (dot product) with FAISS
    Top-30 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_inner_product_MLPencoded.tsv
    ⏱️ Execution time: 8.14 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_inner_product_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_inner_product_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1039
    📊 Evaluation (P / R / F1): {'Precision': 0.5213, 'Recall': 0.5041, 'F1': 0.5126}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_inner_product_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 304150 rows
    ✅ After removing ignored classes: 304150 rows
    ✅ After keeping only test SrcEntities: 71573 rows
    ✅ After applying threshold ≥ 0.0: 71573 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_inner_product_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2659 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_inner_product_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1310
    📊 Evaluation (P / R / F1): {'P': 0.493, 'R': 0.492, 'F1': 0.492}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_faiss_inner_product_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.5129, 'Recall@1': 0.4792, 'F1@1': 0.4954}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_inner_product_mrr_hit_MLPencoded.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-30 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_inner_product_mrr_hit_MLPencoded.tsv
    ⏱️ Execution time: 3.15 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_faiss_inner_product_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8128102405359304, 'Hits@1': 0.7465264739016148, 'Hits@5': 0.8880961321817499, 'Hits@10': 0.8937288772061585}


# **K=100**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_inner_product(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_inner_product_MLPencoded.tsv"
)
```

    🔹 Using Inner Product (dot product) with FAISS
    Top-100 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_inner_product_MLPencoded.tsv
    ⏱️ Execution time: 12.23 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_inner_product_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_inner_product_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1039
    📊 Evaluation (P / R / F1): {'Precision': 0.5213, 'Recall': 0.5041, 'F1': 0.5126}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_inner_product_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 1018720 rows
    ✅ After removing ignored classes: 1018720 rows
    ✅ After keeping only test SrcEntities: 240325 rows
    ✅ After applying threshold ≥ 0.0: 240325 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_inner_product_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2659 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_inner_product_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1310
    📊 Evaluation (P / R / F1): {'P': 0.493, 'R': 0.492, 'F1': 0.492}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_faiss_inner_product_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.5129, 'Recall@1': 0.4792, 'F1@1': 0.4954}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_inner_product_mrr_hit_MLPencoded.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-100 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_inner_product_mrr_hit_MLPencoded.tsv
    ⏱️ Execution time: 4.10 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_faiss_inner_product_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8423187288882651, 'Hits@1': 0.7645512579797221, 'Hits@5': 0.9380398047315058, 'Hits@10': 0.9519339091250469}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using inner_product distance on ResMLP-encoded embeddings, then save the results.

topk_faiss_inner_product(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_inner_product_MLPencoded.tsv"  # Save results to this output file
)
```

    🔹 Using Inner Product (dot product) with FAISS
    Top-200 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_inner_product_MLPencoded.tsv
    ⏱️ Execution time: 19.51 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_inner_product_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_inner_product_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1039
    📊 Evaluation (P / R / F1): {'Precision': 0.5213, 'Recall': 0.5041, 'F1': 0.5126}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_inner_product_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 2042024 rows
    ✅ After removing ignored classes: 2042024 rows
    ✅ After keeping only test SrcEntities: 482604 rows
    ✅ After applying threshold ≥ 0.0: 482604 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_inner_product_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2659 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_inner_product_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1310
    📊 Evaluation (P / R / F1): {'P': 0.493, 'R': 0.492, 'F1': 0.492}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_faiss_inner_product_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.5129, 'Recall@1': 0.4792, 'F1@1': 0.4954}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_inner_product_mrr_hit_MLPencoded.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-200 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_inner_product_mrr_hit_MLPencoded.tsv
    ⏱️ Execution time: 5.94 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_faiss_inner_product_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8491103074347518, 'Hits@1': 0.7683064213293278, 'Hits@5': 0.9519339091250469, 'Hits@10': 0.9688321441982726}


# **LinearEncoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = LinearEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_Linencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_Linencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_cands_with_embeddings_Linencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_Linencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_inner_product(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_inner_product_Linencoded.tsv"
)
```

    🔹 Using Inner Product (dot product) with FAISS
    Top-1 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_inner_product_Linencoded.tsv
    ⏱️ Execution time: 4.96 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_inner_product_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_inner_product_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1225
    📊 Evaluation (P / R / F1): {'Precision': 0.6147, 'Recall': 0.5944, 'F1': 0.6043}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_inner_product_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9778 rows
    ✅ After removing ignored classes: 9778 rows
    ✅ After keeping only test SrcEntities: 2398 rows
    ✅ After applying threshold ≥ 0.0: 2398 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_inner_product_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2398 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_inner_product_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1475
    📊 Evaluation (P / R / F1): {'P': 0.615, 'R': 0.554, 'F1': 0.583}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_faiss_inner_product_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6151, 'Recall@1': 0.5759, 'F1@1': 0.5949}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_inner_product_mrr_hit_Linencoded.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-1 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_inner_product_mrr_hit_Linencoded.tsv
    ⏱️ Execution time: 2.47 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_faiss_inner_product_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.5669779139594173, 'Hits@1': 0.5538865940668419, 'Hits@5': 0.5538865940668419, 'Hits@10': 0.5538865940668419}


# **K=10**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_inner_product(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_inner_product_Linencoded.tsv"
)
```

    🔹 Using Inner Product (dot product) with FAISS
    Top-10 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_inner_product_Linencoded.tsv
    ⏱️ Execution time: 6.09 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_inner_product_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_inner_product_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1225
    📊 Evaluation (P / R / F1): {'Precision': 0.6147, 'Recall': 0.5944, 'F1': 0.6043}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_inner_product_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 98542 rows
    ✅ After removing ignored classes: 98542 rows
    ✅ After keeping only test SrcEntities: 23164 rows
    ✅ After applying threshold ≥ 0.0: 23164 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_inner_product_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2593 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_inner_product_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1532
    📊 Evaluation (P / R / F1): {'P': 0.591, 'R': 0.575, 'F1': 0.583}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_faiss_inner_product_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6049, 'Recall@1': 0.5652, 'F1@1': 0.5844}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_inner_product_mrr_hit_Linencoded.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-10 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_inner_product_mrr_hit_Linencoded.tsv
    ⏱️ Execution time: 2.64 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_faiss_inner_product_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8279229997913028, 'Hits@1': 0.7769432970334209, 'Hits@5': 0.8813368381524597, 'Hits@10': 0.8820878708223808}


# **K=30**


```python
# Compute the top-10 most similar mappings using faiss_inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the faiss_inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_inner_product(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_inner_product_Linencoded.tsv"
)
```

    🔹 Using Inner Product (dot product) with FAISS
    Top-30 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_inner_product_Linencoded.tsv
    ⏱️ Execution time: 7.53 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_inner_product_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_inner_product_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1225
    📊 Evaluation (P / R / F1): {'Precision': 0.6147, 'Recall': 0.5944, 'F1': 0.6043}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_inner_product_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 298103 rows
    ✅ After removing ignored classes: 298103 rows
    ✅ After keeping only test SrcEntities: 69996 rows
    ✅ After applying threshold ≥ 0.0: 69996 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_inner_product_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2593 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_inner_product_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1532
    📊 Evaluation (P / R / F1): {'P': 0.591, 'R': 0.575, 'F1': 0.583}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_faiss_inner_product_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6049, 'Recall@1': 0.5652, 'F1@1': 0.5844}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_inner_product_mrr_hit_Linencoded.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-30 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_inner_product_mrr_hit_Linencoded.tsv
    ⏱️ Execution time: 3.17 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_faiss_inner_product_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.870148416611878, 'Hits@1': 0.806609087495306, 'Hits@5': 0.9459256477656778, 'Hits@10': 0.9485542621104018}


# **K=100**


```python
# Compute the top-10 most similar mappings using faiss_inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the faiss_inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_inner_product(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_inner_product_Linencoded.tsv"
)
```

    🔹 Using Inner Product (dot product) with FAISS
    Top-100 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_inner_product_Linencoded.tsv
    ⏱️ Execution time: 12.96 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_inner_product_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_inner_product_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1225
    📊 Evaluation (P / R / F1): {'Precision': 0.6147, 'Recall': 0.5944, 'F1': 0.6043}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_inner_product_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 1000493 rows
    ✅ After removing ignored classes: 1000493 rows
    ✅ After keeping only test SrcEntities: 235482 rows
    ✅ After applying threshold ≥ 0.0: 235482 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_inner_product_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2593 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_inner_product_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1532
    📊 Evaluation (P / R / F1): {'P': 0.591, 'R': 0.575, 'F1': 0.583}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_faiss_inner_product_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6049, 'Recall@1': 0.5652, 'F1@1': 0.5844}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_inner_product_mrr_hit_Linencoded.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-100 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_inner_product_mrr_hit_Linencoded.tsv
    ⏱️ Execution time: 4.69 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_faiss_inner_product_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.885032446039347, 'Hits@1': 0.8144949305294781, 'Hits@5': 0.9725873075478784, 'Hits@10': 0.9819752159218926}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using faiss_inner_product distance on ResMLP-encoded embeddings, then save the results.

topk_faiss_inner_product(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_inner_product_Linencoded.tsv"  # Save results to this output file
)
```

    🔹 Using Inner Product (dot product) with FAISS
    Top-200 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_inner_product_Linencoded.tsv
    ⏱️ Execution time: 19.77 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_inner_product_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_inner_product_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1225
    📊 Evaluation (P / R / F1): {'Precision': 0.6147, 'Recall': 0.5944, 'F1': 0.6043}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_inner_product_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 2006508 rows
    ✅ After removing ignored classes: 2006508 rows
    ✅ After keeping only test SrcEntities: 472703 rows
    ✅ After applying threshold ≥ 0.0: 472703 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_inner_product_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2593 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_inner_product_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1532
    📊 Evaluation (P / R / F1): {'P': 0.591, 'R': 0.575, 'F1': 0.583}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_faiss_inner_product_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.6049, 'Recall@1': 0.5652, 'F1@1': 0.5844}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on faiss_inner_product distance
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_inner_product_mrr_hit_Linencoded.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-200 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_inner_product_mrr_hit_Linencoded.tsv
    ⏱️ Execution time: 5.85 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_faiss_inner_product_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8870608792414517, 'Hits@1': 0.8152459631993991, 'Hits@5': 0.9774690199023658, 'Hits@10': 0.9883589936162223}


# **Transformer Encoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = TransformerEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_TRencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_TRencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_cands_with_embeddings_TRencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_TRencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_inner_product(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_inner_product_TRencoded.tsv"
)
```

    🔹 Using Inner Product (dot product) with FAISS
    Top-1 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_inner_product_TRencoded.tsv
    ⏱️ Execution time: 5.32 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_inner_product_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_inner_product_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1457
    📊 Evaluation (P / R / F1): {'Precision': 0.7311, 'Recall': 0.7069, 'F1': 0.7188}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_faiss_inner_product_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9466 rows
    ✅ After removing ignored classes: 9466 rows
    ✅ After keeping only test SrcEntities: 2397 rows
    ✅ After applying threshold ≥ 0.0: 2397 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_inner_product_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2397 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_inner_product_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1775
    📊 Evaluation (P / R / F1): {'P': 0.741, 'R': 0.667, 'F1': 0.702}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_faiss_inner_product_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7405, 'Recall@1': 0.6931, 'F1@1': 0.716}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_1_mappings_faiss_inner_product_mrr_hit_TRencoded.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-1 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_faiss_inner_product_mrr_hit_TRencoded.tsv
    ⏱️ Execution time: 5.02 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_faiss_inner_product_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6763238528607026, 'Hits@1': 0.6665414945550131, 'Hits@5': 0.6665414945550131, 'Hits@10': 0.6665414945550131}


# **K=10**


```python
# Compute the top-10 most similar mappings using faiss_inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the faiss_inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_inner_product(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_inner_product_TRencoded.tsv"
)
```

    🔹 Using Inner Product (dot product) with FAISS
    Top-10 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_inner_product_TRencoded.tsv
    ⏱️ Execution time: 6.39 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_inner_product_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_inner_product_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1457
    📊 Evaluation (P / R / F1): {'Precision': 0.7311, 'Recall': 0.7069, 'F1': 0.7188}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_faiss_inner_product_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 96004 rows
    ✅ After removing ignored classes: 96004 rows
    ✅ After keeping only test SrcEntities: 22604 rows
    ✅ After applying threshold ≥ 0.0: 22604 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_inner_product_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2560 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_inner_product_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1843
    📊 Evaluation (P / R / F1): {'P': 0.72, 'R': 0.692, 'F1': 0.706}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_faiss_inner_product_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7299, 'Recall@1': 0.6819, 'F1@1': 0.7051}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_10_mappings_faiss_inner_product_mrr_hit_TRencoded.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-10 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_faiss_inner_product_mrr_hit_TRencoded.tsv
    ⏱️ Execution time: 3.34 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_faiss_inner_product_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8987402924288271, 'Hits@1': 0.8584303417198648, 'Hits@5': 0.9417949680811115, 'Hits@10': 0.9417949680811115}


# **K=30**


```python
# Compute the top-10 most similar mappings using faiss_inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the faiss_inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_inner_product(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_inner_product_TRencoded.tsv"
)
```

    🔹 Using Inner Product (dot product) with FAISS
    Top-30 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_inner_product_TRencoded.tsv
    ⏱️ Execution time: 8.12 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_inner_product_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_inner_product_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1457
    📊 Evaluation (P / R / F1): {'Precision': 0.7311, 'Recall': 0.7069, 'F1': 0.7188}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_faiss_inner_product_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 291142 rows
    ✅ After removing ignored classes: 291142 rows
    ✅ After keeping only test SrcEntities: 68457 rows
    ✅ After applying threshold ≥ 0.0: 68457 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_inner_product_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2560 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_inner_product_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1843
    📊 Evaluation (P / R / F1): {'P': 0.72, 'R': 0.692, 'F1': 0.706}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_faiss_inner_product_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7299, 'Recall@1': 0.6819, 'F1@1': 0.7051}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_30_mappings_faiss_inner_product_mrr_hit_TRencoded.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-30 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_faiss_inner_product_mrr_hit_TRencoded.tsv
    ⏱️ Execution time: 3.35 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_faiss_inner_product_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.915202074555657, 'Hits@1': 0.867818250093879, 'Hits@5': 0.9714607585429966, 'Hits@10': 0.9744648892226812}


# **K=100**


```python
# Compute the top-10 most similar mappings using faiss_inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the faiss_inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_faiss_inner_product(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_inner_product_TRencoded.tsv"
)
```

    🔹 Using Inner Product (dot product) with FAISS
    Top-100 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_inner_product_TRencoded.tsv
    ⏱️ Execution time: 11.64 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_inner_product_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_inner_product_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1457
    📊 Evaluation (P / R / F1): {'Precision': 0.7311, 'Recall': 0.7069, 'F1': 0.7188}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_faiss_inner_product_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 979504 rows
    ✅ After removing ignored classes: 979504 rows
    ✅ After keeping only test SrcEntities: 230334 rows
    ✅ After applying threshold ≥ 0.0: 230334 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_inner_product_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2560 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_inner_product_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1843
    📊 Evaluation (P / R / F1): {'P': 0.72, 'R': 0.692, 'F1': 0.706}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_faiss_inner_product_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7299, 'Recall@1': 0.6819, 'F1@1': 0.7051}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_100_mappings_faiss_inner_product_mrr_hit_TRencoded.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-100 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_faiss_inner_product_mrr_hit_TRencoded.tsv
    ⏱️ Execution time: 4.09 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_faiss_inner_product_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9206396708137127, 'Hits@1': 0.8693203154337213, 'Hits@5': 0.9827262485918138, 'Hits@10': 0.9917386406308675}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using faiss_inner_product distance on ResMLP-encoded embeddings, then save the results.

topk_faiss_inner_product(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_inner_product_TRencoded.tsv"  # Save results to this output file
)
```

    🔹 Using Inner Product (dot product) with FAISS
    Top-200 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_inner_product_TRencoded.tsv
    ⏱️ Execution time: 19.04 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_inner_product_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_inner_product_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1457
    📊 Evaluation (P / R / F1): {'Precision': 0.7311, 'Recall': 0.7069, 'F1': 0.7188}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_faiss_inner_product_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1966711 rows
    ✅ After removing ignored classes: 1966711 rows
    ✅ After keeping only test SrcEntities: 462464 rows
    ✅ After applying threshold ≥ 0.0: 462464 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_inner_product_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2560 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_inner_product_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1843
    📊 Evaluation (P / R / F1): {'P': 0.72, 'R': 0.692, 'F1': 0.706}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_faiss_inner_product_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7299, 'Recall@1': 0.6819, 'F1@1': 0.7051}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_200_mappings_faiss_inner_product_mrr_hit_TRencoded.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-200 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_faiss_inner_product_mrr_hit_TRencoded.tsv
    ⏱️ Execution time: 5.55 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_faiss_inner_product_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9209066960283051, 'Hits@1': 0.8693203154337213, 'Hits@5': 0.9834772812617348, 'Hits@10': 0.9932407059707097}



```python

```


```python

```

# **Using faiss: topk_diem**

# **K=1**


```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_1_mappings_diem.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-1 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_diem.tsv
    ⏱️ Execution time: 7.94 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_diem.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_diem_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1459
    📊 Evaluation (P / R / F1): {'Precision': 0.7321, 'Recall': 0.7079, 'F1': 0.7198}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_diem.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9451 rows
    ✅ After removing ignored classes: 9451 rows
    ✅ After keeping only test SrcEntities: 2399 rows
    ✅ After applying threshold ≥ 0.0: 2399 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_diem_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2399 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_diem_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1780
    📊 Evaluation (P / R / F1): {'P': 0.742, 'R': 0.668, 'F1': 0.703}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/neoplas_top_1_mappings_diem_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.742, 'Recall@1': 0.6942, 'F1@1': 0.7173}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_1_mappings_diem_mrr_hit.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-1 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_diem_mrr_hit.tsv
    ⏱️ Execution time: 8.25 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_diem_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.5884842709145166, 'Hits@1': 0.5760420578295156, 'Hits@5': 0.5760420578295156, 'Hits@10': 0.5760420578295156}


# **K=10**


```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_10_mappings_diem.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-10 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_diem.tsv
    ⏱️ Execution time: 12.00 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_diem.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_diem_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1459
    📊 Evaluation (P / R / F1): {'Precision': 0.7321, 'Recall': 0.7079, 'F1': 0.7198}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_diem.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 95820 rows
    ✅ After removing ignored classes: 95820 rows
    ✅ After keeping only test SrcEntities: 22590 rows
    ✅ After applying threshold ≥ 0.0: 22590 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_diem_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2559 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_diem_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1850
    📊 Evaluation (P / R / F1): {'P': 0.723, 'R': 0.695, 'F1': 0.709}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_diem_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7323, 'Recall@1': 0.6842, 'F1@1': 0.7074}



```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_10_mappings_diem_mrr_hit.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-10 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_diem_mrr_hit.tsv
    ⏱️ Execution time: 3.89 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_diem_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.900416649202266, 'Hits@1': 0.8603079233946677, 'Hits@5': 0.9432970334209538, 'Hits@10': 0.9432970334209538}


# **K=30**


```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_30_mappings_diem.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-30 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_diem.tsv
    ⏱️ Execution time: 11.52 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_diem.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_diem_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1459
    📊 Evaluation (P / R / F1): {'Precision': 0.7321, 'Recall': 0.7079, 'F1': 0.7198}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_diem.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 290477 rows
    ✅ After removing ignored classes: 290477 rows
    ✅ After keeping only test SrcEntities: 68300 rows
    ✅ After applying threshold ≥ 0.0: 68300 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_diem_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2559 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_diem_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1850
    📊 Evaluation (P / R / F1): {'P': 0.723, 'R': 0.695, 'F1': 0.709}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_diem_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7323, 'Recall@1': 0.6842, 'F1@1': 0.7074}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_30_mappings_diem_mrr_hit.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-30 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_diem_mrr_hit.tsv
    ⏱️ Execution time: 3.70 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_diem_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8831730515483852, 'Hits@1': 0.8238828389034923, 'Hits@5': 0.955313556139692, 'Hits@10': 0.9586932031543373}


# **K=100**


```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_100_mappings_diem.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-100 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_diem.tsv
    ⏱️ Execution time: 16.86 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_diem.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_diem_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1459
    📊 Evaluation (P / R / F1): {'Precision': 0.7321, 'Recall': 0.7079, 'F1': 0.7198}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_diem.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 977660 rows
    ✅ After removing ignored classes: 977660 rows
    ✅ After keeping only test SrcEntities: 229936 rows
    ✅ After applying threshold ≥ 0.0: 229936 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_diem_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2559 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_diem_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1850
    📊 Evaluation (P / R / F1): {'P': 0.723, 'R': 0.695, 'F1': 0.709}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_diem_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7323, 'Recall@1': 0.6842, 'F1@1': 0.7074}



```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_100_mappings_diem_mrr_hit.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-100 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_diem_mrr_hit.tsv
    ⏱️ Execution time: 5.56 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_diem_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9222835490341191, 'Hits@1': 0.8715734134434848, 'Hits@5': 0.9853548629365377, 'Hits@10': 0.9917386406308675}


# **K=200**


```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_200_mappings_diem.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-200 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_diem.tsv
    ⏱️ Execution time: 25.58 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_diem.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_diem_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1459
    📊 Evaluation (P / R / F1): {'Precision': 0.7321, 'Recall': 0.7079, 'F1': 0.7198}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_diem.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1963408 rows
    ✅ After removing ignored classes: 1963408 rows
    ✅ After keeping only test SrcEntities: 461845 rows
    ✅ After applying threshold ≥ 0.0: 461845 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_diem_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2559 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_diem_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1850
    📊 Evaluation (P / R / F1): {'P': 0.723, 'R': 0.695, 'F1': 0.709}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_diem_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7323, 'Recall@1': 0.6842, 'F1@1': 0.7074}



```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_200_mappings_diem_mrr_hit.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-200 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_diem_mrr_hit.tsv
    ⏱️ Execution time: 6.75 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_diem_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9228282869993054, 'Hits@1': 0.8715734134434848, 'Hits@5': 0.9864814119414195, 'Hits@10': 0.9936162223056703}


# **K=500**


```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=500,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_500_mappings_diem.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-500 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_diem.tsv
    ⏱️ Execution time: 46.53 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_500_mappings_diem.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 5703500 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 996500 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_diem_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1459
    📊 Evaluation (P / R / F1): {'Precision': 0.7321, 'Recall': 0.7079, 'F1': 0.7198}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_500_mappings_diem.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 5703500 rows
    ✅ After removing train-only URIs: 4931351 rows
    ✅ After removing ignored classes: 4931351 rows
    ✅ After keeping only test SrcEntities: 1161325 rows
    ✅ After applying threshold ≥ 0.0: 1161325 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_diem_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2559 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_diem_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1850
    📊 Evaluation (P / R / F1): {'P': 0.723, 'R': 0.695, 'F1': 0.709}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_500_mappings_diem_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7323, 'Recall@1': 0.6842, 'F1@1': 0.7074}



```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=500,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_500_mappings_diem_mrr_hit.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-500 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_diem_mrr_hit.tsv
    ⏱️ Execution time: 12.44 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_500_mappings_diem_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9232734999880481, 'Hits@1': 0.8715734134434848, 'Hits@5': 0.9872324446113406, 'Hits@10': 0.9954938039804732}


# **K=1000**


```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1000,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_1000_mappings_diem.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-1000 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_diem.tsv
    ⏱️ Execution time: 83.44 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1000_mappings_diem.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407000 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993000 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_diem_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1459
    📊 Evaluation (P / R / F1): {'Precision': 0.7321, 'Recall': 0.7079, 'F1': 0.7198}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1000_mappings_diem.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 11407000 rows
    ✅ After removing train-only URIs: 9899485 rows
    ✅ After removing ignored classes: 9899485 rows
    ✅ After keeping only test SrcEntities: 2334909 rows
    ✅ After applying threshold ≥ 0.0: 2334909 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_diem_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2559 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_diem_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1850
    📊 Evaluation (P / R / F1): {'P': 0.723, 'R': 0.695, 'F1': 0.709}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1000_mappings_diem_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7323, 'Recall@1': 0.6842, 'F1@1': 0.7074}



```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1000,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_1000_mappings_diem_mrr_hit.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-1000 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_diem_mrr_hit.tsv
    ⏱️ Execution time: 19.77 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1000_mappings_diem_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9233136280892157, 'Hits@1': 0.8715734134434848, 'Hits@5': 0.9872324446113406, 'Hits@10': 0.9958693203154337}


# **K=2000**


```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=2000,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_2000_mappings_diem.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-2000 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_diem.tsv
    ⏱️ Execution time: 153.99 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_2000_mappings_diem.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 22814000 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 3986000 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_diem_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1459
    📊 Evaluation (P / R / F1): {'Precision': 0.7321, 'Recall': 0.7079, 'F1': 0.7198}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_2000_mappings_diem.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 22814000 rows
    ✅ After removing train-only URIs: 19869004 rows
    ✅ After removing ignored classes: 19869004 rows
    ✅ After keeping only test SrcEntities: 4694947 rows
    ✅ After applying threshold ≥ 0.0: 4694947 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_diem_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2559 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_diem_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1850
    📊 Evaluation (P / R / F1): {'P': 0.723, 'R': 0.695, 'F1': 0.709}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_2000_mappings_diem_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7323, 'Recall@1': 0.6842, 'F1@1': 0.7074}



```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=2000,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_2000_mappings_diem_mrr_hit.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-2000 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_diem_mrr_hit.tsv
    ⏱️ Execution time: 35.50 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_2000_mappings_diem_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9234778831821223, 'Hits@1': 0.8715734134434848, 'Hits@5': 0.9876079609463012, 'Hits@10': 0.997371385655276}


# **With Encoders**

# **ResMLPEncoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = ResMLPEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_cands_with_embeddings_ResMLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_ResMLPencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_diem(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_diem_ResMLPencoded.tsv"
)
```

    🔹 Using DIEM similarity measure
    Top-1 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_diem_ResMLPencoded.tsv
    ⏱️ Execution time: 8.21 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_diem_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_diem_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1464
    📊 Evaluation (P / R / F1): {'Precision': 0.7346, 'Recall': 0.7103, 'F1': 0.7222}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_diem_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9444 rows
    ✅ After removing ignored classes: 9444 rows
    ✅ After keeping only test SrcEntities: 2397 rows
    ✅ After applying threshold ≥ 0.0: 2397 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_diem_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2397 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_diem_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1785
    📊 Evaluation (P / R / F1): {'P': 0.745, 'R': 0.67, 'F1': 0.706}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_diem_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7447, 'Recall@1': 0.6967, 'F1@1': 0.7199}



```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_1_mappings_diem_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-1 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_diem_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 4.02 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_diem_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6799692013493283, 'Hits@1': 0.6702966579046189, 'Hits@5': 0.6702966579046189, 'Hits@10': 0.6702966579046189}


# **K=10**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_diem(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_diem_ResMLPencoded.tsv"
)
```

    🔹 Using DIEM similarity measure
    Top-10 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_diem_ResMLPencoded.tsv
    ⏱️ Execution time: 9.43 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_diem_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_diem_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1464
    📊 Evaluation (P / R / F1): {'Precision': 0.7346, 'Recall': 0.7103, 'F1': 0.7222}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_diem_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 95705 rows
    ✅ After removing ignored classes: 95705 rows
    ✅ After keeping only test SrcEntities: 22571 rows
    ✅ After applying threshold ≥ 0.0: 22571 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_diem_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2552 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_diem_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1850
    📊 Evaluation (P / R / F1): {'P': 0.725, 'R': 0.695, 'F1': 0.709}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_diem_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7343, 'Recall@1': 0.6861, 'F1@1': 0.7094}



```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_10_mappings_diem_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-10 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_diem_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 3.93 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_diem_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9020555138931134, 'Hits@1': 0.8640630867442733, 'Hits@5': 0.9429215170859933, 'Hits@10': 0.9429215170859933}


# **K=30**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_diem(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_diem_ResMLPencoded.tsv"
)
```

    🔹 Using DIEM similarity measure
    Top-30 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_diem_ResMLPencoded.tsv
    ⏱️ Execution time: 10.61 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_diem_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_diem_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1464
    📊 Evaluation (P / R / F1): {'Precision': 0.7346, 'Recall': 0.7103, 'F1': 0.7222}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_diem_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 290224 rows
    ✅ After removing ignored classes: 290224 rows
    ✅ After keeping only test SrcEntities: 68256 rows
    ✅ After applying threshold ≥ 0.0: 68256 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_diem_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2552 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_diem_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1850
    📊 Evaluation (P / R / F1): {'P': 0.725, 'R': 0.695, 'F1': 0.709}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_diem_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7343, 'Recall@1': 0.6861, 'F1@1': 0.7094}



```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_30_mappings_diem_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-30 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_diem_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 4.25 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_diem_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9195738101716044, 'Hits@1': 0.8738265114532482, 'Hits@5': 0.9740893728877206, 'Hits@10': 0.9770935035674052}


# **K=100**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_diem(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_diem_ResMLPencoded.tsv"
)
```

    🔹 Using DIEM similarity measure
    Top-100 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_diem_ResMLPencoded.tsv
    ⏱️ Execution time: 17.18 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_diem_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_diem_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1464
    📊 Evaluation (P / R / F1): {'Precision': 0.7346, 'Recall': 0.7103, 'F1': 0.7222}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_diem_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 976988 rows
    ✅ After removing ignored classes: 976988 rows
    ✅ After keeping only test SrcEntities: 229786 rows
    ✅ After applying threshold ≥ 0.0: 229786 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_diem_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2552 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_diem_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1850
    📊 Evaluation (P / R / F1): {'P': 0.725, 'R': 0.695, 'F1': 0.709}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_diem_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7343, 'Recall@1': 0.6861, 'F1@1': 0.7094}



```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_100_mappings_diem_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-100 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_diem_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 5.41 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_diem_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9240646788301962, 'Hits@1': 0.8749530604581299, 'Hits@5': 0.9849793466015772, 'Hits@10': 0.992114156965828}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using inner_product distance on ResMLP-encoded embeddings, then save the results.

topk_diem(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_diem_ResMLPencoded.tsv"  # Save results to this output file
)
```

    🔹 Using DIEM similarity measure
    Top-200 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_diem_ResMLPencoded.tsv
    ⏱️ Execution time: 26.13 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_diem_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_diem_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1464
    📊 Evaluation (P / R / F1): {'Precision': 0.7346, 'Recall': 0.7103, 'F1': 0.7222}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_diem_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1961966 rows
    ✅ After removing ignored classes: 1961966 rows
    ✅ After keeping only test SrcEntities: 461508 rows
    ✅ After applying threshold ≥ 0.0: 461508 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_diem_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2552 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_diem_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1850
    📊 Evaluation (P / R / F1): {'P': 0.725, 'R': 0.695, 'F1': 0.709}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_diem_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7343, 'Recall@1': 0.6861, 'F1@1': 0.7094}



```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_200_mappings_diem_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-200 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_diem_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 7.38 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_diem_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9244213449817997, 'Hits@1': 0.8749530604581299, 'Hits@5': 0.9857303792714983, 'Hits@10': 0.9936162223056703}


# **MLPEncoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = MLPEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_MLPencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_MLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_cands_with_embeddings_MLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_MLPencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_diem(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_diem_MLPencoded.tsv"
)
```

    🔹 Using DIEM similarity measure
    Top-1 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_diem_MLPencoded.tsv
    ⏱️ Execution time: 8.95 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_diem_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_diem_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1456
    📊 Evaluation (P / R / F1): {'Precision': 0.7306, 'Recall': 0.7065, 'F1': 0.7183}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_diem_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9513 rows
    ✅ After removing ignored classes: 9513 rows
    ✅ After keeping only test SrcEntities: 2403 rows
    ✅ After applying threshold ≥ 0.0: 2403 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_diem_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2403 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_diem_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1770
    📊 Evaluation (P / R / F1): {'P': 0.737, 'R': 0.665, 'F1': 0.699}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_diem_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7366, 'Recall@1': 0.6893, 'F1@1': 0.7121}



```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_1_mappings_diem_mrr_hit_MLPencoded.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-1 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_diem_mrr_hit_MLPencoded.tsv
    ⏱️ Execution time: 4.21 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_diem_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6745014941763351, 'Hits@1': 0.6646639128802103, 'Hits@5': 0.6646639128802103, 'Hits@10': 0.6646639128802103}


# **K=10**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_diem(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_diem_MLPencoded.tsv"
)
```

    🔹 Using DIEM similarity measure
    Top-10 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_diem_MLPencoded.tsv
    ⏱️ Execution time: 10.84 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_diem_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_diem_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1456
    📊 Evaluation (P / R / F1): {'Precision': 0.7306, 'Recall': 0.7065, 'F1': 0.7183}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_diem_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 96074 rows
    ✅ After removing ignored classes: 96074 rows
    ✅ After keeping only test SrcEntities: 22624 rows
    ✅ After applying threshold ≥ 0.0: 22624 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_diem_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2577 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_diem_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1833
    📊 Evaluation (P / R / F1): {'P': 0.711, 'R': 0.688, 'F1': 0.7}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_diem_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7255, 'Recall@1': 0.6778, 'F1@1': 0.7008}



```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_10_mappings_diem_mrr_hit_MLPencoded.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-10 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_diem_mrr_hit_MLPencoded.tsv
    ⏱️ Execution time: 3.97 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_diem_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9020555138931134, 'Hits@1': 0.8640630867442733, 'Hits@5': 0.9429215170859933, 'Hits@10': 0.9429215170859933}


# **K=30**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_diem(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_diem_MLPencoded.tsv"
)
```

    🔹 Using DIEM similarity measure
    Top-30 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_diem_MLPencoded.tsv
    ⏱️ Execution time: 11.75 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_diem_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_diem_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1456
    📊 Evaluation (P / R / F1): {'Precision': 0.7306, 'Recall': 0.7065, 'F1': 0.7183}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_diem_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 290631 rows
    ✅ After removing ignored classes: 290631 rows
    ✅ After keeping only test SrcEntities: 68295 rows
    ✅ After applying threshold ≥ 0.0: 68295 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_diem_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2577 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_diem_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1833
    📊 Evaluation (P / R / F1): {'P': 0.711, 'R': 0.688, 'F1': 0.7}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_diem_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7255, 'Recall@1': 0.6778, 'F1@1': 0.7008}



```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_30_mappings_diem_mrr_hit_MLPencoded.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-30 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_diem_mrr_hit_MLPencoded.tsv
    ⏱️ Execution time: 4.40 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_diem_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9166003245771688, 'Hits@1': 0.8738265114532482, 'Hits@5': 0.9680811115283515, 'Hits@10': 0.9714607585429966}


# **K=100**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_diem(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_diem_MLPencoded.tsv"
)
```

    🔹 Using DIEM similarity measure
    Top-100 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_diem_MLPencoded.tsv
    ⏱️ Execution time: 16.44 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_diem_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_diem_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1456
    📊 Evaluation (P / R / F1): {'Precision': 0.7306, 'Recall': 0.7065, 'F1': 0.7183}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_diem_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 976887 rows
    ✅ After removing ignored classes: 976887 rows
    ✅ After keeping only test SrcEntities: 229617 rows
    ✅ After applying threshold ≥ 0.0: 229617 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_diem_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2577 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_diem_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1833
    📊 Evaluation (P / R / F1): {'P': 0.711, 'R': 0.688, 'F1': 0.7}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_diem_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7255, 'Recall@1': 0.6778, 'F1@1': 0.7008}



```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_100_mappings_diem_mrr_hit_MLPencoded.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-100 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_diem_mrr_hit_MLPencoded.tsv
    ⏱️ Execution time: 5.09 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_diem_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9219123573077704, 'Hits@1': 0.8757040931280511, 'Hits@5': 0.9804731505820503, 'Hits@10': 0.9876079609463012}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using inner_product distance on ResMLP-encoded embeddings, then save the results.

topk_diem(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_diem_MLPencoded.tsv"  # Save results to this output file
)
```

    🔹 Using DIEM similarity measure
    Top-200 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_diem_MLPencoded.tsv
    ⏱️ Execution time: 25.49 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_diem_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_diem_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1456
    📊 Evaluation (P / R / F1): {'Precision': 0.7306, 'Recall': 0.7065, 'F1': 0.7183}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_diem_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1961369 rows
    ✅ After removing ignored classes: 1961369 rows
    ✅ After keeping only test SrcEntities: 461470 rows
    ✅ After applying threshold ≥ 0.0: 461470 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_diem_MLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2577 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_diem_MLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1833
    📊 Evaluation (P / R / F1): {'P': 0.711, 'R': 0.688, 'F1': 0.7}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_diem_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7255, 'Recall@1': 0.6778, 'F1@1': 0.7008}



```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_200_mappings_diem_mrr_hit_MLPencoded.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-200 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_diem_mrr_hit_MLPencoded.tsv
    ⏱️ Execution time: 7.11 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_diem_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9227123773502218, 'Hits@1': 0.8760796094630117, 'Hits@5': 0.9827262485918138, 'Hits@10': 0.9902365752910252}


# **LinearEncoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = LinearEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_Linencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_Linencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_cands_with_embeddings_Linencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_Linencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_diem(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_diem_Linencoded.tsv"
)
```

    🔹 Using DIEM similarity measure
    Top-1 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_diem_Linencoded.tsv
    ⏱️ Execution time: 10.44 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_diem_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_diem_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1464
    📊 Evaluation (P / R / F1): {'Precision': 0.7346, 'Recall': 0.7103, 'F1': 0.7222}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_diem_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9502 rows
    ✅ After removing ignored classes: 9502 rows
    ✅ After keeping only test SrcEntities: 2399 rows
    ✅ After applying threshold ≥ 0.0: 2399 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_diem_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2399 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_diem_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1785
    📊 Evaluation (P / R / F1): {'P': 0.744, 'R': 0.67, 'F1': 0.705}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_diem_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7441, 'Recall@1': 0.6962, 'F1@1': 0.7193}



```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_1_mappings_diem_mrr_hit_Linencoded.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-1 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_diem_mrr_hit_Linencoded.tsv
    ⏱️ Execution time: 4.09 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_diem_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6799679391095469, 'Hits@1': 0.6702966579046189, 'Hits@5': 0.6702966579046189, 'Hits@10': 0.6702966579046189}


# **K=10**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_diem(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_diem_Linencoded.tsv"
)
```

    🔹 Using DIEM similarity measure
    Top-10 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_diem_Linencoded.tsv
    ⏱️ Execution time: 10.14 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_diem_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_diem_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1464
    📊 Evaluation (P / R / F1): {'Precision': 0.7346, 'Recall': 0.7103, 'F1': 0.7222}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_diem_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 95985 rows
    ✅ After removing ignored classes: 95985 rows
    ✅ After keeping only test SrcEntities: 22595 rows
    ✅ After applying threshold ≥ 0.0: 22595 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_diem_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2549 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_diem_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1844
    📊 Evaluation (P / R / F1): {'P': 0.723, 'R': 0.692, 'F1': 0.708}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_diem_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7343, 'Recall@1': 0.6861, 'F1@1': 0.7094}



```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_10_mappings_diem_mrr_hit_Linencoded.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-10 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_diem_mrr_hit_Linencoded.tsv
    ⏱️ Execution time: 4.00 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_diem_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9013042021469044, 'Hits@1': 0.8629365377393917, 'Hits@5': 0.9414194517461509, 'Hits@10': 0.9414194517461509}


# **K=30**


```python
# Compute the top-10 most similar mappings using diem distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the diem distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_diem(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_diem_Linencoded.tsv"
)
```

    🔹 Using DIEM similarity measure
    Top-30 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_diem_Linencoded.tsv
    ⏱️ Execution time: 11.85 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_diem_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_diem_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1464
    📊 Evaluation (P / R / F1): {'Precision': 0.7346, 'Recall': 0.7103, 'F1': 0.7222}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_diem_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 290964 rows
    ✅ After removing ignored classes: 290964 rows
    ✅ After keeping only test SrcEntities: 68394 rows
    ✅ After applying threshold ≥ 0.0: 68394 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_diem_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2549 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_diem_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1844
    📊 Evaluation (P / R / F1): {'P': 0.723, 'R': 0.692, 'F1': 0.708}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_diem_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7343, 'Recall@1': 0.6861, 'F1@1': 0.7094}



```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_30_mappings_diem_mrr_hit_Linencoded.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-30 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_diem_mrr_hit_Linencoded.tsv
    ⏱️ Execution time: 4.35 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_diem_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9191138111434182, 'Hits@1': 0.873075478783327, 'Hits@5': 0.97371385655276, 'Hits@10': 0.9770935035674052}


# **K=100**


```python
# Compute the top-10 most similar mappings using diem distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the diem distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_diem(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_diem_Linencoded.tsv"
)
```

    🔹 Using DIEM similarity measure
    Top-100 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_diem_Linencoded.tsv
    ⏱️ Execution time: 17.49 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_diem_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_diem_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1464
    📊 Evaluation (P / R / F1): {'Precision': 0.7346, 'Recall': 0.7103, 'F1': 0.7222}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_diem_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 979090 rows
    ✅ After removing ignored classes: 979090 rows
    ✅ After keeping only test SrcEntities: 230087 rows
    ✅ After applying threshold ≥ 0.0: 230087 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_diem_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2549 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_diem_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1844
    📊 Evaluation (P / R / F1): {'P': 0.723, 'R': 0.692, 'F1': 0.708}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_diem_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7343, 'Recall@1': 0.6861, 'F1@1': 0.7094}



```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_100_mappings_diem_mrr_hit_Linencoded.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-100 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_diem_mrr_hit_Linencoded.tsv
    ⏱️ Execution time: 8.36 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_diem_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9233031653605134, 'Hits@1': 0.8738265114532482, 'Hits@5': 0.984228313931656, 'Hits@10': 0.9913631242959069}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using diem distance on ResMLP-encoded embeddings, then save the results.

topk_diem(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_diem_Linencoded.tsv"  # Save results to this output file
)
```

    🔹 Using DIEM similarity measure
    Top-200 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_diem_Linencoded.tsv
    ⏱️ Execution time: 25.62 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_diem_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_diem_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1464
    📊 Evaluation (P / R / F1): {'Precision': 0.7346, 'Recall': 0.7103, 'F1': 0.7222}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_diem_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1966645 rows
    ✅ After removing ignored classes: 1966645 rows
    ✅ After keeping only test SrcEntities: 462464 rows
    ✅ After applying threshold ≥ 0.0: 462464 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_diem_Linencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2549 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_diem_Linencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1844
    📊 Evaluation (P / R / F1): {'P': 0.723, 'R': 0.692, 'F1': 0.708}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_diem_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7343, 'Recall@1': 0.6861, 'F1@1': 0.7094}



```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on diem distance
    output_file=f"{results_dir}/{task}_top_200_mappings_diem_mrr_hit_Linencoded.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-200 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_diem_mrr_hit_Linencoded.tsv
    ⏱️ Execution time: 8.71 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_diem_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9236716963852937, 'Hits@1': 0.8738265114532482, 'Hits@5': 0.9849793466015772, 'Hits@10': 0.9932407059707097}


# **Transformer Encoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = TransformerEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_TRencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_TRencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_cands_with_embeddings_TRencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_TRencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_diem(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_diem_TRencoded.tsv"
)
```

    🔹 Using DIEM similarity measure
    Top-1 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_diem_TRencoded.tsv
    ⏱️ Execution time: 9.11 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_diem_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_diem_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1451
    📊 Evaluation (P / R / F1): {'Precision': 0.728, 'Recall': 0.704, 'F1': 0.7158}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_diem_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9486 rows
    ✅ After removing ignored classes: 9486 rows
    ✅ After keeping only test SrcEntities: 2402 rows
    ✅ After applying threshold ≥ 0.0: 2402 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_diem_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2402 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_diem_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1772
    📊 Evaluation (P / R / F1): {'P': 0.738, 'R': 0.665, 'F1': 0.7}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_diem_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7377, 'Recall@1': 0.6906, 'F1@1': 0.7134}



```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_1_mappings_diem_mrr_hit_TRencoded.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-1 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_diem_mrr_hit_TRencoded.tsv
    ⏱️ Execution time: 4.37 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_diem_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6752298065301915, 'Hits@1': 0.6654149455501315, 'Hits@5': 0.6654149455501315, 'Hits@10': 0.6654149455501315}


# **K=10**


```python
# Compute the top-10 most similar mappings using diem distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the diem distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_diem(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_diem_TRencoded.tsv"
)
```

    🔹 Using DIEM similarity measure
    Top-10 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_diem_TRencoded.tsv
    ⏱️ Execution time: 11.17 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_diem_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_diem_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1451
    📊 Evaluation (P / R / F1): {'Precision': 0.728, 'Recall': 0.704, 'F1': 0.7158}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_diem_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 96146 rows
    ✅ After removing ignored classes: 96146 rows
    ✅ After keeping only test SrcEntities: 22638 rows
    ✅ After applying threshold ≥ 0.0: 22638 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_diem_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2557 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_diem_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1829
    📊 Evaluation (P / R / F1): {'P': 0.715, 'R': 0.687, 'F1': 0.701}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_diem_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7287, 'Recall@1': 0.6808, 'F1@1': 0.7039}



```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_10_mappings_diem_mrr_hit_TRencoded.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-10 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_diem_mrr_hit_TRencoded.tsv
    ⏱️ Execution time: 4.47 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_diem_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8984349166358322, 'Hits@1': 0.8603079233946677, 'Hits@5': 0.9391663537363876, 'Hits@10': 0.9391663537363876}


# **K=30**


```python
# Compute the top-10 most similar mappings using diem distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the diem distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_diem(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_diem_TRencoded.tsv"
)
```

    🔹 Using DIEM similarity measure
    Top-30 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_diem_TRencoded.tsv
    ⏱️ Execution time: 11.89 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_diem_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_diem_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1451
    📊 Evaluation (P / R / F1): {'Precision': 0.728, 'Recall': 0.704, 'F1': 0.7158}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_diem_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 291483 rows
    ✅ After removing ignored classes: 291483 rows
    ✅ After keeping only test SrcEntities: 68488 rows
    ✅ After applying threshold ≥ 0.0: 68488 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_diem_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2557 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_diem_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1829
    📊 Evaluation (P / R / F1): {'P': 0.715, 'R': 0.687, 'F1': 0.701}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_diem_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7287, 'Recall@1': 0.6808, 'F1@1': 0.7039}



```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_30_mappings_diem_mrr_hit_TRencoded.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-30 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_diem_mrr_hit_TRencoded.tsv
    ⏱️ Execution time: 5.24 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_diem_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9168946417504511, 'Hits@1': 0.8711978971085242, 'Hits@5': 0.9722117912129178, 'Hits@10': 0.9744648892226812}


# **K=100**


```python
# Compute the top-10 most similar mappings using diem distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the diem distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_diem(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_diem_TRencoded.tsv"
)
```

    🔹 Using DIEM similarity measure
    Top-100 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_diem_TRencoded.tsv
    ⏱️ Execution time: 17.47 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_diem_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_diem_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1451
    📊 Evaluation (P / R / F1): {'Precision': 0.728, 'Recall': 0.704, 'F1': 0.7158}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_diem_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 981153 rows
    ✅ After removing ignored classes: 981153 rows
    ✅ After keeping only test SrcEntities: 230587 rows
    ✅ After applying threshold ≥ 0.0: 230587 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_diem_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2557 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_diem_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1829
    📊 Evaluation (P / R / F1): {'P': 0.715, 'R': 0.687, 'F1': 0.701}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_diem_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7287, 'Recall@1': 0.6808, 'F1@1': 0.7039}



```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_100_mappings_diem_mrr_hit_TRencoded.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-100 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_diem_mrr_hit_TRencoded.tsv
    ⏱️ Execution time: 6.24 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_diem_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9226968207979722, 'Hits@1': 0.8734509951182876, 'Hits@5': 0.9846038302666166, 'Hits@10': 0.9913631242959069}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using diem distance on ResMLP-encoded embeddings, then save the results.

topk_diem(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_diem_TRencoded.tsv"  # Save results to this output file
)
```

    🔹 Using DIEM similarity measure
    Top-200 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_diem_TRencoded.tsv
    ⏱️ Execution time: 25.83 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_diem_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_diem_TRencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1451
    📊 Evaluation (P / R / F1): {'Precision': 0.728, 'Recall': 0.704, 'F1': 0.7158}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_diem_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1970864 rows
    ✅ After removing ignored classes: 1970864 rows
    ✅ After keeping only test SrcEntities: 463457 rows
    ✅ After applying threshold ≥ 0.0: 463457 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_diem_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2557 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_diem_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1829
    📊 Evaluation (P / R / F1): {'P': 0.715, 'R': 0.687, 'F1': 0.701}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_diem_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7287, 'Recall@1': 0.6808, 'F1@1': 0.7039}



```python
topk_diem(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_200_mappings_diem_mrr_hit_TRencoded.tsv"
)

```

    🔹 Using DIEM similarity measure
    Top-200 DIEM similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_diem_mrr_hit_TRencoded.tsv
    ⏱️ Execution time: 9.05 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_diem_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9232885668953403, 'Hits@1': 0.8738265114532482, 'Hits@5': 0.9853548629365377, 'Hits@10': 0.9924896733007886}



```python

```


```python

```

# **Using faiss: topk_cosine**

# **K=1**


```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_1_mappings_cosine.tsv"
)

```

    🔹 Using Cosine Similarity (Sentence-Transformers)
    Top-1 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_cosine.tsv
    ⏱️ Execution time: 4.94 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_cosine_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1456
    📊 Evaluation (P / R / F1): {'Precision': 0.7306, 'Recall': 0.7065, 'F1': 0.7183}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9455 rows
    ✅ After removing ignored classes: 9455 rows
    ✅ After keeping only test SrcEntities: 2398 rows
    ✅ After applying threshold ≥ 0.0: 2398 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_cosine_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2398 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_cosine_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1776
    📊 Evaluation (P / R / F1): {'P': 0.741, 'R': 0.667, 'F1': 0.702}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/neoplas_top_1_mappings_cosine_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7406, 'Recall@1': 0.6929, 'F1@1': 0.716}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_1_mappings_cosine_mrr_hit.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-1 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_cosine_mrr_hit.tsv
    ⏱️ Execution time: 3.48 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_cosine_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.5884842709145166, 'Hits@1': 0.5760420578295156, 'Hits@5': 0.5760420578295156, 'Hits@10': 0.5760420578295156}


# **K=10**


```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_10_mappings_cosine.tsv"
)

```

    🔹 Using Cosine Similarity (Sentence-Transformers)
    Top-10 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_cosine.tsv
    ⏱️ Execution time: 6.30 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_cosine_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1456
    📊 Evaluation (P / R / F1): {'Precision': 0.7306, 'Recall': 0.7065, 'F1': 0.7183}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 95994 rows
    ✅ After removing ignored classes: 95994 rows
    ✅ After keeping only test SrcEntities: 22625 rows
    ✅ After applying threshold ≥ 0.0: 22625 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_cosine_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2560 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_cosine_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1845
    📊 Evaluation (P / R / F1): {'P': 0.721, 'R': 0.693, 'F1': 0.706}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_cosine_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7315, 'Recall@1': 0.6834, 'F1@1': 0.7067}



```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_10_mappings_cosine_mrr_hit.tsv"
)

```

    🔹 Using Cosine Similarity (Sentence-Transformers)
    Top-10 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_cosine_mrr_hit.tsv
    ⏱️ Execution time: 4.03 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_cosine_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.899072574208393, 'Hits@1': 0.8588058580548253, 'Hits@5': 0.9421704844160721, 'Hits@10': 0.9421704844160721}


# **K=30**


```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_30_mappings_cosine.tsv"
)

```

    🔹 Using Cosine Similarity (Sentence-Transformers)
    Top-30 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_cosine.tsv
    ⏱️ Execution time: 8.49 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_cosine_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1456
    📊 Evaluation (P / R / F1): {'Precision': 0.7306, 'Recall': 0.7065, 'F1': 0.7183}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 342210 rows
    ✅ After removing train-only URIs: 290962 rows
    ✅ After removing ignored classes: 290962 rows
    ✅ After keeping only test SrcEntities: 68393 rows
    ✅ After applying threshold ≥ 0.0: 68393 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_cosine_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2560 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_cosine_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1845
    📊 Evaluation (P / R / F1): {'P': 0.721, 'R': 0.693, 'F1': 0.706}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_cosine_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7315, 'Recall@1': 0.6834, 'F1@1': 0.7067}



```python
topk_faiss_inner_product(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_30_mappings_cosine_mrr_hit.tsv"
)

```

    🔹 Using Inner Product (dot product) with FAISS
    Top-30 FAISS similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_cosine_mrr_hit.tsv
    ⏱️ Execution time: 4.19 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_cosine_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8831730515483852, 'Hits@1': 0.8238828389034923, 'Hits@5': 0.955313556139692, 'Hits@10': 0.9586932031543373}


# **K=100**


```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_100_mappings_cosine.tsv"
)

```

    🔹 Using Cosine Similarity (Sentence-Transformers)
    Top-100 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_cosine.tsv
    ⏱️ Execution time: 19.06 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_cosine_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1456
    📊 Evaluation (P / R / F1): {'Precision': 0.7306, 'Recall': 0.7065, 'F1': 0.7183}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 979790 rows
    ✅ After removing ignored classes: 979790 rows
    ✅ After keeping only test SrcEntities: 230431 rows
    ✅ After applying threshold ≥ 0.0: 230431 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_cosine_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2560 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_cosine_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1845
    📊 Evaluation (P / R / F1): {'P': 0.721, 'R': 0.693, 'F1': 0.706}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_cosine_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7315, 'Recall@1': 0.6834, 'F1@1': 0.7067}



```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_100_mappings_cosine_mrr_hit.tsv"
)

```

    🔹 Using Cosine Similarity (Sentence-Transformers)
    Top-100 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_cosine_mrr_hit.tsv
    ⏱️ Execution time: 5.25 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_cosine_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9213368575296516, 'Hits@1': 0.8700713481036425, 'Hits@5': 0.9846038302666166, 'Hits@10': 0.992114156965828}


# **K=200**


```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_200_mappings_cosine.tsv"
)

```

    🔹 Using Cosine Similarity (Sentence-Transformers)
    Top-200 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_cosine.tsv
    ⏱️ Execution time: 35.76 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 2281400 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 398600 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_cosine_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1456
    📊 Evaluation (P / R / F1): {'Precision': 0.7306, 'Recall': 0.7065, 'F1': 0.7183}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 2281400 rows
    ✅ After removing train-only URIs: 1967850 rows
    ✅ After removing ignored classes: 1967850 rows
    ✅ After keeping only test SrcEntities: 462857 rows
    ✅ After applying threshold ≥ 0.0: 462857 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_cosine_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2560 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_cosine_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1845
    📊 Evaluation (P / R / F1): {'P': 0.721, 'R': 0.693, 'F1': 0.706}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_cosine_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7315, 'Recall@1': 0.6834, 'F1@1': 0.7067}



```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_200_mappings_cosine_mrr_hit.tsv"
)

```

    🔹 Using Cosine Similarity (Sentence-Transformers)
    Top-200 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_200_mappings_cosine_mrr_hit.tsv
    ⏱️ Execution time: 8.73 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_cosine_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9218645742472368, 'Hits@1': 0.8700713481036425, 'Hits@5': 0.9857303792714983, 'Hits@10': 0.9936162223056703}


# **K=500**


```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=500,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_500_mappings_cosine.tsv"
)

```

    🔹 Using Cosine Similarity (Sentence-Transformers)
    Top-500 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_cosine.tsv
    ⏱️ Execution time: 81.45 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_500_mappings_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 5703500 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 996500 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_cosine_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1456
    📊 Evaluation (P / R / F1): {'Precision': 0.7306, 'Recall': 0.7065, 'F1': 0.7183}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_500_mappings_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 5703500 rows
    ✅ After removing train-only URIs: 4943894 rows
    ✅ After removing ignored classes: 4943894 rows
    ✅ After keeping only test SrcEntities: 1164359 rows
    ✅ After applying threshold ≥ 0.0: 1164359 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_cosine_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2560 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_cosine_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1845
    📊 Evaluation (P / R / F1): {'P': 0.721, 'R': 0.693, 'F1': 0.706}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_500_mappings_cosine_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7315, 'Recall@1': 0.6834, 'F1@1': 0.7067}



```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=500,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_500_mappings_cosine_mrr_hit.tsv"
)

```

    🔹 Using Cosine Similarity (Sentence-Transformers)
    Top-500 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_500_mappings_cosine_mrr_hit.tsv
    ⏱️ Execution time: 17.96 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_500_mappings_cosine_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9223080312033205, 'Hits@1': 0.8700713481036425, 'Hits@5': 0.98685692827638, 'Hits@10': 0.9951182876455126}


# **K=1000**


```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1000,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_1000_mappings_cosine.tsv"
)

```

    🔹 Using Cosine Similarity (Sentence-Transformers)
    Top-1000 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_cosine.tsv
    ⏱️ Execution time: 157.44 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1000_mappings_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 11407000 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 1993000 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_cosine_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1456
    📊 Evaluation (P / R / F1): {'Precision': 0.7306, 'Recall': 0.7065, 'F1': 0.7183}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1000_mappings_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 11407000 rows
    ✅ After removing train-only URIs: 9925622 rows
    ✅ After removing ignored classes: 9925622 rows
    ✅ After keeping only test SrcEntities: 2341451 rows
    ✅ After applying threshold ≥ 0.0: 2341451 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_cosine_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2560 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_cosine_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1845
    📊 Evaluation (P / R / F1): {'P': 0.721, 'R': 0.693, 'F1': 0.706}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1000_mappings_cosine_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7315, 'Recall@1': 0.6834, 'F1@1': 0.7067}



```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1000,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_1000_mappings_cosine_mrr_hit.tsv"
)

```

    🔹 Using Cosine Similarity (Sentence-Transformers)
    Top-1000 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1000_mappings_cosine_mrr_hit.tsv
    ⏱️ Execution time: 34.02 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1000_mappings_cosine_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9223326529703382, 'Hits@1': 0.8700713481036425, 'Hits@5': 0.98685692827638, 'Hits@10': 0.9954938039804732}


# **K=2000**


```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",

    # Number of top matches to retrieve per source entity
    top_k=2000,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_2000_mappings_cosine.tsv"
)

```

    🔹 Using Cosine Similarity (Sentence-Transformers)
    Top-2000 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_cosine.tsv
    ⏱️ Execution time: 312.69 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_2000_mappings_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 22814000 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 3986000 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_cosine_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1456
    📊 Evaluation (P / R / F1): {'Precision': 0.7306, 'Recall': 0.7065, 'F1': 0.7183}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_2000_mappings_cosine.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).
    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.
    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )

```

    🔍 Initial file: 22814000 rows
    ✅ After removing train-only URIs: 19917638 rows
    ✅ After removing ignored classes: 19917638 rows
    ✅ After keeping only test SrcEntities: 4707389 rows
    ✅ After applying threshold ≥ 0.0: 4707389 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_cosine_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2560 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_cosine_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1845
    📊 Evaluation (P / R / F1): {'P': 0.721, 'R': 0.693, 'F1': 0.706}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_2000_mappings_cosine_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-1
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7315, 'Recall@1': 0.6834, 'F1@1': 0.7067}



```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",

    # Number of top matches to retrieve per source entity
    top_k=2000,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_2000_mappings_cosine_mrr_hit.tsv"
)

```

    🔹 Using Cosine Similarity (Sentence-Transformers)
    Top-2000 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_2000_mappings_cosine_mrr_hit.tsv
    ⏱️ Execution time: 64.92 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_2000_mappings_cosine_mrr_hit.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9225189724532561, 'Hits@1': 0.8700713481036425, 'Hits@5': 0.98685692827638, 'Hits@10': 0.997371385655276}


# **With Encoders**

# **ResMLPEncoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = ResMLPEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv"
)
```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv



```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```


```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/ncit.neoplas_cands_with_embeddings_ResMLPencoded.tsv


# **K=1**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_cosine_ResMLPencoded.tsv"
)
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 11407 rows
    ✅ After removing train-only URIs: 9457 rows
    ✅ After removing ignored classes: 9457 rows
    ✅ After keeping only test SrcEntities: 2399 rows
    ✅ After applying threshold ≥ 0.0: 2399 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_cosine_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2399 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_cosine_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1776
    📊 Evaluation (P / R / F1): {'P': 0.74, 'R': 0.667, 'F1': 0.702}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_cosine_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7403, 'Recall@1': 0.6921, 'F1@1': 0.7154}



```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_1_mappings_cosine_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using Cosine Similarity (Sentence-Transformers)
    Top-1 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_cosine_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 2.38 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_cosine_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.6766873779177401, 'Hits@1': 0.6669170108899737, 'Hits@5': 0.6669170108899737, 'Hits@10': 0.6669170108899737}


# **K=10**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_cosine_ResMLPencoded.tsv"
)
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 114070 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 19930 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_cosine_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1457
    📊 Evaluation (P / R / F1): {'Precision': 0.7311, 'Recall': 0.7069, 'F1': 0.7188}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 114070 rows
    ✅ After removing train-only URIs: 95948 rows
    ✅ After removing ignored classes: 95948 rows
    ✅ After keeping only test SrcEntities: 22620 rows
    ✅ After applying threshold ≥ 0.0: 22620 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_cosine_ResMLPencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2564 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_cosine_ResMLPencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1844
    📊 Evaluation (P / R / F1): {'P': 0.719, 'R': 0.692, 'F1': 0.706}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_cosine_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```

    {'Precision@1': 0.7303, 'Recall@1': 0.6823, 'F1@1': 0.7055}



```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_10_mappings_cosine_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using Cosine Similarity (Sentence-Transformers)
    Top-10 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_10_mappings_cosine_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 3.06 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_cosine_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.8990013817999427, 'Hits@1': 0.8595568907247465, 'Hits@5': 0.9414194517461509, 'Hits@10': 0.9414194517461509}


# **K=30**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_cosine_ResMLPencoded.tsv"
)
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_cosine_ResMLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1457
    📊 Evaluation (P / R / F1): {'Precision': 0.7311, 'Recall': 0.7069, 'F1': 0.7188}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```


```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_cosine_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```


```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_30_mappings_cosine_mrr_hit_ResMLPencoded.tsv"
)

```

    🔹 Using Cosine Similarity (Sentence-Transformers)
    Top-30 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_cosine_mrr_hit_ResMLPencoded.tsv
    ⏱️ Execution time: 3.66 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_cosine_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

# **K=100**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_cosine_ResMLPencoded.tsv"
)
```

    🔹 Using Cosine Similarity (Sentence-Transformers)
    Top-100 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_cosine_ResMLPencoded.tsv
    ⏱️ Execution time: 18.49 seconds



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```


```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_cosine_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```


```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_100_mappings_cosine_mrr_hit_ResMLPencoded.tsv"
)

```


```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_cosine_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9225781361522407, 'Hits@1': 0.872324446113406, 'Hits@5': 0.9846038302666166, 'Hits@10': 0.9913631242959069}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using inner_product distance on ResMLP-encoded embeddings, then save the results.

topk_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_ResMLPencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_cosine_ResMLPencoded.tsv"  # Save results to this output file
)
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_cosine_ResMLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```


```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_cosine_ResMLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```


```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_200_mappings_cosine_mrr_hit_ResMLPencoded.tsv"
)

```


```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_cosine_mrr_hit_ResMLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9230419870959452, 'Hits@1': 0.872324446113406, 'Hits@5': 0.9853548629365377, 'Hits@10': 0.9928651896357491}


# **MLPEncoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = MLPEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv"
)

```


```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv"
)
```


```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```


```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

# **K=1**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_cosine_MLPencoded.tsv"
)
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```


```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_cosine_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```


```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_1_mappings_cosine_mrr_hit_MLPencoded.tsv"
)

```


```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_cosine_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

# **K=10**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_cosine_MLPencoded.tsv"
)
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```


```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_cosine_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```


```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_ResMLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_10_mappings_cosine_mrr_hit_MLPencoded.tsv"
)

```


```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_cosine_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

# **K=30**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_cosine_MLPencoded.tsv"
)
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```


```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_cosine_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```


```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_30_mappings_cosine_mrr_hit_MLPencoded.tsv"
)

```


```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_cosine_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

# **K=100**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_cosine_MLPencoded.tsv"
)
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 1140700 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 199300 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_cosine_MLPencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1445
    📊 Evaluation (P / R / F1): {'Precision': 0.725, 'Recall': 0.7011, 'F1': 0.7129}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```


```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_cosine_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```


```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_100_mappings_cosine_mrr_hit_MLPencoded.tsv"
)

```


```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_cosine_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using inner_product distance on ResMLP-encoded embeddings, then save the results.

topk_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_MLPencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_cosine_MLPencoded.tsv"  # Save results to this output file
)
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_cosine_MLPencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```


```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_cosine_MLPencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```


```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_MLPencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_200_mappings_cosine_mrr_hit_MLPencoded.tsv"
)

```


```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_cosine_mrr_hit_MLPencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

# **LinearEncoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = LinearEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv"
)

```

    ✅ Encoded embeddings saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Data/snomed.neoplas_final_embeddings_ignored_class_cleaned_Linencoded.tsv



```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv"
)
```


```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```


```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

# **K=1**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_cosine_Linencoded.tsv"
)
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```


```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_cosine_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```


```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_1_mappings_inner_product_mrr_hit_Linencoded.tsv"
)

```


```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_cosine_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

# **K=10**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_cosine_Linencoded.tsv"
)
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```


```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_cosine_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```


```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_10_mappings_cosine_mrr_hit_Linencoded.tsv"
)

```


```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_cosine_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

# **K=30**


```python
# Compute the top-10 most similar mappings using cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_cosine_Linencoded.tsv"
)
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```

    🔍 Initial file: 342210 rows
    ❗ Common entities to remove: 502
    ✅ After filtering to test SrcEntities only: 59790 rows
    🏆 After Top-1 selection: 1993 rows
    📁 Final Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_30_mappings_cosine_Linencoded_Top1_testclean.tsv
    🎯 Correct mappings (Top-1): 1431
    📊 Evaluation (P / R / F1): {'Precision': 0.718, 'Recall': 0.6943, 'F1': 0.706}



```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```


```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_cosine_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```


```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_30_mappings_cosine_mrr_hit_Linencoded.tsv"
)

```


```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_cosine_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

# **K=100**


```python
# Compute the top-10 most similar mappings using cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_cosine_Linencoded.tsv"
)
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```


```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_cosine_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```


```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_100_mappings_cosine_mrr_hit_Linencoded.tsv"
)

```


```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_cosine_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.916907430941842, 'Hits@1': 0.8651896357491551, 'Hits@5': 0.9819752159218926, 'Hits@10': 0.989485542621104}


# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using cosine distance on ResMLP-encoded embeddings, then save the results.

topk_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_Linencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_cosine_Linencoded.tsv"  # Save results to this output file
)
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_cosine_Linencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```


```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_cosine_Linencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```


```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_Linencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_Linencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on cosine distance
    output_file=f"{results_dir}/{task}_top_200_mappings_cosine_mrr_hit_Linencoded.tsv"
)

```


```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_cosine_mrr_hit_Linencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

# **Transformer Encoder**


```python
# Instantiate the encoder model: Residual Multi-Layer Perceptron with input/output dimension = 768
# (You can also use LinearEncoder or MLPEncoder depending on the encoding strategy)
encoder = TransformerEncoder(embedding_dim=768)

# Apply the encoder model to the cleaned source embeddings (after removing ignored classes)
# The encoded embeddings are saved to a new TSV file, preserving the original 'Concept' URIs
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv"
)

```


```python
# Apply the encoder model to the cleaned target embeddings (after removing ignored classes)
# This will generate a new file where each embedding vector has been transformed using the encoder,
# and the concept URIs are preserved in the "Concept" column.
encode_embeddings_with_concept_column(
    encoder_model=encoder,
    input_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned.tsv",
    output_file=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv"
)
```


```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{src_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```


```python
# Encode the candidate entity embeddings using the specified encoder model.
# This will transform the existing embeddings (e.g., semantic + structural)
# into new representations via the encoder (e.g., ResMLP, Transformer, etc.)

encode_embeddings_with_concept_column(
    encoder_model=encoder,  # The trained encoder model (e.g., ResMLP or GatedCombination)

    input_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings.tsv",
    # Input file: contains source-target candidate pairs with their initial embeddings and a "Concept" column

    output_file=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv"
    # Output file: will contain the same candidate pairs but with updated embeddings produced by the encoder
)

```

# **K=1**


```python
# Compute the top-10 most similar mappings using inner_product distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the inner_product distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=1,
    output_file=f"{results_dir}/{task}_top_1_mappings_cosine_TRencoded.tsv"
)
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_1_mappings_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_1_mappings_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```


```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_1_mappings_cosine_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```


```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=1,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_1_mappings_cosine_mrr_hit_TRencoded.tsv"
)

```

    🔹 Using Cosine Similarity (Sentence-Transformers)
    Top-1 cosine similarity results saved to: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_1_mappings_cosine_mrr_hit_TRencoded.tsv
    ⏱️ Execution time: 5.14 seconds



```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_1_mappings_cosine_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

# **K=10**


```python
# Compute the top-10 most similar mappings using cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=10,
    output_file=f"{results_dir}/{task}_top_10_mappings_cosine_TRencoded.tsv"
)
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_10_mappings_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_10_mappings_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```


```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_10_mappings_cosine_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```


```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=10,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_10_mappings_cosine_mrr_hit_TRencoded.tsv"
)

```


```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_10_mappings_cosine_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

# **K=30**


```python
# Compute the top-10 most similar mappings using cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=30,
    output_file=f"{results_dir}/{task}_top_30_mappings_cosine_TRencoded.tsv"
)
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_30_mappings_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_30_mappings_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```


```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_30_mappings_cosine_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```


```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=30,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_30_mappings_cosine_mrr_hit_TRencoded.tsv"
)

```


```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_30_mappings_cosine_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

# **K=100**


```python
# Compute the top-10 most similar mappings using cosine distance
# between ResMLP-encoded embeddings of the source and target ontologies.
# The input embeddings were previously encoded using the ResMLPEncoder,
# and the similarity score is computed as the inverse of the cosine distance.
# Results are saved in a TSV file with columns: SrcEntity, TgtEntity, Score.
topk_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",
    top_k=100,
    output_file=f"{results_dir}/{task}_top_100_mappings_cosine_TRencoded.tsv"
)
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_100_mappings_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_100_mappings_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto,
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```

    🔍 Initial file: 1140700 rows
    ✅ After removing train-only URIs: 980686 rows
    ✅ After removing ignored classes: 980686 rows
    ✅ After keeping only test SrcEntities: 230640 rows
    ✅ After applying threshold ≥ 0.0: 230640 rows
    📁 Filtered predictions saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_cosine_TRencoded_filtered.tsv
    🏆 Selected candidates within 99.7% of best score per SrcEntity: 2571 rows
    📁 Filtered Top-1 file saved: /content/gdrive/My Drive/BioGITOM-VLDB//neoplas/Results/neoplas_top_100_mappings_cosine_TRencoded_filtered_top1_th0.0.tsv
    🎯 Correct mappings (Top-1): 1843
    📊 Evaluation (P / R / F1): {'P': 0.717, 'R': 0.692, 'F1': 0.704}



```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_100_mappings_cosine_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```


```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=100,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_100_mappings_cosine_mrr_hit_TRencoded.tsv"
)

```


```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_100_mappings_cosine_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

# **K=200**


```python
# Compute the top-200 most similar mappings between source and target entities
# using cosine distance on ResMLP-encoded embeddings, then save the results.

topk_cosine(
    src_emb_path=f"{data_dir}/{src_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",  # Source embeddings after ResMLP encoding and filtering
    tgt_emb_path=f"{data_dir}/{tgt_ent}_final_embeddings_ignored_class_cleaned_TRencoded.tsv",  # Target embeddings after ResMLP encoding and filtering
    top_k=200,  # Retrieve the top 200 most similar target entities per source entity
    output_file=f"{results_dir}/{task}_top_200_mappings_cosine_TRencoded.tsv"  # Save results to this output file
)
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1_remove_common_entities(
    topk_file=f"{results_dir}/{task}_top_200_mappings_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).
    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).
    test_file=f"{dataset_dir}/refs_equiv/test.tsv"
    # Path to the test reference file (used as the gold standard for evaluation).

    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
    )
```


```python
# Run the evaluation on the predicted top-1 mappings using a filtering and evaluation function.

output_file, metrics, correct = filter_and_evaluate_predictions_top1(
    topk_file=f"{results_dir}/{task}_top_200_mappings_cosine_TRencoded.tsv",
    # Path to the TSV file containing predicted mappings with scores (before filtering).

    train_file=f"{dataset_dir}/refs_equiv/train.tsv",
    # Path to the training reference file (used to exclude mappings involving train-only entities).

    test_file=f"{dataset_dir}/refs_equiv/test.tsv",
    # Path to the test reference file (used as the gold standard for evaluation).

    src_onto=src_onto,
    # The source ontology object, used to detect ignored classes or perform additional filtering.

    tgt_onto=tgt_onto
    # The target ontology object, used similarly for filtering ignored or irrelevant classes.
)

# This function returns:
# - `output_file`: the path to the filtered and evaluated output file.
# - `metrics`: a tuple containing (Precision, Recall, F1-score).
# - `correct`: the number of correctly predicted mappings found in the gold standard.

```


```python
# Supposons que tu as chargé les fichiers suivants :
predictions_df = pd.read_csv(f"{results_dir}/{task}_top_200_mappings_cosine_TRencoded_filtered_top1_th0.0.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity', 'Score'
reference_df = pd.read_csv(f"{dataset_dir}/refs_equiv/test.tsv", sep="\t")  # contient 'SrcEntity', 'TgtEntity'

# Appel de la fonction avec top-15
results = evaluate_topk(predictions_df, reference_df, k=1)

# Affichage des résultats
print(results)
```


```python
topk_cosine(
    # Path to the source ontology embeddings (after removing ignored classes)
    src_emb_path=f"{data_dir}/{src_ent}_cands_with_embeddings_TRencoded.tsv",

    # Path to the target ontology embeddings (after removing ignored classes)
    tgt_emb_path=f"{data_dir}/{tgt_ent}_cands_with_embeddings_TRencoded.tsv",

    # Number of top matches to retrieve per source entity
    top_k=200,

    # Path to save the top-10 similarity mappings based on inner_product distance
    output_file=f"{results_dir}/{task}_top_200_mappings_cosine_mrr_hit_TRencoded.tsv"
)

```


```python
# Compute MRR and Hits@k metrics
# This function evaluates the predicted rankings against the reference mappings
results = compute_mrr_and_hits(
    reference_file=test_cands,             # Reference file with true ranks
    predicted_file=f"{results_dir}/{task}_top_200_mappings_cosine_mrr_hit_TRencoded.tsv",             # File containing predicted rankings
    output_file=formatted_predictions_path,    # File path to save formatted predictions
    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10
)

```


```python
results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])
print("Ranking Evaluation Results at K=1, 5, and 10:")
print(results)
```

    Ranking Evaluation Results at K=1, 5, and 10:
    {'MRR': 0.9221363841644391, 'Hits@1': 0.8715734134434848, 'Hits@5': 0.9849793466015772, 'Hits@10': 0.9924896733007886}

