{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkAX88H3RNRW"
   },
   "source": [
    "# **Package Installation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 558719,
     "status": "ok",
     "timestamp": 1732173131715,
     "user": {
      "displayName": "BioGITOM Ontology Matching",
      "userId": "17792409086127109994"
     },
     "user_tz": -60
    },
    "id": "HPAlAgjLMVhw",
    "outputId": "0385683a-d7a3-4263-c7eb-e530668c1b86"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_OPTS\"] = \"-Xmx8g\"\n",
    "\n",
    "# Import pandas for data manipulation and analysis, such as loading, processing, and saving tabular data.\n",
    "import pandas as pd\n",
    "\n",
    "# Import pickle for saving and loading serialized objects (e.g., trained models or preprocessed data).\n",
    "import pickle\n",
    "\n",
    "# Import function to convert a directed graph to an undirected one, useful for certain graph algorithms.\n",
    "from torch_geometric.utils import to_undirected\n",
    "\n",
    "# Import optimizer module from PyTorch for training models using gradient-based optimization techniques.\n",
    "import torch.optim as optim\n",
    "\n",
    "# Import PyTorch's modules for defining neural network architectures and operations:\n",
    "from torch.nn import (\n",
    "    Linear,       # For linear transformations (dense layers).\n",
    "    Sequential,   # For stacking layers sequentially.\n",
    "    BatchNorm1d,  # For normalizing input within mini-batches.\n",
    "    PReLU,        # Parametric ReLU activation function.\n",
    "    Dropout       # For regularization by randomly dropping connections during training.\n",
    ")\n",
    "\n",
    "# Import functional API from PyTorch for operations like activations and loss functions.\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import Matplotlib for visualizations, such as plotting training loss curves.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import PyTorch Geometric's graph convolutional layers:\n",
    "from torch_geometric.nn import GCNConv, GINConv\n",
    "\n",
    "# Import pooling operations for aggregating node embeddings to graph-level representations:\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
    "\n",
    "# Import NumPy for numerical operations, such as working with arrays and matrices.\n",
    "import numpy as np\n",
    "\n",
    "# Import time module for measuring execution time of code blocks.\n",
    "import time\n",
    "\n",
    "# Import typing module for specifying types in function arguments and return values.\n",
    "from typing import Optional, Tuple, Union, Callable\n",
    "\n",
    "# Import PyTorch's DataLoader and TensorDataset for handling data batching and loading during training.\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Import PyTorch's Parameter class for defining learnable parameters in custom models.\n",
    "from torch.nn import Parameter\n",
    "\n",
    "# Import math module for performing mathematical computations.\n",
    "import math\n",
    "\n",
    "# Import Tensor type from PyTorch for defining and manipulating tensors.\n",
    "from torch import Tensor\n",
    "\n",
    "# Import PyTorch's nn module for defining and building neural network architectures.\n",
    "import torch.nn as nn\n",
    "\n",
    "# Import initialization utilities from PyTorch Geometric for resetting weights and biases in layers.\n",
    "from torch_geometric.nn.inits import reset\n",
    "\n",
    "# Import the base class for defining message-passing layers in graph neural networks (GNNs).\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "# Import linear transformation utilities for creating dense representations in graph models.\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "\n",
    "# Import typing utilities for defining adjacency matrices and tensor types specific to PyTorch Geometric.\n",
    "from torch_geometric.typing import Adj, OptTensor, PairTensor, SparseTensor\n",
    "\n",
    "# Import softmax function for normalizing attention scores in GNNs.\n",
    "from torch_geometric.utils import softmax\n",
    "\n",
    "# Import initialization utilities for weight initialization (e.g., Glorot initialization).\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "\n",
    "# Import F1 score metric from scikit-learn for evaluating model performance in binary/multi-class tasks.\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Import JSON module for reading and writing JSON files, useful for storing configuration or ontology data.\n",
    "import json\n",
    "\n",
    "# Import Ontology class from DeepOnto for representing and manipulating ontologies in the pipeline.\n",
    "from deeponto.onto import Ontology\n",
    "\n",
    "# Import tools from DeepOnto for handling Ontology Alignment Evaluation Initiative (OAEI) tasks.\n",
    "from deeponto.align.oaei import *\n",
    "\n",
    "# Import evaluation tools from DeepOnto for assessing alignment results using metrics like precision, recall, and F1.\n",
    "from deeponto.align.evaluation import AlignmentEvaluator\n",
    "\n",
    "# Import mapping utilities from DeepOnto for working with reference mappings and entity pairs.\n",
    "from deeponto.align.mapping import ReferenceMapping, EntityMapping\n",
    "\n",
    "# Import utility function for reading tables (e.g., TSV, CSV) from DeepOnto.\n",
    "from deeponto.utils import read_table\n",
    "\n",
    "# Importing the train_test_split function from sklearn's model_selection module.\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-abbBHOoRdWl"
   },
   "source": [
    "# **Paths Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36ttssQ3W7cx"
   },
   "outputs": [],
   "source": [
    "# Define the source ontology name\n",
    "src_ent = \"ncit\"\n",
    "\n",
    "# Define the target ontology name\n",
    "tgt_ent = \"doid\"\n",
    "\n",
    "# Define the task name for this ontology matching process\n",
    "task = \"ncit2doid\"\n",
    "\n",
    "# Define the weight for the training data\n",
    "# This weight is likely used to balance the training process, giving more emphasis to certain examples.\n",
    "# For instance, a weight of 10.0 could be applied to penalize errors in certain types of predictions more heavily.\n",
    "weight_train = 10.0\n",
    "\n",
    "# Define the similarity threshold for validating matches\n",
    "thres = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJpvkdwVSQye"
   },
   "outputs": [],
   "source": [
    "dir = \"../../\"\n",
    "\n",
    "# Define the directory for the dataset containing source and target ontologies\n",
    "dataset_dir = f\"{dir}/Datasets/{task}\"\n",
    "\n",
    "# Define the data directory for storing embeddings, adjacency matrices, and related files\n",
    "data_dir = f\"{dir}/{task}/Data\"\n",
    "\n",
    "# Define the directory for storing the results\n",
    "results_dir = f\"{dir}/{task}/Results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFDNSFef23er"
   },
   "outputs": [],
   "source": [
    "# Load the Source ontology using the Ontology class from DeepOnto\n",
    "# This initializes the source ontology by loading its .owl file.\n",
    "src_onto = Ontology(f\"{dataset_dir}/{src_ent}.owl\")\n",
    "\n",
    "# Load the Target ontology using the Ontology class from DeepOnto\n",
    "# This initializes the target ontology by loading its .owl file.\n",
    "tgt_onto = Ontology(f\"{dataset_dir}/{tgt_ent}.owl\")\n",
    "\n",
    "# Define the file path for the Source embeddings CSV file\n",
    "# Embeddings for the source ontology entities are stored in this file.\n",
    "src_Emb = f\"{data_dir}/{src_ent}_emb.csv\"\n",
    "\n",
    "# Define the file path for the Target embeddings CSV file\n",
    "# Embeddings for the target ontology entities are stored in this file.\n",
    "tgt_Emb = f\"{data_dir}/{tgt_ent}_emb.csv\"\n",
    "\n",
    "# Define the file path for the Source adjacency matrix\n",
    "# This file represents the relationships (edges) between entities in the source ontology.\n",
    "src_Adjacence = f\"{data_dir}/{src_ent}_adjacence.csv\"\n",
    "\n",
    "# Define the file path for the Target adjacency matrix\n",
    "# This file represents the relationships (edges) between entities in the target ontology.\n",
    "tgt_Adjacence = f\"{data_dir}/{tgt_ent}_adjacence.csv\"\n",
    "\n",
    "# Define the file path for the JSON file containing the Source ontology class labels\n",
    "# This file maps the source ontology entities to their labels or names.\n",
    "src_class = f\"{data_dir}/{src_ent}_classes.json\"\n",
    "\n",
    "# Define the file path for the JSON file containing the Target ontology class labels\n",
    "# This file maps the target ontology entities to their labels or names.\n",
    "tgt_class = f\"{data_dir}/{tgt_ent}_classes.json\"\n",
    "\n",
    "# Define the file path for the train data\n",
    "train_file = f\"{data_dir}/{task}_train.csv\"\n",
    "\n",
    "# Define the file path for the test data\n",
    "# The test file contains reference mappings (ground truth) between the source and target ontologies.\n",
    "test_file = f\"{dataset_dir}/refs_equiv/test.tsv\"\n",
    "\n",
    "# Define the file path for the candidate mappings used during testing\n",
    "# This file includes the candidate pairs (source and target entities) for ranking and evaluation.\n",
    "test_cands = f\"{dataset_dir}/refs_equiv/test.cands.tsv\"\n",
    "\n",
    "# Define the file path for the candidate mappings between Source to Target entities\n",
    "# This file contains cleaned, combined, and encoded candidates used for predictions.\n",
    "candidates_Prediction = f\"{data_dir}/{task}_candidates_prediction.csv\"\n",
    "\n",
    "# Define the file path for the candidate mappings between Source to Target entities for ranking-based metrics\n",
    "# This file is used to compute ranking-based metrics like MRR and Hits@k.\n",
    "candidates_Rank = f\"{data_dir}/{task}_candidates.csv\"\n",
    "\n",
    "# Define the path where the prediction results will be saved in TSV format\n",
    "# This file will store the final predictions (mappings) between source and target entities.\n",
    "prediction_path = f\"{results_dir}/{task}_matching_results.tsv\"\n",
    "\n",
    "# Define the path where all prediction results will be saved in TSV format\n",
    "# This file will store detailed prediction results, including all candidate scores.\n",
    "all_predictions_path = f\"{results_dir}/{task}_all_predictions.tsv\"\n",
    "\n",
    "# Define the path where all ranking prediction results will be saved in TSV format\n",
    "# This file will store predictions sorted by rank based on their scores.\n",
    "all_predictions_path_ranked = f\"{results_dir}/{task}_all_predictions_ranked.tsv\"\n",
    "\n",
    "# Define the path where formatted ranking predictions will be saved in TSV format\n",
    "# This file will contain predictions formatted for evaluation using ranking-based metrics.\n",
    "formatted_predictions_path = f\"{results_dir}/{task}_formatted_predictions.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqEXsgPGMVhw"
   },
   "source": [
    "# **GIT Architecture**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_d6XCsUMVhx"
   },
   "outputs": [],
   "source": [
    "# RGIT class definition which inherits from PyTorch Geometric's MessagePassing class\n",
    "class RGIT(MessagePassing):\n",
    "\n",
    "    _alpha: OptTensor  # Define _alpha as an optional tensor for storing attention weights\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        nn: Callable,  # Neural network to be used in the final layer of the GNN\n",
    "        in_channels: Union[int, Tuple[int, int]],  # Input dimension, can be a single or pair of integers\n",
    "        out_channels: int,  # Output dimension of the GNN\n",
    "        eps: float = 0.,  # GIN parameter: epsilon for GIN aggregation\n",
    "        train_eps: bool = False,  # GIN parameter: whether epsilon should be learnable\n",
    "        heads: int = 1,  # Transformer parameter: number of attention heads\n",
    "        dropout: float = 0.,  # Dropout rate for attention weights\n",
    "        edge_dim: Optional[int] = None,  # Dimension for edge attributes (optional)\n",
    "        bias: bool = True,  # Whether to use bias in linear layers\n",
    "        root_weight: bool = True,  # GIN parameter: whether to apply root weight in aggregation\n",
    "        **kwargs,  # Additional arguments passed to the parent class\n",
    "    ):\n",
    "        # Set the aggregation type to 'add' and initialize the parent class with node_dim=0\n",
    "        kwargs.setdefault('aggr', 'add')\n",
    "        super().__init__(node_dim=0, **kwargs)\n",
    "\n",
    "        # Initialize input/output dimensions, neural network, and GIN/transformer parameters\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.nn = nn  # Neural network used by the GNN\n",
    "        self.initial_eps = eps  # Initial value of epsilon for GIN\n",
    "\n",
    "        # Set epsilon to be learnable or fixed\n",
    "        if train_eps:\n",
    "            self.eps = torch.nn.Parameter(torch.empty(1))  # Learnable epsilon\n",
    "        else:\n",
    "            self.register_buffer('eps', torch.empty(1))  # Non-learnable epsilon (fixed)\n",
    "\n",
    "        # Initialize transformer-related parameters\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "        self.edge_dim = edge_dim\n",
    "        self._alpha = None  # Placeholder for attention weights\n",
    "\n",
    "        # Handle case where in_channels is a single integer or a tuple\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "\n",
    "        # Define the linear layers for key, query, and value for the transformer mechanism\n",
    "        self.lin_key = Linear(in_channels[0], heads * out_channels)\n",
    "        self.lin_query = Linear(in_channels[1], heads * out_channels)\n",
    "        self.lin_value = Linear(in_channels[0], heads * out_channels)\n",
    "\n",
    "        # Define linear transformation for edge embeddings if provided\n",
    "        if edge_dim is not None:\n",
    "            self.lin_edge = Linear(edge_dim, heads * out_channels, bias=False)\n",
    "        else:\n",
    "            self.lin_edge = self.register_parameter('lin_edge', None)\n",
    "\n",
    "        # Reset all parameters to their initial values\n",
    "        self.reset_parameters()\n",
    "\n",
    "    # Function to reset model parameters\n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()  # Call parent class reset method\n",
    "        self.lin_key.reset_parameters()  # Reset key linear layer\n",
    "        self.lin_query.reset_parameters()  # Reset query linear layer\n",
    "        self.lin_value.reset_parameters()  # Reset value linear layer\n",
    "        if self.edge_dim:\n",
    "            self.lin_edge.reset_parameters()  # Reset edge linear layer if used\n",
    "        reset(self.nn)  # Reset the neural network provided\n",
    "        self.eps.data.fill_(self.initial_eps)  # Initialize epsilon with the starting value\n",
    "\n",
    "    # Forward function defining how the input data flows through the model\n",
    "    def forward(self, x: Union[Tensor, PairTensor], edge_index: Adj,\n",
    "                edge_attr: OptTensor = None, return_attention_weights=None):\n",
    "        # Unpack number of heads and output channels\n",
    "        H, C = self.heads, self.out_channels\n",
    "\n",
    "        # If x is a tensor, treat it as a pair of tensors (source and target embeddings)\n",
    "        if isinstance(x, Tensor):\n",
    "            x: PairTensor = (x, x)\n",
    "\n",
    "        # Extract source node embeddings\n",
    "        x_t = x[0]\n",
    "\n",
    "        # Apply linear transformations and reshape query, key, and value for multi-head attention\n",
    "        query = self.lin_query(x[1]).view(-1, H, C)\n",
    "        key = self.lin_key(x[0]).view(-1, H, C)\n",
    "        value = self.lin_value(x[0]).view(-1, H, C)\n",
    "\n",
    "        # Propagate messages through the graph using the propagate function\n",
    "        out = self.propagate(edge_index, query=query, key=key, value=value,\n",
    "                             edge_attr=edge_attr, size=None)\n",
    "\n",
    "        # Retrieve attention weights and reset them\n",
    "        alpha = self._alpha\n",
    "        self._alpha = None  # Reset _alpha after use\n",
    "        out = out.mean(dim=1)  # Take the mean over all attention heads\n",
    "\n",
    "        # Apply GIN aggregation by adding epsilon-scaled original node embeddings\n",
    "        out = out + (1 + self.eps) * x_t\n",
    "        return self.nn(out)  # Pass through the neural network\n",
    "\n",
    "    # Message passing function which calculates attention and combines messages\n",
    "    def message(self, query_i: Tensor, key_j: Tensor, value_j: Tensor,\n",
    "                edge_attr: OptTensor, index: Tensor, ptr: OptTensor,\n",
    "                size_i: Optional[int]) -> Tensor:\n",
    "        # If edge attributes are used, apply linear transformation and add them to the key\n",
    "        if self.lin_edge is not None:\n",
    "            assert edge_attr is not None\n",
    "            edge_attr = self.lin_edge(edge_attr).view(-1, self.heads, self.out_channels)\n",
    "            key_j = key_j + edge_attr\n",
    "\n",
    "        # Calculate attention (alpha) using the dot product between query and key\n",
    "        alpha = (query_i * key_j).sum(dim=-1) / math.sqrt(self.out_channels)\n",
    "        alpha = softmax(alpha, index, ptr, size_i)  # Apply softmax to normalize attention\n",
    "        self._alpha = alpha  # Store attention weights\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)  # Apply dropout\n",
    "\n",
    "        # Calculate the output message by applying attention to the value\n",
    "        out = value_j\n",
    "        if edge_attr is not None:\n",
    "            out = out + edge_attr  # Add edge embeddings to the output if present\n",
    "        out = out * alpha.view(-1, self.heads, 1)  # Scale by attention weights\n",
    "        return out\n",
    "\n",
    "    # String representation function for debugging or printing\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
    "                f'{self.out_channels}, heads={self.heads})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwFv6RgHmGCf"
   },
   "outputs": [],
   "source": [
    "# Define the RGIT_mod class, a multi-layer GNN that uses both RGIT and linear layers\n",
    "class RGIT_mod(torch.nn.Module):\n",
    "    \"\"\"Multi-layer RGIT with optional linear layers\"\"\"\n",
    "\n",
    "    # Initialize the model with hidden dimension, number of RGIT layers, and number of linear layers\n",
    "    def __init__(self, dim_h, num_layers, num_linear_layers=1):\n",
    "        super(RGIT_mod, self).__init__()\n",
    "        self.num_layers = num_layers  # Number of RGIT layers\n",
    "        self.num_linear_layers = num_linear_layers  # Number of linear layers\n",
    "        self.linears = torch.nn.ModuleList()  # List to store linear layers\n",
    "        self.rgit_layers = torch.nn.ModuleList()  # List to store RGIT layers\n",
    "\n",
    "        # Create a list of Linear and PReLU layers (for encoding entity names)\n",
    "        for _ in range(num_linear_layers):\n",
    "            self.linears.append(Linear(dim_h, dim_h))  # Linear transformation layer\n",
    "            self.linears.append(PReLU(num_parameters=dim_h))  # Parametric ReLU activation function\n",
    "\n",
    "        # Create a list of RGIT layers\n",
    "        for _ in range(num_layers):\n",
    "            self.rgit_layers.append(RGIT(  # Each RGIT layer contains a small MLP with Linear and PReLU\n",
    "                Sequential(Linear(dim_h, dim_h), PReLU(num_parameters=dim_h),\n",
    "                           Linear(dim_h, dim_h), PReLU(num_parameters=dim_h)), dim_h, dim_h))\n",
    "\n",
    "    # Forward pass through the model\n",
    "    def forward(self, x, edge_index):\n",
    "        # Apply the linear layers first to the input\n",
    "        for layer in self.linears:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Then apply the RGIT layers for message passing\n",
    "        for layer in self.rgit_layers:\n",
    "            x = layer(x, edge_index)\n",
    "\n",
    "        return x  # Return the final node embeddings after all layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxCn5ztKVztw"
   },
   "source": [
    "# **Gated Network Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7MKQUv7o7zay"
   },
   "outputs": [],
   "source": [
    "# Define the GatedCombination class for combining two pairs of embeddings using a gating mechanism\n",
    "class GatedCombination(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        \"\"\"\n",
    "        Initialize the GatedCombination model.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): The dimensionality of the input embeddings (x1, x2, x3, x4).\n",
    "        \"\"\"\n",
    "        super(GatedCombination, self).__init__()\n",
    "\n",
    "        # Define a linear layer (gate) for combining embeddings x1 and x2 (first pair)\n",
    "        self.gate_A_fc = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "        # Define a linear layer (gate) for combining embeddings x3 and x4 (second pair)\n",
    "        self.gate_B_fc = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "        # A final fully connected layer that outputs a single neuron (binary classification)\n",
    "        self.fc = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x1, x2, x3, x4):\n",
    "        \"\"\"\n",
    "        Forward pass through the gating mechanism and cosine similarity.\n",
    "\n",
    "        Args:\n",
    "            x1 (torch.Tensor): First set of embeddings (source embeddings after update).\n",
    "            x2 (torch.Tensor): Second set of embeddings (original source embeddings).\n",
    "            x3 (torch.Tensor): Third set of embeddings (target embeddings after update).\n",
    "            x4 (torch.Tensor): Fourth set of embeddings (original target embeddings).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output of the model (probability score for binary classification).\n",
    "        \"\"\"\n",
    "        # Compute gate values for the first pair (x1 and x2) using a sigmoid activation\n",
    "        gate_values1 = torch.sigmoid(self.gate_A_fc(x1))\n",
    "\n",
    "        # Combine x1 and x2 using the gate values\n",
    "        # The result is a weighted combination of x1 and x2\n",
    "        a = x1 * gate_values1 + x2 * (1 - gate_values1)\n",
    "\n",
    "        # Compute gate values for the second pair (x3 and x4) using a sigmoid activation\n",
    "        gate_values2 = torch.sigmoid(self.gate_B_fc(x3))\n",
    "\n",
    "        # Combine x3 and x4 using the gate values\n",
    "        # The result is a weighted combination of x3 and x4\n",
    "        b = x3 * gate_values2 + x4 * (1 - gate_values2)\n",
    "\n",
    "        # Compute cosine similarity between the combined vectors a and b\n",
    "        x = torch.cosine_similarity(a, b, dim=1)\n",
    "\n",
    "        # Pass the cosine similarity result through a fully connected layer (fc) for classification\n",
    "        # Use a sigmoid activation to output a probability for binary classification\n",
    "        out = torch.sigmoid(self.fc(x.unsqueeze(1)))  # unsqueeze(1) to match the input shape for the fc layer\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PCzq6hHCD8vg"
   },
   "outputs": [],
   "source": [
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self, pos_weight):\n",
    "        \"\"\"\n",
    "        Weighted Binary Cross-Entropy Loss.\n",
    "\n",
    "        Args:\n",
    "            pos_weight (float): Weight for the positive class.\n",
    "        \"\"\"\n",
    "        super(WeightedBCELoss, self).__init__()\n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            outputs (torch.Tensor): Predicted probabilities from the model (after sigmoid).\n",
    "            targets (torch.Tensor): Ground truth labels (0 or 1).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Computed weighted binary cross-entropy loss.\n",
    "        \"\"\"\n",
    "        # Compute weighted BCE loss\n",
    "        loss = - (self.pos_weight * targets * torch.log(outputs + 1e-8) +\n",
    "                  (1 - targets) * torch.log(1 - outputs + 1e-8))\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kO42TTCqQZ8"
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2):\n",
    "        \"\"\"\n",
    "        Focal Loss for binary classification.\n",
    "\n",
    "        Args:\n",
    "            alpha (float): Balancing factor for positive/negative classes.\n",
    "            gamma (float): Focusing parameter for hard examples.\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            outputs (torch.Tensor): Predicted probabilities from the model (after sigmoid).\n",
    "            targets (torch.Tensor): Ground truth labels (0 or 1).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Computed focal loss.\n",
    "        \"\"\"\n",
    "        # Compute binary cross-entropy loss\n",
    "        bce_loss = F.binary_cross_entropy(outputs, targets, reduction='none')\n",
    "\n",
    "        # Compute modulating factor (1 - p_t)^gamma\n",
    "        pt = torch.where(targets == 1, outputs, 1 - outputs)  # pt = p if y==1 else 1-p\n",
    "        modulating_factor = (1 - pt) ** self.gamma\n",
    "\n",
    "        # Apply alpha and modulating factor\n",
    "        focal_loss = self.alpha * modulating_factor * bce_loss\n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLJ5j9FNMVhy"
   },
   "source": [
    "# **Utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0L86DgUQjMU"
   },
   "outputs": [],
   "source": [
    "def adjacency_matrix_to_undirected_edge_index(adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Converts an adjacency matrix into an undirected edge index for use in graph-based neural networks.\n",
    "\n",
    "    Args:\n",
    "        adjacency_matrix: A 2D list or array representing the adjacency matrix of a graph.\n",
    "\n",
    "    Returns:\n",
    "        edge_index_undirected: A PyTorch tensor representing the undirected edges.\n",
    "    \"\"\"\n",
    "    # Convert each element in the adjacency matrix to an integer (from boolean or float)\n",
    "    adjacency_matrix = [[int(element) for element in sublist] for sublist in adjacency_matrix]\n",
    "\n",
    "    # Convert the adjacency matrix into a PyTorch LongTensor (used for indexing)\n",
    "    edge_index = torch.tensor(adjacency_matrix, dtype=torch.long)\n",
    "\n",
    "    # Transpose the edge_index tensor so that rows represent edges in the form [source, target]\n",
    "    edge_index = edge_index.t().contiguous()\n",
    "\n",
    "    # Convert the directed edge_index into an undirected edge_index, meaning both directions are added (i.e., (i, j) and (j, i))\n",
    "    edge_index_undirected = to_undirected(edge_index)\n",
    "\n",
    "    return edge_index_undirected  # Return the undirected edge index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YvmOxkLcpf9w"
   },
   "outputs": [],
   "source": [
    "def build_indexed_dict(file_path):\n",
    "    \"\"\"\n",
    "    Builds a dictionary with numeric indexes for each key from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        indexed_dict (dict): A new dictionary where each key from the JSON file is assigned a numeric index.\n",
    "    \"\"\"\n",
    "    # Load the JSON file into a Python dictionary\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Create a new dictionary with numeric indexes as keys and the original JSON keys as values\n",
    "    indexed_dict = {index: key for index, key in enumerate(data.keys())}\n",
    "\n",
    "    return indexed_dict  # Return the newly created dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QgFINoPGl9Wg"
   },
   "outputs": [],
   "source": [
    "def select_rows_by_index(embedding_vector, index_vector):\n",
    "    \"\"\"\n",
    "    Select rows from an embedding vector using an index vector.\n",
    "\n",
    "    Args:\n",
    "        embedding_vector (torch.Tensor): 2D tensor representing the embedding vector with shape [num_rows, embedding_size].\n",
    "        index_vector (torch.Tensor): 1D tensor representing the index vector.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: New tensor with selected rows from the embedding vector.\n",
    "    \"\"\"\n",
    "    # Use torch.index_select to select the desired rows\n",
    "    new_tensor = torch.index_select(embedding_vector, 0, index_vector)\n",
    "\n",
    "    return new_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a12L7vEmmCJq"
   },
   "outputs": [],
   "source": [
    "def contrastive_loss(source_embeddings, target_embeddings, labels, margin=1.0):\n",
    "    \"\"\"\n",
    "    Computes the contrastive loss, a type of loss function used to train models in tasks like matching or similarity learning.\n",
    "\n",
    "    Args:\n",
    "        source_embeddings (torch.Tensor): Embeddings of the source graphs, shape [batch_size, embedding_size].\n",
    "        target_embeddings (torch.Tensor): Embeddings of the target graphs, shape [batch_size, embedding_size].\n",
    "        labels (torch.Tensor): Binary labels indicating if the pairs are matched (1) or not (0), shape [batch_size].\n",
    "        margin (float): Margin value for the contrastive loss. Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The contrastive loss value.\n",
    "    \"\"\"\n",
    "    # Calculate the pairwise Euclidean distance between source and target embeddings\n",
    "    distances = F.pairwise_distance(source_embeddings, target_embeddings)\n",
    "\n",
    "    # Compute the contrastive loss:\n",
    "    # - For matched pairs (label == 1), the loss is the squared distance between embeddings.\n",
    "    # - For non-matched pairs (label == 0), the loss is based on how far apart the embeddings are,\n",
    "    #   but penalizes them only if the distance is less than the margin.\n",
    "    loss = torch.mean(\n",
    "        labels * 0.4 * distances.pow(2) +  # For positive pairs, minimize the distance (squared)\n",
    "        (1 - labels) * 0.4 * torch.max(torch.zeros_like(distances), margin - distances).pow(2)  # For negative pairs, maximize the distance (up to the margin)\n",
    "    )\n",
    "\n",
    "    return loss  # Return the computed contrastive loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhCizXEb7D4N"
   },
   "outputs": [],
   "source": [
    "def Prediction_with_candidates(model, X1_tt, X2_tt, X3_tt, X4_tt, src_entity_tensor_o, tgt_entity_tensor_o,\n",
    "                                   indexed_dict_src, indexed_dict_tgt, all_predictions_path):\n",
    "    \"\"\"\n",
    "    Evaluates the GatedCombination model using the given embeddings and candidate entity pairs.\n",
    "    Saves the predictions and evaluation results to a file.\n",
    "\n",
    "    Args:\n",
    "        model: Trained GatedCombination model.\n",
    "        X1_tt, X2_tt, X3_tt, X4_tt (torch.Tensor): Tensors of source and target entity embeddings (updated and original).\n",
    "        src_entity_tensor_o, tgt_entity_tensor_o (torch.Tensor): Tensors of source and target entity indices.\n",
    "        indexed_dict_src, indexed_dict_tgt (dict): Dictionaries mapping entity indices to URIs for source and target.\n",
    "        output_file (str): Path to save the predictions and results.\n",
    "        hits_at_k_values (list): List of k-values for which hits@k is evaluated.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Move the model to CPU and set it to evaluation mode\n",
    "    model = model.to(\"cpu\")\n",
    "    model.eval()\n",
    "\n",
    "    # Set batch size for evaluation\n",
    "    batch_size_test = 32\n",
    "\n",
    "    # Create a DataLoader for the evaluation data\n",
    "    test_dataset = TensorDataset(X1_tt, X2_tt, X3_tt, X4_tt)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size_test)\n",
    "\n",
    "    # Prepare for collecting predictions and results\n",
    "    predictions = []\n",
    "    results = []\n",
    "    count_predictions = 0  # Counter for predictions above threshold (0.5)\n",
    "\n",
    "    # Measure prediction time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Disable gradient computation for evaluation\n",
    "    with torch.no_grad():\n",
    "        # Iterate over batches and compute model predictions\n",
    "        for batch_X1, batch_X2, batch_X3, batch_X4 in test_dataloader:\n",
    "            outputs = model(batch_X1, batch_X2, batch_X3, batch_X4)\n",
    "            predictions.extend(outputs.cpu().numpy())  # Collect predictions in CPU memory\n",
    "\n",
    "    end_time = time.time()\n",
    "    predicting_time = end_time - start_time\n",
    "    print(f\"Predicting time: {predicting_time:.2f} seconds\")\n",
    "\n",
    "    # Convert tensors to lists for easier iteration\n",
    "    src_indices = src_entity_tensor_o.tolist()\n",
    "    tgt_indices = tgt_entity_tensor_o.tolist()\n",
    "\n",
    "    # Prepare results\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] >= 0.00:  # Consider only predictions greater than 0.5\n",
    "            count_predictions += 1  # Increment the counter\n",
    "\n",
    "            # Map the source and target entity indices to their URIs\n",
    "            src_code = src_indices[i]\n",
    "            tgt_code = tgt_indices[i]\n",
    "\n",
    "            src_uri = indexed_dict_src.get(int(src_code), \"Unknown URI\")\n",
    "            tgt_uri = indexed_dict_tgt.get(int(tgt_code), \"Unknown URI\")\n",
    "\n",
    "            # Get the model's predicted score for the current pair\n",
    "            score = predictions[i]\n",
    "\n",
    "            # Append the results (with URIs instead of entity indices)\n",
    "            results.append({\n",
    "                'SrcEntity': src_uri,\n",
    "                'TgtEntity': tgt_uri,\n",
    "                'Score': score\n",
    "            })\n",
    "\n",
    "    # Convert the results into a pandas DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "    # Save the results to a TSV file\n",
    "    df_results.to_csv(all_predictions_path, sep='\\t', index=False)\n",
    "\n",
    "    print(f\"Predictions saved to {all_predictions_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TslUdYHBcGVj"
   },
   "outputs": [],
   "source": [
    "def filter_highest_predictions(input_file_path, output_file_path, threshold=thres):\n",
    "    # Load the all predictions file\n",
    "    df = pd.read_csv(input_file_path, sep='\\t')\n",
    "\n",
    "    # Extract the similarity score from the list in the 'Score' column\n",
    "    df['Score'] = df['Score'].apply(lambda x: float(x.strip('[]')))\n",
    "\n",
    "    # Sorting the dataframe by similarity score in descending order\n",
    "    df_sorted = df.sort_values(by='Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Initialize variables with threshold value\n",
    "    source_concepts = set(df_sorted['SrcEntity'])\n",
    "    target_concepts = set(df_sorted['TgtEntity'])\n",
    "    matched_sources = set()\n",
    "    matched_targets = set()\n",
    "    result = []\n",
    "\n",
    "    # Iterate through the sorted dataframe and find highest correspondences\n",
    "    for _, row in df_sorted.iterrows():\n",
    "        source, target, similarity = row['SrcEntity'], row['TgtEntity'], row['Score']\n",
    "\n",
    "        # Check if the source or target has already been matched and if the similarity is above the threshold\n",
    "        if source not in matched_sources and target not in matched_targets and similarity >= threshold:\n",
    "            # Add the match to the result list\n",
    "            result.append((source, target, similarity))\n",
    "            # Mark the source and target as matched\n",
    "            matched_sources.add(source)\n",
    "            matched_targets.add(target)\n",
    "\n",
    "    # Create a dataframe for the matching results with threshold applied\n",
    "    matching_results_df_threshold = pd.DataFrame(result, columns=['SrcEntity', 'TgtEntity', 'Score'])\n",
    "\n",
    "    # Save the matching results with the updated column names to a new TSV file\n",
    "    matching_results_df_threshold.to_csv(output_file_path, sep='\\t', index=False)\n",
    "\n",
    "    # Print the number of predictions saved\n",
    "    print(f\"Number of Positive predictions: {len(matching_results_df_threshold)}\")\n",
    "\n",
    "    return matching_results_df_threshold, len(matching_results_df_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ggVYlTiO_WA"
   },
   "outputs": [],
   "source": [
    "def compute_mrr_and_hits(reference_file, predicted_file, output_file, k_values=[1, 5, 10]):\n",
    "    \"\"\"\n",
    "    Compute MRR and Hits@k for ontology matching predictions based on a reference file.\n",
    "\n",
    "    Args:\n",
    "        reference_file (str): Path to the reference file (test.cands.tsv format).\n",
    "        predicted_file (str): Path to the predictions file with scores.\n",
    "        output_file (str): Path to save the scored results.\n",
    "        k_values (list): List of k values for Hits@k.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing MRR and Hits@k metrics.\n",
    "    \"\"\"\n",
    "    # Read the reference mappings\n",
    "    test_candidate_mappings = read_table(reference_file).values.tolist()\n",
    "    ranking_results = []\n",
    "\n",
    "    # Read the predicted scores\n",
    "    predicted_data = pd.read_csv(predicted_file, sep=\"\\t\")\n",
    "    predicted_data[\"Score\"] = predicted_data[\"Score\"].apply(lambda x: float(x.strip(\"[]\")))\n",
    "\n",
    "    # Create a lookup dictionary for predicted scores\n",
    "    score_lookup = {}\n",
    "    for _, row in predicted_data.iterrows():\n",
    "        score_lookup[(row[\"SrcEntity\"], row[\"TgtEntity\"])] = row[\"Score\"]\n",
    "\n",
    "    for src_ref_class, tgt_ref_class, tgt_cands in test_candidate_mappings:\n",
    "        tgt_cands = eval(tgt_cands)  # Convert string to list of candidates\n",
    "        scored_cands = []\n",
    "        for tgt_cand in tgt_cands:\n",
    "            # Retrieve score for each candidate, defaulting to a very low score if not found\n",
    "            matching_score = score_lookup.get((src_ref_class, tgt_cand), -1e9)\n",
    "            scored_cands.append((tgt_cand, matching_score))\n",
    "\n",
    "        # Sort candidates by score in descending order\n",
    "        scored_cands = sorted(scored_cands, key=lambda x: x[1], reverse=True)\n",
    "        ranking_results.append((src_ref_class, tgt_ref_class, scored_cands))\n",
    "\n",
    "    # Save the ranked results to a file\n",
    "    pd.DataFrame(ranking_results, columns=[\"SrcEntity\", \"TgtEntity\", \"TgtCandidates\"]).to_csv(output_file, sep=\"\\t\", index=False)\n",
    "\n",
    "    # Compute MRR and Hits@k\n",
    "    total_entities = len(ranking_results)\n",
    "    reciprocal_ranks = []\n",
    "    hits_at_k = {k: 0 for k in k_values}\n",
    "\n",
    "    for src_entity, tgt_ref_class, tgt_cands in ranking_results:\n",
    "        ranked_candidates = [candidate[0] for candidate in tgt_cands]\n",
    "        if tgt_ref_class in ranked_candidates:\n",
    "            rank = ranked_candidates.index(tgt_ref_class) + 1\n",
    "            reciprocal_ranks.append(1 / rank)\n",
    "            for k in k_values:\n",
    "                if rank <= k:\n",
    "                    hits_at_k[k] += 1\n",
    "        else:\n",
    "            reciprocal_ranks.append(0)\n",
    "\n",
    "    mrr = sum(reciprocal_ranks) / total_entities\n",
    "    hits_at_k = {k: hits / total_entities for k, hits in hits_at_k.items()}\n",
    "\n",
    "    return {\"MRR\": mrr, \"Hits@k\": hits_at_k}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HyWMsw1MVhz"
   },
   "source": [
    "# **Main Code**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IC37FlwGDqGM"
   },
   "source": [
    "\n",
    "\n",
    "# Reading semantic node embeddings provided by the ENE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FuEfSnw5mod0"
   },
   "outputs": [],
   "source": [
    "# Read the source embeddings from a CSV file into a pandas DataFrame\n",
    "df_embbedings_src = pd.read_csv(src_Emb, index_col=0)\n",
    "\n",
    "# Convert the DataFrame to a NumPy array, which will remove the index and store the data as a raw matrix\n",
    "numpy_array = df_embbedings_src.to_numpy()\n",
    "\n",
    "# Convert the NumPy array into a PyTorch FloatTensor, which is the format required for PyTorch operations\n",
    "x_src = torch.FloatTensor(numpy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "STUwqMUXmlG2"
   },
   "outputs": [],
   "source": [
    "# Read the target embeddings from a CSV file into a pandas DataFrame\n",
    "df_embbedings_tgt = pd.read_csv(tgt_Emb, index_col=0)\n",
    "\n",
    "# Convert the DataFrame to a NumPy array, which removes the index and converts the data to a raw matrix\n",
    "numpy_array = df_embbedings_tgt.to_numpy()\n",
    "\n",
    "# Convert the NumPy array into a PyTorch FloatTensor, which is required for PyTorch operations\n",
    "x_tgt = torch.FloatTensor(numpy_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZIu9P08DqGN"
   },
   "source": [
    "# Reading adjacency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pH69Up40mycz"
   },
   "outputs": [],
   "source": [
    "# Read the source adjacency matrix from a CSV file into a pandas DataFrame\n",
    "df_ma1 = pd.read_csv(src_Adjacence, index_col=0)\n",
    "\n",
    "# Convert the DataFrame to a list of lists (Python native list format)\n",
    "ma1 = df_ma1.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hYCmAO5Ymzpl"
   },
   "outputs": [],
   "source": [
    "# Read the target adjacency matrix from a CSV file into a pandas DataFrame\n",
    "df_ma2 = pd.read_csv(tgt_Adjacence, index_col=0)\n",
    "\n",
    "# Convert the DataFrame to a list of lists (Python native list format)\n",
    "ma2 = df_ma2.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSyksqh3TrU-"
   },
   "source": [
    "# Convert Adjacency matrix (in list format) to an undirected edge index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uVt-Pce5m5ll"
   },
   "outputs": [],
   "source": [
    "# Convert the source adjacency matrix (in list format) to an undirected edge index for PyTorch Geometric\n",
    "edge_src = adjacency_matrix_to_undirected_edge_index(ma1)\n",
    "\n",
    "# Convert the target adjacency matrix (in list format) to an undirected edge index for PyTorch Geometric\n",
    "edge_tgt = adjacency_matrix_to_undirected_edge_index(ma2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9wMTRdqT4aY"
   },
   "source": [
    "# GIT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eqiEKCLSMVh3"
   },
   "outputs": [],
   "source": [
    "def train_model_gnn(model, x_src, edge_src, x_tgt, edge_tgt,\n",
    "                    tensor_term1, tensor_term2, tensor_score,\n",
    "                    learning_rate, weight_decay_value, num_epochs, print_interval=10):\n",
    "    \"\"\"\n",
    "    Trains a graph neural network (GNN) model using source and target embeddings and contrastive loss.\n",
    "\n",
    "    Args:\n",
    "        model: The GNN model to be trained.\n",
    "        x_src (torch.Tensor): Source node embeddings.\n",
    "        edge_src (torch.Tensor): Source graph edges.\n",
    "        x_tgt (torch.Tensor): Target node embeddings.\n",
    "        edge_tgt (torch.Tensor): Target graph edges.\n",
    "        tensor_term1 (torch.Tensor): Indices of the source nodes to be compared.\n",
    "        tensor_term2 (torch.Tensor): Indices of the target nodes to be compared.\n",
    "        tensor_score (torch.Tensor): Labels indicating if the pairs are matched (1) or not (0).\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "        weight_decay_value (float): Weight decay (L2 regularization) value for the optimizer.\n",
    "        num_epochs (int): Number of epochs for training.\n",
    "        print_interval (int): Interval at which training progress is printed (every `print_interval` epochs).\n",
    "\n",
    "    Returns:\n",
    "        model: The trained GNN model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Set device (GPU or CPU) for computation\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Step 2: Move the model and all inputs to the selected device\n",
    "    model.to(device)\n",
    "    x_tgt = x_tgt.to(device)               # Target node embeddings\n",
    "    edge_tgt = edge_tgt.to(device)         # Target graph edges\n",
    "    x_src = x_src.to(device)               # Source node embeddings\n",
    "    edge_src = edge_src.to(device)         # Source graph edges\n",
    "    tensor_term1 = tensor_term1.to(device) # Indices for source nodes\n",
    "    tensor_term2 = tensor_term2.to(device) # Indices for target nodes\n",
    "    tensor_score = tensor_score.to(device) # Ground truth labels\n",
    "\n",
    "    # Step 3: Define optimizer with learning rate and regularization\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay_value)\n",
    "\n",
    "    # Step 4: Initialize list to store training losses\n",
    "    train_losses = []\n",
    "\n",
    "    # Record the start time of training\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Step 5: Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Zero out gradients from the previous iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: Compute embeddings for source and target graphs\n",
    "        out1 = model(x_src, edge_src)  # Updated source embeddings\n",
    "        out2 = model(x_tgt, edge_tgt)  # Updated target embeddings\n",
    "\n",
    "        # Extract specific rows of embeddings for terms being compared\n",
    "        src_embeddings = select_rows_by_index(out1, tensor_term1)\n",
    "        tgt_embeddings = select_rows_by_index(out2, tensor_term2)\n",
    "\n",
    "        # Compute contrastive loss based on the embeddings and ground truth labels\n",
    "        loss = contrastive_loss(src_embeddings, tgt_embeddings, tensor_score)\n",
    "\n",
    "        # Backward pass: Compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the model's parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Append the loss for this iteration to the list\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # Print loss every `print_interval` epochs\n",
    "        if (epoch + 1) % print_interval == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item()}\")\n",
    "\n",
    "    # Step 6: Record end time of training\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Step 7: Plot the training loss over time\n",
    "    plt.semilogy(range(1, num_epochs + 1), train_losses, label=\"Training Loss\", marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Print the total training time\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Training complete! Total training time: {training_time:.2f} seconds\")\n",
    "\n",
    "    # Step 8: Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_tzUG_emtBg"
   },
   "outputs": [],
   "source": [
    "# Initialize the GIT_mod model with the dimensionality of the target embeddings\n",
    "# The first argument is the dimensionality of the target node embeddings (x_tgt.shape[1])\n",
    "# The second argument (1) represents the number of RGIT layers in the model\n",
    "GIT_model = RGIT_mod(x_tgt.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVo-s7UQssSp"
   },
   "outputs": [],
   "source": [
    "# Reading the training pairs from a CSV file into a pandas DataFrame\n",
    "df_embbedings = pd.read_csv(train_file, index_col=0)\n",
    "\n",
    "# Extract the 'SrcEntity' and 'TgtEntity' columns as NumPy arrays and convert them to integers\n",
    "tensor_term1 = df_embbedings['SrcEntity'].values.astype(int)  # Source entity indices\n",
    "tensor_term2 = df_embbedings['TgtEntity'].values.astype(int)  # Target entity indices\n",
    "\n",
    "# Extract the 'Score' column as a NumPy array and convert it to floats\n",
    "tensor_score = df_embbedings['Score'].values.astype(float)  # Scores (labels) indicating if pairs match (1) or not (0)\n",
    "\n",
    "# Convert the NumPy arrays to PyTorch LongTensors (for indices) and FloatTensors (for scores)\n",
    "tensor_term1_o = torch.from_numpy(tensor_term1).type(torch.LongTensor)  # Source entity tensor\n",
    "tensor_term2_o = torch.from_numpy(tensor_term2).type(torch.LongTensor)  # Target entity tensor\n",
    "tensor_score_o = torch.from_numpy(tensor_score).type(torch.FloatTensor)  # Score tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1067355,
     "status": "ok",
     "timestamp": 1732174248980,
     "user": {
      "displayName": "BioGITOM Ontology Matching",
      "userId": "17792409086127109994"
     },
     "user_tz": -60
    },
    "id": "agHlFNesMVh3",
    "outputId": "36d021de-b570-4a7e-c590-3c866fd09926"
   },
   "outputs": [],
   "source": [
    "# Train the GNN model using the provided source and target graph embeddings, edges, and training data\n",
    "trained_model = train_model_gnn(\n",
    "    model=GIT_model,                # The GNN model to be trained (initialized earlier)\n",
    "    x_src=x_src,                    # Source node embeddings (tensor for source graph)\n",
    "    edge_src=edge_src,              # Source graph edges (undirected edge index for source graph)\n",
    "    x_tgt=x_tgt,                    # Target node embeddings (tensor for target graph)\n",
    "    edge_tgt=edge_tgt,              # Target graph edges (undirected edge index for target graph)\n",
    "    tensor_term1=tensor_term1_o,    # Indices of source entities for training\n",
    "    tensor_term2=tensor_term2_o,    # Indices of target entities for training\n",
    "    tensor_score=tensor_score_o,    # Scores (labels) indicating if pairs match (1) or not (0)\n",
    "    learning_rate=0.0001,            # Learning rate for the Adam optimizer\n",
    "    weight_decay_value=1e-4,        # Weight decay for L2 regularization to prevent overfitting\n",
    "    num_epochs=1000,                # Number of training epochs\n",
    "    print_interval=10               # Interval at which to print training progress (every 10 epochs)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYyhvjcTUaae"
   },
   "source": [
    "# GIT Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZ_VqP6tq6iD"
   },
   "outputs": [],
   "source": [
    "# Determine if a GPU is available and move the computations to it; otherwise, use the CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming the model has been trained and hyperparameters (x_src, edge_src, x_tgt, edge_tgt) are set\n",
    "\n",
    "# Move the trained GIT_model to the device (GPU or CPU)\n",
    "GIT_model.to(device)\n",
    "\n",
    "# Move the data tensors to the same device (GPU or CPU)\n",
    "x_tgt = x_tgt.to(device)         # Target node embeddings\n",
    "edge_tgt = edge_tgt.to(device)   # Target graph edges\n",
    "x_src = x_src.to(device)         # Source node embeddings\n",
    "edge_src = edge_src.to(device)   # Source graph edges\n",
    "\n",
    "# Set the model to evaluation mode; this disables dropout and batch normalization\n",
    "GIT_model.eval()\n",
    "\n",
    "# Pass the source and target embeddings through the trained GNN model to update the embeddings\n",
    "with torch.no_grad():  # Disable gradient computation (inference mode)\n",
    "    embeddings_tgt = GIT_model(x_tgt, edge_tgt)  # Get updated embeddings for the target graph\n",
    "    embeddings_src = GIT_model(x_src, edge_src)  # Get updated embeddings for the source graph\n",
    "\n",
    "# Detach the embeddings from the computation graph and move them back to the CPU\n",
    "# This step is useful if you need to use the embeddings for tasks outside PyTorch (e.g., saving to disk)\n",
    "embeddings_tgt = embeddings_tgt.detach().cpu()  # Target graph embeddings\n",
    "embeddings_src = embeddings_src.detach().cpu()  # Source graph embeddings\n",
    "\n",
    "# At this point, embeddings_tgt and embeddings_src contain the updated embeddings, ready for downstream tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Og5fdoGJrTCG"
   },
   "source": [
    "# Selecting embedding pairs to train the Gated Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0nTwc-dnjLn"
   },
   "outputs": [],
   "source": [
    "# Read the training pairs from a CSV file into a pandas DataFrame\n",
    "df_embeddings = pd.read_csv(train_file, index_col=0)\n",
    "\n",
    "# Extract columns and convert to NumPy arrays\n",
    "tensor_term1 = df_embeddings['SrcEntity'].values.astype(int)  # Source entity indices\n",
    "tensor_term2 = df_embeddings['TgtEntity'].values.astype(int)  # Target entity indices\n",
    "tensor_score = df_embeddings['Score'].values.astype(float)  # Matching scores\n",
    "\n",
    "# Split data into training and validation sets\n",
    "tensor_term1_train, tensor_term1_val, tensor_term2_train, tensor_term2_val, tensor_score_train, tensor_score_val = train_test_split(\n",
    "    tensor_term1, tensor_term2, tensor_score, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Convert split data to PyTorch tensors\n",
    "tensor_term1_train = torch.from_numpy(tensor_term1_train).type(torch.LongTensor)\n",
    "tensor_term2_train = torch.from_numpy(tensor_term2_train).type(torch.LongTensor)\n",
    "tensor_score_train = torch.from_numpy(tensor_score_train).type(torch.FloatTensor)\n",
    "\n",
    "tensor_term1_val = torch.from_numpy(tensor_term1_val).type(torch.LongTensor)\n",
    "tensor_term2_val = torch.from_numpy(tensor_term2_val).type(torch.LongTensor)\n",
    "tensor_score_val = torch.from_numpy(tensor_score_val).type(torch.FloatTensor)\n",
    "\n",
    "# Move the embeddings back to the CPU if not already there\n",
    "x_tgt = x_tgt.cpu()  # Target node embeddings\n",
    "x_src = x_src.cpu()  # Source node embeddings\n",
    "\n",
    "# Select embeddings for the training set\n",
    "X1_train = select_rows_by_index(embeddings_src, tensor_term1_train)\n",
    "X2_train = select_rows_by_index(x_src, tensor_term1_train)\n",
    "X3_train = select_rows_by_index(embeddings_tgt, tensor_term2_train)\n",
    "X4_train = select_rows_by_index(x_tgt, tensor_term2_train)\n",
    "\n",
    "# Select embeddings for the validation set\n",
    "X1_val = select_rows_by_index(embeddings_src, tensor_term1_val)\n",
    "X2_val = select_rows_by_index(x_src, tensor_term1_val)\n",
    "X3_val = select_rows_by_index(embeddings_tgt, tensor_term2_val)\n",
    "X4_val = select_rows_by_index(x_tgt, tensor_term2_val)\n",
    "\n",
    "# Now you have:\n",
    "# - Training tensors: X1_train, X2_train, X3_train, X4_train, tensor_score_train\n",
    "# - Validation tensors: X1_val, X2_val, X3_val, X4_val, tensor_score_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wR3PbrbBETJA"
   },
   "outputs": [],
   "source": [
    "positive_weight = len(tensor_score_train) / (weight_train * tensor_score_train.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNdCgaTMExPK"
   },
   "source": [
    "# Gated Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gof1eIPIWSVU"
   },
   "outputs": [],
   "source": [
    "def train_gated_combination_model(X1_t, X2_t, X3_t, X4_t, tensor_score_o,\n",
    "                                  X1_val, X2_val, X3_val, X4_val, tensor_score_val,\n",
    "                                  epochs=120, batch_size=32, learning_rate=0.001, weight_decay=1e-5):\n",
    "    \"\"\"\n",
    "    Trains the GatedCombination model with training and validation data, using ReduceLROnPlateau scheduler.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create datasets and DataLoaders\n",
    "    train_dataset = TensorDataset(X1_t, X2_t, X3_t, X4_t, tensor_score_o)\n",
    "    val_dataset = TensorDataset(X1_val, X2_val, X3_val, X4_val, tensor_score_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = GatedCombination(X1_t.shape[1]).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Use ReduceLROnPlateau scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "    criterion = WeightedBCELoss(pos_weight=positive_weight).to(device)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss, y_true_train, y_pred_train = 0.0, [], []\n",
    "\n",
    "        for batch_X1, batch_X2, batch_X3, batch_X4, batch_y in train_loader:\n",
    "            batch_X1, batch_X2, batch_X3, batch_X4, batch_y = (batch_X1.to(device), batch_X2.to(device),\n",
    "                                                               batch_X3.to(device), batch_X4.to(device), batch_y.to(device))\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X1, batch_X2, batch_X3, batch_X4)\n",
    "            loss = criterion(outputs, batch_y.unsqueeze(1).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            y_true_train.extend(batch_y.cpu().numpy())\n",
    "            y_pred_train.extend((outputs > 0.2).float().cpu().numpy())\n",
    "\n",
    "        train_f1 = f1_score(y_true_train, y_pred_train)\n",
    "        train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss, y_true_val, y_pred_val = 0.0, [], []\n",
    "        with torch.no_grad():\n",
    "            for batch_X1, batch_X2, batch_X3, batch_X4, batch_y in val_loader:\n",
    "                batch_X1, batch_X2, batch_X3, batch_X4, batch_y = (batch_X1.to(device), batch_X2.to(device),\n",
    "                                                                   batch_X3.to(device), batch_X4.to(device), batch_y.to(device))\n",
    "                outputs = model(batch_X1, batch_X2, batch_X3, batch_X4)\n",
    "                val_loss = criterion(outputs, batch_y.unsqueeze(1).float())\n",
    "                total_val_loss += val_loss.item()\n",
    "                y_true_val.extend(batch_y.cpu().numpy())\n",
    "                y_pred_val.extend((outputs > 0.4).float().cpu().numpy())\n",
    "\n",
    "        val_f1 = f1_score(y_true_val, y_pred_val)\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Step the scheduler with validation loss\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Print training and validation metrics\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}] Training Loss: {train_loss:.4f}, F1 Score: {train_f1:.4f} | \"\n",
    "              f\"Validation Loss: {avg_val_loss:.4f}, F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label=\"Training Loss\", marker='o')\n",
    "    plt.plot(val_losses, label=\"Validation Loss\", marker='x')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Training complete! Total time: {end_time - start_time:.2f} seconds\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1721495,
     "status": "ok",
     "timestamp": 1732175971324,
     "user": {
      "displayName": "BioGITOM Ontology Matching",
      "userId": "17792409086127109994"
     },
     "user_tz": -60
    },
    "id": "l11dgb8ei69T",
    "outputId": "f5d955e0-a45c-4995-9b45-669dd9b4a888"
   },
   "outputs": [],
   "source": [
    "# Train the GatedCombination model using training and validation data\n",
    "trained_model = train_gated_combination_model(\n",
    "    X1_train,          # Updated source embeddings (after applying the GNN model)\n",
    "    X2_train,          # Original source embeddings (before applying the GNN model)\n",
    "    X3_train,          # Updated target embeddings (after applying the GNN model)\n",
    "    X4_train,          # Original target embeddings (before applying the GNN model)\n",
    "    tensor_score_train, # Ground truth labels for the training set (1 for matched pairs, 0 for unmatched pairs)\n",
    "\n",
    "    X1_val,            # Updated source embeddings for the validation set\n",
    "    X2_val,            # Original source embeddings for the validation set\n",
    "    X3_val,            # Updated target embeddings for the validation set\n",
    "    X4_val,            # Original target embeddings for the validation set\n",
    "    tensor_score_val,  # Ground truth labels for the validation set (1 for matched pairs, 0 for unmatched pairs)\n",
    "\n",
    "    epochs=100,        # Number of epochs (iterations over the entire training dataset)\n",
    "    batch_size=32,     # Number of training samples processed in one forward/backward pass\n",
    "    learning_rate=0.001, # Learning rate for the optimizer (controls step size during optimization)\n",
    "    weight_decay=1e-5  # Weight decay (L2 regularization) to prevent overfitting\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAmLuMtGW9c2"
   },
   "source": [
    "# **Mappings Selector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnFtpUAfJQHl"
   },
   "outputs": [],
   "source": [
    "# Build an indexed dictionary for the source ontology classes\n",
    "# src_class is the file path to the JSON file containing the source ontology classes\n",
    "indexed_dict_src = build_indexed_dict(src_class)\n",
    "\n",
    "# Build an indexed dictionary for the target ontology classes\n",
    "# tgt_class is the file path to the JSON file containing the target ontology classes\n",
    "indexed_dict_tgt = build_indexed_dict(tgt_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "beipwavuJQHl"
   },
   "outputs": [],
   "source": [
    "# Read the candidate pairs from a Candidates CSV file into a pandas DataFrame\n",
    "df_embbedings = pd.read_csv(candidates_Prediction, index_col=0)\n",
    "\n",
    "# Extract the 'SrcEntity' column (source entity indices) and convert it to a NumPy array of integers\n",
    "tensor_term1 = df_embbedings['SrcEntity'].values.astype(int)\n",
    "\n",
    "# Extract the 'TgtEntity' column (target entity indices) and convert it to a NumPy array of integers\n",
    "tensor_term2 = df_embbedings['TgtEntity'].values.astype(int)\n",
    "\n",
    "# Convert the source entity indices to a PyTorch LongTensor\n",
    "src_entity_tensor_o = torch.from_numpy(tensor_term1).type(torch.LongTensor)\n",
    "\n",
    "# Convert the target entity indices to a PyTorch LongTensor\n",
    "tgt_entity_tenso_or = torch.from_numpy(tensor_term2).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECLhmxyKJQHl"
   },
   "outputs": [],
   "source": [
    "# Select rows from the updated source embeddings based on the indices in src_entity_tensor_o\n",
    "X1_tt = select_rows_by_index(embeddings_src, src_entity_tensor_o)\n",
    "\n",
    "# Select rows from the original source embeddings based on the indices in src_entity_tensor_o\n",
    "X2_tt = select_rows_by_index(x_src, src_entity_tensor_o)\n",
    "\n",
    "# Select rows from the updated target embeddings based on the indices in tgt_entity_tenso_or\n",
    "X3_tt = select_rows_by_index(embeddings_tgt, tgt_entity_tenso_or)\n",
    "\n",
    "# Select rows from the original target embeddings based on the indices in tgt_entity_tenso_or\n",
    "X4_tt = select_rows_by_index(x_tgt, tgt_entity_tenso_or)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37366,
     "status": "ok",
     "timestamp": 1732177524877,
     "user": {
      "displayName": "BioGITOM Ontology Matching",
      "userId": "17792409086127109994"
     },
     "user_tz": -60
    },
    "id": "UFP6OQR-7D4N",
    "outputId": "b6cb7b1c-13d7-455b-c064-7c70bc313a45"
   },
   "outputs": [],
   "source": [
    "# Generate predictions for candidate mappings using the trained GatedCombination model\n",
    "Prediction_with_candidates(\n",
    "    model=trained_model,             # The trained GatedCombination model used to evaluate similarity\n",
    "    X1_tt=X1_tt,                     # Updated source embeddings (after applying the GIT model)\n",
    "    X2_tt=X2_tt,                     # Original source embeddings (before applying the GIT model)\n",
    "    X3_tt=X3_tt,                     # Updated target embeddings (after applying the GIT model)\n",
    "    X4_tt=X4_tt,                     # Original target embeddings (before applying the GIT model)\n",
    "    src_entity_tensor_o=src_entity_tensor_o,  # Tensor of source entity indices used for evaluation\n",
    "    tgt_entity_tensor_o=tgt_entity_tenso_or,  # Tensor of target entity indices used for evaluation\n",
    "    indexed_dict_src=indexed_dict_src,        # Dictionary mapping source entity indices to their URIs\n",
    "    indexed_dict_tgt=indexed_dict_tgt,        # Dictionary mapping target entity indices to their URIs\n",
    "    all_predictions_path=all_predictions_path # Path to save all predictions with similarity scores in TSV format\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15053,
     "status": "ok",
     "timestamp": 1732177613734,
     "user": {
      "displayName": "BioGITOM Ontology Matching",
      "userId": "17792409086127109994"
     },
     "user_tz": -60
    },
    "id": "mEc12J-B7D4N",
    "outputId": "504f2f00-0e13-4565-c2db-fdccd8254152"
   },
   "outputs": [],
   "source": [
    "# Filter the highest scoring predictions from the predictions file and save the results to a new file\n",
    "matching_results_df = filter_highest_predictions(\n",
    "    all_predictions_path,  # Path to the file containing all predictions with scores for all candidate pairs\n",
    "    prediction_path        # Path where the filtered predictions with highest scores will be saved\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIx74lNV7D4O"
   },
   "source": [
    "# **Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pp3IpBBfWn9m"
   },
   "source": [
    "# Global metrics calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 369,
     "status": "ok",
     "timestamp": 1732177614101,
     "user": {
      "displayName": "BioGITOM Ontology Matching",
      "userId": "17792409086127109994"
     },
     "user_tz": -60
    },
    "id": "bkOewzXr7D4O",
    "outputId": "e247a257-fdcb-496e-98dd-54f0fb74402d"
   },
   "outputs": [],
   "source": [
    "# Retrieve the indices of the ignored classes (from source and target ontologies)\n",
    "ignored_class_index = get_ignored_class_index(src_onto)  # Get ignored class indices from source ontology\n",
    "ignored_class_index.update(get_ignored_class_index(tgt_onto))  # Update with ignored class indices from target ontology\n",
    "\n",
    "# Read the predicted mappings from the prediction results file\n",
    "preds = EntityMapping.read_table_mappings(prediction_path)\n",
    "\n",
    "# Read the reference mappings from the ground truth test file\n",
    "refs = ReferenceMapping.read_table_mappings(test_file)\n",
    "\n",
    "# Filter the predicted mappings to remove any mappings that involve ignored classes\n",
    "preds = remove_ignored_mappings(preds, ignored_class_index)\n",
    "\n",
    "# Compute the precision, recall, and F1-score by comparing predictions with the reference mappings\n",
    "results = AlignmentEvaluator.f1(preds, refs)\n",
    "\n",
    "preds2 = [p.to_tuple() for p in preds]\n",
    "refs2 = [r.to_tuple() for r in refs]\n",
    "\n",
    "correct= len(set(preds2).intersection(set(refs2)))\n",
    "\n",
    "print(f\"Number of Correct Predictions: {correct}\")\n",
    "\n",
    "# Print the computed precision, recall, and F1-score metrics\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aECB6igZW04C"
   },
   "source": [
    "# Ranked-based metrics calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-AK-jADkSbTa"
   },
   "outputs": [],
   "source": [
    "# Read the candidate pairs from a Candidates CSV file into a pandas DataFrame\n",
    "df_embbedings = pd.read_csv(candidates_Rank, index_col=0)\n",
    "\n",
    "# Extract the 'SrcEntity' column (source entity indices) and convert it to a NumPy array of integers\n",
    "tensor_term1 = df_embbedings['SrcEntity'].values.astype(int)\n",
    "\n",
    "# Extract the 'TgtEntity' column (target entity indices) and convert it to a NumPy array of integers\n",
    "tensor_term2 = df_embbedings['TgtEntity'].values.astype(int)\n",
    "\n",
    "# Convert the source entity indices to a PyTorch LongTensor\n",
    "src_entity_tensor_o = torch.from_numpy(tensor_term1).type(torch.LongTensor)\n",
    "\n",
    "# Convert the target entity indices to a PyTorch LongTensor\n",
    "tgt_entity_tenso_or = torch.from_numpy(tensor_term2).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oyOzcLv-SbTb"
   },
   "outputs": [],
   "source": [
    "# Select rows from the updated source embeddings based on the indices in src_entity_tensor_o\n",
    "X1_tt = select_rows_by_index(embeddings_src, src_entity_tensor_o)\n",
    "\n",
    "# Select rows from the original source embeddings based on the indices in src_entity_tensor_o\n",
    "X2_tt = select_rows_by_index(x_src, src_entity_tensor_o)\n",
    "\n",
    "# Select rows from the updated target embeddings based on the indices in tgt_entity_tenso_or\n",
    "X3_tt = select_rows_by_index(embeddings_tgt, tgt_entity_tenso_or)\n",
    "\n",
    "# Select rows from the original target embeddings based on the indices in tgt_entity_tenso_or\n",
    "X4_tt = select_rows_by_index(x_tgt, tgt_entity_tenso_or)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29764,
     "status": "ok",
     "timestamp": 1732176052639,
     "user": {
      "displayName": "BioGITOM Ontology Matching",
      "userId": "17792409086127109994"
     },
     "user_tz": -60
    },
    "id": "-O2f7X6cSb-m",
    "outputId": "989a0d59-9883-4483-e9af-1d580c2d4510"
   },
   "outputs": [],
   "source": [
    "# Perform ranking-based predictions using the trained GatedCombination model\n",
    "# Generate predictions for candidate mappings using the trained GatedCombination model\n",
    "Prediction_with_candidates(\n",
    "    model=trained_model,             # The trained GatedCombination model used to evaluate similarity\n",
    "    X1_tt=X1_tt,                     # Updated source embeddings (after applying the GIT model)\n",
    "    X2_tt=X2_tt,                     # Original source embeddings (before applying the GIT model)\n",
    "    X3_tt=X3_tt,                     # Updated target embeddings (after applying the GIT model)\n",
    "    X4_tt=X4_tt,                     # Original target embeddings (before applying the GIT model)\n",
    "    src_entity_tensor_o=src_entity_tensor_o,  # Tensor of source entity indices used for evaluation\n",
    "    tgt_entity_tensor_o=tgt_entity_tenso_or,  # Tensor of target entity indices used for evaluation\n",
    "    indexed_dict_src=indexed_dict_src,        # Dictionary mapping source entity indices to their URIs\n",
    "    indexed_dict_tgt=indexed_dict_tgt,        # Dictionary mapping target entity indices to their URIs\n",
    "    all_predictions_path=all_predictions_path_ranked, # Path where the ranked predictions will be saved in TSV format\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17108,
     "status": "ok",
     "timestamp": 1732176069738,
     "user": {
      "displayName": "BioGITOM Ontology Matching",
      "userId": "17792409086127109994"
     },
     "user_tz": -60
    },
    "id": "_402seVv7D4O",
    "outputId": "79347806-c0cc-4b88-f0e9-a1570c7530d6"
   },
   "outputs": [],
   "source": [
    "# Compute MRR and Hits@k metrics\n",
    "# This function evaluates the predicted rankings against the reference mappings\n",
    "results = compute_mrr_and_hits(\n",
    "    reference_file=test_cands,             # Reference file with true ranks\n",
    "    predicted_file=all_predictions_path_ranked,             # File containing predicted rankings\n",
    "    output_file=formatted_predictions_path,    # File path to save formatted predictions\n",
    "    k_values=[1, 5, 10]                        # Evaluate Hits@1, Hits@5, and Hits@10\n",
    ")\n",
    "\n",
    "# Display the computed metrics\n",
    "print(\"MRR and Hits@k Results:\")\n",
    "print(results)  # Output the Mean Reciprocal Rank (MRR) and Hits@k metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3639,
     "status": "ok",
     "timestamp": 1732176073365,
     "user": {
      "displayName": "BioGITOM Ontology Matching",
      "userId": "17792409086127109994"
     },
     "user_tz": -60
    },
    "id": "wStfa4eZ7D4O",
    "outputId": "de4cbc85-dd3f-4e0d-ff27-53ae4f63b8b7"
   },
   "outputs": [],
   "source": [
    "# Call the ranking evaluation function, passing the path to the formatted predictions file.\n",
    "# Ks specifies the evaluation levels, checking if the correct target is within the top K candidates.\n",
    "results = ranking_eval(formatted_predictions_path, Ks=[1, 5, 10])\n",
    "print(\"Ranking Evaluation Results at K=1, 5, and 10:\")\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
