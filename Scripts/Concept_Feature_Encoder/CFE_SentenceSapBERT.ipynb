{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Installation block: Installs the required libraries.\n",
        "# Run this in environments like Jupyter Notebook or Colab for proper setup.\n",
        "# - rdflib: For working with RDF data and ontologies.\n",
        "# - torch: PyTorch library for building and training neural networks.\n",
        "# - networkx: For creating, analyzing, and manipulating graph structures.\n",
        "# - matplotlib: For data visualization and plotting.\n",
        "# - sentence-transformers: For generating sentence embeddings (e.g., BERT-based models).\n",
        "# - pandas: For data manipulation and analysis.\n",
        "# - lxml: For parsing and processing XML and HTML.\n",
        "# - beautifulsoup4: For web scraping and parsing HTML/XML documents.\n",
        "!pip install rdflib torch networkx matplotlib sentence-transformers pandas lxml beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89-VvhP4jcTv",
        "outputId": "39365b57-1752-4532-bc02-1a79961505c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdflib\n",
            "  Downloading rdflib-7.1.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.4.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (5.3.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Collecting isodate<1.0.0,>=0.7.2 (from rdflib)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "Downloading rdflib-7.1.1-py3-none-any.whl (562 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m562.4/562.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: isodate, rdflib\n",
            "Successfully installed isodate-0.7.2 rdflib-7.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxDuWP0TK-LK"
      },
      "outputs": [],
      "source": [
        "# Importing all necessary libraries:\n",
        "\n",
        "# Core libraries\n",
        "import numpy as np  # For numerical computations and array operations.\n",
        "import json  # For handling JSON data.\n",
        "\n",
        "# Libraries for ontology and graph processing\n",
        "from rdflib import Graph, Namespace  # For working with RDF data and creating namespaces.\n",
        "from bs4 import BeautifulSoup  # For parsing HTML and XML documents.\n",
        "from lxml import etree  # For efficient XML and HTML parsing.\n",
        "\n",
        "# Libraries for data manipulation and machine learning\n",
        "import pandas as pd  # For data manipulation and analysis.\n",
        "import torch  # For building and training machine learning models.\n",
        "\n",
        "# Libraries for file handling and serialization\n",
        "import pickle  # For serializing and deserializing Python objects.\n",
        "import os  # For interacting with the file system.\n",
        "\n",
        "# Graph processing and visualization\n",
        "import networkx as nx  # For creating and analyzing graph structures.\n",
        "import matplotlib.pyplot as plt  # For visualizing data and graphs.\n",
        "\n",
        "# Importing PyTorch for tensor operations and deep learning workflows.\n",
        "import torch\n",
        "\n",
        "# Importing classes from Hugging Face's transformers library:\n",
        "# - AutoTokenizer: Automatically loads the appropriate tokenizer for a given pre-trained model.\n",
        "# - AutoModel: Automatically loads the appropriate pre-trained transformer model.\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evZSgaXxSe0Y",
        "outputId": "37f86fcb-2b49-49f1-b322-17f893c2aed4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Importing the 'drive' module from Google Colab to interact with Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount the user's Google Drive to the Colab environment\n",
        "# After running this, a link will appear to authorize access, and Google Drive will be mounted at '/content/gdrive'\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36ttssQ3W7cx"
      },
      "outputs": [],
      "source": [
        "# Define the source ontology name\n",
        "src_ent = \"ncit\"\n",
        "\n",
        "# Define the target ontology name\n",
        "tgt_ent = \"doid\"\n",
        "\n",
        "# Define the task name for this ontology matching process\n",
        "task = \"ncit2doid\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJpvkdwVSQye"
      },
      "outputs": [],
      "source": [
        "dir = \"/content/gdrive/My Drive/BioGITOM-VLDB/\"\n",
        "\n",
        "# Define the directory for the dataset containing source and target ontologies\n",
        "dataset_dir = f\"{dir}/Datasets/{task}\"\n",
        "\n",
        "# Define the data directory for storing embeddings, adjacency matrices, and related files\n",
        "prepath = f\"{dir}/{task}/Data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFDNSFef23er"
      },
      "outputs": [],
      "source": [
        "# Load the Source ontology using the Ontology class from DeepOnto\n",
        "# This initializes the source ontology by loading its .owl file.\n",
        "src_onto = f\"{dataset_dir}/{src_ent}.owl\"\n",
        "\n",
        "# Load the Target ontology using the Ontology class from DeepOnto\n",
        "# This initializes the target ontology by loading its .owl file.\n",
        "tgt_onto = f\"{dataset_dir}/{tgt_ent}.owl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7bYcyiLK-LM"
      },
      "outputs": [],
      "source": [
        "def get_exact_matches(graph, class_iri, label, namespace, annotation):\n",
        "    \"\"\"\n",
        "    Retrieves exact matches for a given class in an ontology graph based on a specified annotation property.\n",
        "\n",
        "    Args:\n",
        "        graph: The RDF graph containing ontology data.\n",
        "        class_iri (str): The IRI of the class to retrieve exact matches for.\n",
        "        label (str): The initial label of the class, which will be included in the matches.\n",
        "        namespace: The namespace of the ontology (e.g., 'http://www.w3.org/2004/02/skos/core#').\n",
        "        annotation (str): The specific annotation property to identify exact matches (e.g., 'exactMatch').\n",
        "\n",
        "    Returns:\n",
        "        list: A list of unique exact matches, including the original label.\n",
        "    \"\"\"\n",
        "    # Start with the given label as the initial match for the class.\n",
        "    exact_matches = [label]\n",
        "\n",
        "    # Iterate over all triples in the graph.\n",
        "    # Triple pattern: (subject, predicate, object) where the predicate matches the specified annotation property.\n",
        "    for iri, _, exact_match in graph.triples((None, namespace[annotation], None)):\n",
        "        # Convert the subject IRI to a string and check if it matches the provided class IRI.\n",
        "        if str(iri) == class_iri:\n",
        "            # If the subject matches the class IRI, add the object of the triple (exact match) to the list.\n",
        "            exact_matches.append(str(exact_match))\n",
        "\n",
        "    # Ensure all entries in the exact_matches list are unique using numpy's unique function.\n",
        "    return list(np.unique(exact_matches))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zn-co4SqK-LM"
      },
      "outputs": [],
      "source": [
        "def get_file_synonym_properties(file_path):\n",
        "    \"\"\"\n",
        "    Function to retrieve synonym properties for a specific ontology file.\n",
        "\n",
        "    Args:\n",
        "    file_path (str): The path of the file for which synonym properties are required.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of synonym properties associated with the given file name.\n",
        "    \"\"\"\n",
        "    # Open the JSON file containing the dictionary of synonym properties\n",
        "    f = open(f\"{dir}/dictionary.json\", \"r\")\n",
        "\n",
        "    # Load the content of the JSON file into a Python dictionary\n",
        "    dic = json.loads(f.read())\n",
        "\n",
        "    # Close the file after reading to free system resources\n",
        "    f.close()\n",
        "\n",
        "    # Extract the file name from the provided file path\n",
        "    file_name = file_path.split(\"/\")[-1]\n",
        "\n",
        "    # Return the synonym properties corresponding to the file name\n",
        "    return dic.get(file_name, [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhICeTWDK-LM"
      },
      "outputs": [],
      "source": [
        "def extract_nodes(file_path):\n",
        "    \"\"\"\n",
        "    Extract nodes from an OWL file and map IRIs to their labels and exact matches.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The file path to the .owl file.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where the key is the IRI and the value is the label along with exact matches.\n",
        "    \"\"\"\n",
        "    class_dict = {}\n",
        "\n",
        "    # Initialize an RDF graph and parse the OWL file\n",
        "    g = Graph()\n",
        "    g.parse(file_path, format='xml')\n",
        "\n",
        "    # Define namespaces for RDFS and custom annotations\n",
        "    RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
        "\n",
        "    # Retrieve SKOS namespace and annotation from the provided file path\n",
        "    SKOS, annotation = get_file_synonym_properties(file_path)\n",
        "\n",
        "    # Initialize counters\n",
        "    i = L = 0\n",
        "\n",
        "    # Count the total number of subjects in the graph\n",
        "    for subj in g.subjects(predicate=None):\n",
        "        L += 1\n",
        "    # Iterate over each subject in the graph\n",
        "    for subj in g.subjects(predicate=None):\n",
        "        # Check if the subject is a valid IRI and has a label\n",
        "        if str(subj).startswith('http') and g.value(subject=subj, predicate=RDFS.label) is not None:\n",
        "            # Retrieve the label of the subject\n",
        "            label = str(g.value(subject=subj, predicate=RDFS.label))\n",
        "\n",
        "            # Retrieve exact matches for the subject\n",
        "            class_dict[str(subj)] = get_exact_matches(g, str(subj), label, Namespace(SKOS), annotation)\n",
        "        # Increment the counter\n",
        "        i += 1\n",
        "        # Print progress\n",
        "        print(f\"{i}/{L} : {i/L*100:.2f}%\\t\\t\\t\", end=\"\\r\")\n",
        "\n",
        "    return class_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onFzfpv3K-LN"
      },
      "outputs": [],
      "source": [
        "def oht(class_dict, offset=0):\n",
        "    \"\"\"\n",
        "    One-hot encoding of IRIs.\n",
        "\n",
        "    Args:\n",
        "        class_dict (dict): A dictionary where keys are IRIs and values are labels.\n",
        "        offset (int, optional): A value to start the encoding from. Defaults to 0.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are the IRIs from the input dictionary and values are their corresponding one-hot encoded positions.\n",
        "    \"\"\"\n",
        "    return dict(zip(list(class_dict.keys()), [i + offset for i in range(len(class_dict))]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hr972Rh6K-LN"
      },
      "outputs": [],
      "source": [
        "def extract_links(file_path, class_dict):\n",
        "    \"\"\"\n",
        "    Extract binary links (subClassOf) from an OWL file and store them in an array.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The file path to the .owl file.\n",
        "        class_dict (dict): A dictionary where keys are IRIs (classes) that we are interested in.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: A NumPy array of shape (n, 2) where each row represents a link [subject, object].\n",
        "    \"\"\"\n",
        "    # Initialize an RDF graph and parse the OWL file\n",
        "    g = Graph()\n",
        "    g.parse(file_path, format='xml')\n",
        "\n",
        "    # Initialize a list to store the subject-object pairs\n",
        "    all_predicates = []\n",
        "\n",
        "    # Iterate over all triples in the graph\n",
        "    for subj, pred, obj in g:\n",
        "        # Check if the subject and object are valid IRIs, and the predicate is 'subClassOf'\n",
        "        if (str(subj).startswith('http') and str(obj).startswith('http') and\n",
        "                str(pred).split(\"#\")[-1].split(\"/\")[-1] == \"subClassOf\" and\n",
        "                str(subj) in class_dict and str(obj) in class_dict):\n",
        "            # Append the subject-object pair to the list\n",
        "            all_predicates.append([str(subj), str(obj)])\n",
        "\n",
        "    # Convert the list to a NumPy array\n",
        "    arr = np.array(all_predicates)\n",
        "\n",
        "    return arr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SapBERT model and tokenizer\n",
        "sapbert_model = AutoModel.from_pretrained(\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\")\n",
        "sapbert_tokenizer = AutoTokenizer.from_pretrained(\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\")\n",
        "\n",
        "# Load SentenceTransformer-compatible BERT model\n",
        "sentence_model = SentenceTransformer(\"sentence-transformers/bert-base-nli-mean-tokens\")"
      ],
      "metadata": {
        "id": "A0CtqYrP13ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_embeddings(sentences, batch_size=8, max_length=128, use_gpu=True):\n",
        "    \"\"\"\n",
        "    Generate sentence embeddings using a SentenceTransformer model with SapBERT weights.\n",
        "\n",
        "    Args:\n",
        "        sentences (list of str): A list of sentences to encode.\n",
        "        batch_size (int): Number of sentences to process at a time (for batching).\n",
        "        max_length (int): Maximum sequence length for tokenization.\n",
        "        use_gpu (bool): Whether to use GPU for computation. Defaults to True.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The embeddings for the input sentences.\n",
        "    \"\"\"\n",
        "    # Determine the device to use: GPU (if available and requested) or CPU\n",
        "    device = torch.device(\"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load the SentenceTransformer model with SapBERT weights\n",
        "    sentence_model = SentenceTransformer(\"sentence-transformers/bert-base-nli-mean-tokens\").to(device)\n",
        "    sentence_transformer_model = sentence_model._first_module().auto_model  # Access underlying BERT model\n",
        "    sentence_transformer_model.load_state_dict(sapbert_model.state_dict(), strict=False)\n",
        "\n",
        "    # Tokenizer for the SentenceTransformer model\n",
        "    tokenizer = sapbert_tokenizer  # Reuse SapBERT tokenizer\n",
        "\n",
        "    # Store all embeddings here\n",
        "    all_embeddings = []\n",
        "\n",
        "    # Process the sentences in batches\n",
        "    for i in range(0, len(sentences), batch_size):\n",
        "        # Get the current batch\n",
        "        batch_sentences = sentences[i:i + batch_size]\n",
        "\n",
        "        # Tokenize the batch with truncation to limit sequence length\n",
        "        encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, max_length=max_length, return_tensors='pt').to(device)\n",
        "\n",
        "        # Generate embeddings without computing gradients (for efficiency)\n",
        "        with torch.no_grad():\n",
        "            model_output = sentence_transformer_model(**encoded_input)\n",
        "            batch_embeddings = model_output.last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "\n",
        "        # Append batch embeddings to the list\n",
        "        all_embeddings.append(batch_embeddings)\n",
        "\n",
        "        # Clear the GPU cache to free memory\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Concatenate all batch embeddings into a single array\n",
        "    all_embeddings = np.vstack(all_embeddings)\n",
        "\n",
        "    return all_embeddings"
      ],
      "metadata": {
        "id": "artuxLrI2EMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCyabZltK-LN"
      },
      "outputs": [],
      "source": [
        "def save(class_dict, embeddings, adjacence, file_path):\n",
        "    \"\"\"\n",
        "    Save class dictionary, embeddings, and adjacency matrix to files.\n",
        "\n",
        "    Args:\n",
        "        class_dict (dict): Dictionary of classes (IRIs and their labels).\n",
        "        embeddings (np.ndarray): Array of embeddings for the classes.\n",
        "        adjacence (np.ndarray): Adjacency matrix representing relationships between classes.\n",
        "        file_path (str): File path of the original OWL file to generate output file names.\n",
        "    \"\"\"\n",
        "    # Construct the base path for saving files by using the file name from the provided file path\n",
        "    base_path = f\"{prepath}{file_path.split('/')[-1]}\"\n",
        "    # Save the class dictionary as a JSON file\n",
        "    with open(base_path + \"_classes.json\", \"w\") as file:\n",
        "        json.dump(class_dict, file, indent=3)\n",
        "\n",
        "    # Save the embeddings as a CSV file\n",
        "    pd.DataFrame(embeddings).to_csv(base_path + \"_emb.csv\")\n",
        "\n",
        "    # Save the adjacency matrix as a CSV file\n",
        "    pd.DataFrame(adjacence).to_csv(base_path + \"_adjacence.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZ2qdJQ0K-LN"
      },
      "outputs": [],
      "source": [
        "def depth_search(graph, node, depths: dict, depth: int, pred, nodes):\n",
        "    \"\"\"\n",
        "    Perform a depth-first search on the graph, tracking the depth of each node.\n",
        "\n",
        "    Args:\n",
        "        graph (rdflib.Graph): The RDF graph to search.\n",
        "        node (rdflib.URIRef or str): The starting node for the depth-first search.\n",
        "        depths (dict): A dictionary to store the depth of each node. Keys are node URIs (as strings), values are their depths.\n",
        "        depth (int): The current depth in the search.\n",
        "        pred (rdflib.URIRef or str): The predicate to use for navigating the graph.\n",
        "        nodes (set or list): A collection of node URIs (as strings) that are of interest.\n",
        "    \"\"\"\n",
        "    # Iterate over all objects that are connected to the current node by the specified predicate\n",
        "    for child in graph.objects(subject=node, predicate=pred):\n",
        "        print(str(child))\n",
        "        if str(child) in nodes:\n",
        "            # Record the depth of the child node\n",
        "            depths[str(child)] = depth\n",
        "            # Recursively search the child node\n",
        "            depth_search(graph, child, depths, depth + 1, pred, nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5VVjbM3K-LO"
      },
      "outputs": [],
      "source": [
        "def extract_triplets_binary_relations(owl_file_path, nodes=None):\n",
        "    \"\"\"\n",
        "    Extract all binary relations (triplets) from an OWL file where both the subject and object\n",
        "    are in the specified nodes list.\n",
        "\n",
        "    Args:\n",
        "        owl_file_path (str): The file path to the OWL file.\n",
        "        nodes (set or list, optional): A collection of node IRIs (as strings) to filter the triplets.\n",
        "                                       If None, no filtering will be applied, and all triplets will be returned.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tuples, each containing (subject, predicate, object) for the binary relations.\n",
        "    \"\"\"\n",
        "    g = Graph()\n",
        "    g.parse(owl_file_path, format='xml')\n",
        "    triplets = []\n",
        "\n",
        "    # Convert nodes to a set for faster lookup if provided\n",
        "    if nodes is not None:\n",
        "        nodes = set(nodes)\n",
        "\n",
        "    for subj, pred, obj in g:\n",
        "        # If nodes are provided, filter by checking if both subj and obj are in the nodes set\n",
        "        if nodes is None or (str(subj) in nodes and str(obj) in nodes):\n",
        "            triplets.append((subj, pred, obj))\n",
        "\n",
        "    return triplets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_osPH27IK-LO"
      },
      "outputs": [],
      "source": [
        "def get_json_name_of_dict(owl_file_path):\n",
        "    \"\"\"\n",
        "    Generate the file path for the JSON file associated with the given OWL file.\n",
        "\n",
        "    Args:\n",
        "        owl_file_path (str): The file path to the OWL file.\n",
        "\n",
        "    Returns:\n",
        "        str: The file path for the corresponding JSON file.\n",
        "    \"\"\"\n",
        "    return f\"{prepath}{owl_file_path.split('/')[-1]}_classes.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iP28Vs1EK-LO"
      },
      "outputs": [],
      "source": [
        "def get_json_file(json_file_path):\n",
        "    \"\"\"\n",
        "    Read a JSON file and return its contents as a Python dictionary.\n",
        "\n",
        "    Args:\n",
        "        json_file_path (str): The file path to the JSON file.\n",
        "\n",
        "    Returns:\n",
        "        dict: The contents of the JSON file as a Python dictionary, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(json_file_path, \"r\") as f:\n",
        "            d = json.load(f)\n",
        "        return d\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File {json_file_path} not found.\")\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: File {json_file_path} is not a valid JSON file.\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract nodes (IRIs and their labels/synonyms) from the OWL file.\n",
        "class_dict = extract_nodes(src_onto)\n",
        "\n",
        "# Extract binary relationships (e.g., subClassOf) between the nodes in `class_dict` from the OWL file.\n",
        "links = extract_links(src_onto, class_dict)\n",
        "\n",
        "# Generate one-hot encoding codes for the nodes in `class_dict`.\n",
        "oht_codes = oht(class_dict)\n",
        "\n",
        "# Prepare the data for generating embeddings by concatenating the lists of synonyms into single strings.\n",
        "# Each string represents the concatenated synonyms or related terms for a given IRI.\n",
        "concat_arr = [\", \".join(list(x)) for x in class_dict.values()]"
      ],
      "metadata": {
        "id": "TaU_XC1opLMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pV2NCAo4jynJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Generate embeddings for the concatenated synonym strings using a model like SentenceTransformer.\n",
        "emb = gen_embeddings(concat_arr)\n",
        "\n",
        "# Define a function to replace node IRIs with their corresponding one-hot encoded values.\n",
        "def replace(val, dict=oht_codes):\n",
        "    return dict[val]\n",
        "\n",
        "# Replace IRIs in the class dictionary with their one-hot encoded values.\n",
        "noeuds = np.vectorize(replace)(np.array(list(class_dict.keys())))\n",
        "\n",
        "# Replace IRIs in the links array with their one-hot encoded values.\n",
        "adjacence = np.vectorize(replace)(links)\n",
        "\n",
        "# Save the processed data: class dictionary, embeddings, and adjacency matrix.\n",
        "# These are saved to files derived from the original OWL file's path.\n",
        "save(class_dict, emb, adjacence, src_onto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCqCdHDnkaqB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Generate and Save Adjacency Matrices for Each Binary Predicate\n",
        "\n",
        "# Define the OWL file path\n",
        "\n",
        "# Load the necessary files\n",
        "class_dict = get_json_file(get_json_name_of_dict(src_onto))\n",
        "oht_encoding = oht(class_dict)\n",
        "\n",
        "# Extract binary relation triplets (subject, predicate, object)\n",
        "triplets = np.array(extract_triplets_binary_relations(src_onto))\n",
        "\n",
        "# Process each unique binary predicate\n",
        "unique_predicates = np.unique(triplets[:, 1])\n",
        "\n",
        "for j, binary_predicate in enumerate(unique_predicates, start=1):\n",
        "    print(f\"\\tpredicate {j}/{len(unique_predicates)}\")\n",
        "\n",
        "    # Filter triplets matching the current predicate\n",
        "    predicate_triplets = triplets[triplets[:, 1] == binary_predicate]\n",
        "\n",
        "    # Convert subject and object IRIs to their one-hot encoded values\n",
        "    subjects = []\n",
        "    objects = []\n",
        "\n",
        "    for subj, obj in zip(predicate_triplets[:, 0], predicate_triplets[:, 2]):\n",
        "        # Only include triplets where both subject and object exist in the one-hot encoding\n",
        "        if subj in oht_encoding and obj in oht_encoding:\n",
        "            subjects.append(oht_encoding[subj])\n",
        "            objects.append(oht_encoding[obj])\n",
        "        else:\n",
        "            print(f\"Skipping triplet with missing key: ({subj}, {binary_predicate}, {obj})\")\n",
        "\n",
        "    # If no valid triplets are found, skip this predicate\n",
        "    if not subjects or not objects:\n",
        "        print(f\"No valid triplets found for predicate {binary_predicate}. Skipping...\")\n",
        "        continue\n",
        "\n",
        "    # Extract the label of the predicate (e.g., \"subClassOf\") for the file name\n",
        "    predicate_label = str(binary_predicate).split(\"#\")[-1].split(\"/\")[-1]\n",
        "\n",
        "    # Create a DataFrame to store the adjacency matrix for the current predicate\n",
        "    predicate_adjacence_matrix = pd.DataFrame({\"Src\": subjects, \"Trg\": objects})\n",
        "\n",
        "    # Define the path where the adjacency matrix CSV file will be saved\n",
        "    adjacence_file_path = f\"{prepath}{src_onto.split('/')[-1]}_adjacence_{predicate_label}.csv\"\n",
        "\n",
        "    # Save the adjacency matrix\n",
        "    predicate_adjacence_matrix.to_csv(adjacence_file_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}