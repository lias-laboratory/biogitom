{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import the pandas library for data manipulation and analysis\n",
        "import pandas as pd\n",
        "\n",
        "# Import the numpy library for numerical operations and handling arrays\n",
        "import numpy as np\n",
        "\n",
        "# Import the json library for working with JSON data, such as reading and writing JSON files\n",
        "import json"
      ],
      "metadata": {
        "id": "cy_joeo7yVqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Google Colab drive module to access Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to the specified directory '/content/gdrive' in the Colab environment\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-IXi8TQ6_QH",
        "outputId": "09579dba-acb7-4055-cda8-31e88d46bcc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8ych4i6F0Ms"
      },
      "outputs": [],
      "source": [
        "# Define the source ontology name\n",
        "src_ent = \"snomed.pharm\"\n",
        "\n",
        "# Define the target ontology name\n",
        "tgt_ent = \"ncit.pharm\"\n",
        "\n",
        "# Define the task name for this ontology matching process\n",
        "task = \"pharm\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0J432fFgF0Mt"
      },
      "outputs": [],
      "source": [
        "dir = \"/content/gdrive/My Drive/BioGITOM-VLDB\"\n",
        "\n",
        "# Define the directory for the dataset containing source and target ontologies\n",
        "dataset_dir = f\"{dir}/Datasets/{task}/refs_equiv\"\n",
        "\n",
        "# Define the data directory for storing embeddings, adjacency matrices, and related files\n",
        "data_dir = f\"{dir}/{task}/Data\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the file path for the training set in TSV format\n",
        "train_path = f\"{dataset_dir}/train.tsv\"  # Update with your path to the train set\n",
        "\n",
        "# Define the file path for the JSON file containing the Source ontology class labels\n",
        "# 'src_class' is the path to the JSON file that stores class labels for the source ontology.\n",
        "src_class = f\"{data_dir}/{src_ent}_classes2.json\"\n",
        "\n",
        "# Define the file path for the JSON file containing the Target ontology class labels\n",
        "# 'tgt_class' is the path to the JSON file that stores class labels for the target ontology.\n",
        "tgt_class = f\"{data_dir}/{tgt_ent}_classes.json\"\n",
        "\n",
        "# Define the file path where the encoded training set will be saved\n",
        "# 'encoded_train_path' is the path where the output encoded training set will be stored in CSV format.\n",
        "encoded_train_path = f\"{data_dir}/{task}_train.encoded.csv\"  # Update with your desired output path\n",
        "\n",
        "# Define the file paths for the source and target embeddings\n",
        "# 'src_emb' is the path to the CSV file containing the embeddings of source entities.\n",
        "# 'tgt_emb' is the path to the CSV file containing the embeddings of target entities.\n",
        "src_emb = f\"{data_dir}/{src_ent}_emb2.csv\"\n",
        "tgt_emb = f\"{data_dir}/{tgt_ent}_emb.csv\""
      ],
      "metadata": {
        "id": "bdxeoNpfi8rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_indexed_dict(file_path):\n",
        "    # Load the JSON file\n",
        "    # The input is a file path to a JSON file. The file is opened in read mode ('r').\n",
        "    # The JSON content is then loaded into the 'data' variable using json.load(),\n",
        "    # which parses the JSON into a Python dictionary.\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Create a new dictionary with numeric indexes for each key (URI)\n",
        "    # The enumerate() function is used to generate index numbers (starting from 0).\n",
        "    # For each key in 'data', the function creates a new dictionary called 'indexed_dict',\n",
        "    # where each key from 'data' is mapped to a unique index.\n",
        "    indexed_dict = {key: index for index, key in enumerate(data.keys())}\n",
        "\n",
        "    # Return the resulting dictionary, where keys are from 'data' and values are their corresponding indexes.\n",
        "    return indexed_dict"
      ],
      "metadata": {
        "id": "FZHZQpl1OymO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to encode URIs using the indexed dictionaries\n",
        "def encode_uris(row, src_dict, tgt_dict):\n",
        "    uri_1, uri_2 = row['SrcEntity'], row['TgtEntity']\n",
        "    encoded_uri_1 = src_dict.get(uri_1, -1)  # -1 if not found\n",
        "    encoded_uri_2 = tgt_dict.get(uri_2, -1)  # -1 if not found\n",
        "\n",
        "    # Ensure the URIs are integers (not floats)\n",
        "    return pd.Series([int(encoded_uri_1), int(encoded_uri_2)])"
      ],
      "metadata": {
        "id": "4tIK2JicP2ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dtm5Kw0iK-LQ"
      },
      "outputs": [],
      "source": [
        "def extract_negatives(f1, f2, df, n_negatives):\n",
        "    # Function to calculate the Euclidean distance between two embedding vectors (from source and target)\n",
        "    def dist(node1, node2, embs1, embs2):\n",
        "        return np.sqrt(np.sum((embs1[node1] - embs2[int(node2)]) ** 2))\n",
        "\n",
        "    # Load embeddings from CSV files for source and target entities\n",
        "    embs1 = pd.read_csv(f1, index_col=0).to_numpy()\n",
        "    embs2 = pd.read_csv(f2, index_col=0).to_numpy()\n",
        "\n",
        "    # Convert the DataFrame (df) to a numpy array for easier manipulation\n",
        "    him = df.to_numpy()\n",
        "\n",
        "    # Initialize an empty list to store the negative samples\n",
        "    negative_samples = []\n",
        "\n",
        "    # Iterate through each source entity in the DataFrame (df)\n",
        "    for i, src in enumerate(df['SrcEntity'].values):\n",
        "        # Get the already positive target entities for the current source entity\n",
        "        already_positive = him[np.where(him[:, 0] == src), 1].astype(int).flatten()\n",
        "\n",
        "        # Prepare a list of candidate target entities by excluding already positive entities\n",
        "        candidate_tgt = np.setdiff1d(np.arange(embs2.shape[0]), already_positive)\n",
        "\n",
        "        # Check if there are enough candidates to sample from\n",
        "        if len(candidate_tgt) < n_negatives:\n",
        "            current_n_negatives = len(candidate_tgt)\n",
        "        else:\n",
        "            current_n_negatives = n_negatives\n",
        "\n",
        "        # Randomly sample 'current_n_negatives' entities from the candidate target entities\n",
        "        kept = np.random.choice(candidate_tgt, size=current_n_negatives, replace=False)\n",
        "\n",
        "        # Append the selected negative entities to the list\n",
        "        for tgt in kept:\n",
        "            negative_samples.append([src, tgt, 0])\n",
        "\n",
        "        # Print progress (percentage of processed entities)\n",
        "        print(f\"{i + 1}/{df.shape[0]} : {(i + 1) / df.shape[0] * 100:.2f}%\\t\\t\\t\", end=\"\\r\")\n",
        "\n",
        "    # Return a DataFrame containing the negative samples with columns \"SrcEntity\", \"TgtEntity\", and \"Score\"\n",
        "    return pd.DataFrame(negative_samples, columns=[\"SrcEntity\", \"TgtEntity\", \"Score\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build an indexed dictionary for the source ontology classes\n",
        "# src_class is the file path to the JSON file containing the source ontology classes\n",
        "indexed_dict_src = build_indexed_dict(src_class)\n",
        "\n",
        "# Build an indexed dictionary for the target ontology classes\n",
        "# tgt_class is the file path to the JSON file containing the target ontology classes\n",
        "indexed_dict_tgt = build_indexed_dict(tgt_class)"
      ],
      "metadata": {
        "id": "xK-EwuCzmHPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the train CSV file\n",
        "entity_pairs_df = pd.read_csv(train_path, sep='\\t')"
      ],
      "metadata": {
        "id": "cYojMZdPPE0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply encoding to the Train DataFrame\n",
        "# 'apply()' is used to apply the 'encode_uris' function to each row of 'entity_pairs_df'.\n",
        "# The function is applied along the rows (axis=1), and two dictionaries 'indexed_dict_omim' and 'indexed_dict_ordo'\n",
        "# are passed as arguments to the 'encode_uris' function to map the URIs to their corresponding indexes.\n",
        "encoded_entity_pairs_df = entity_pairs_df.apply(encode_uris, axis=1, src_dict=indexed_dict_src, tgt_dict=indexed_dict_tgt)\n",
        "\n",
        "# Add a new column 'Score' with a default value of 1 for all rows\n",
        "encoded_entity_pairs_df['Score'] = 1\n",
        "\n",
        "# Rename the DataFrame columns to match the desired structure:\n",
        "# 'SrcEntity' (source entity), 'TgtEntity' (target entity), and 'Score'\n",
        "encoded_entity_pairs_df.columns = ['SrcEntity', 'TgtEntity', 'Score']\n",
        "\n",
        "# Add a new 'ID' column with incremental integer values starting from 0,\n",
        "# which assigns a unique identifier to each row.\n",
        "encoded_entity_pairs_df['ID'] = range(0, len(encoded_entity_pairs_df))\n",
        "\n",
        "# Reorder the columns so that 'ID' is the first column, followed by 'SrcEntity', 'TgtEntity', and 'Score'\n",
        "encoded_entity_pairs_df = encoded_entity_pairs_df[['ID', 'SrcEntity', 'TgtEntity', 'Score']]\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file specified by 'encoded_train_path'\n",
        "# The 'index=False' argument ensures that row indices are not saved in the CSV.\n",
        "encoded_entity_pairs_df.to_csv(encoded_train_path, index=False)\n",
        "\n",
        "# Print a confirmation message indicating the file has been saved with the new structure\n",
        "print(f\"Encoded entity pairs with incremental ID (starting from 0) saved to: {encoded_train_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vlm3olb9QD69",
        "outputId": "67ec3741-9018-4b2d-a3ed-47eeda1c5b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded entity pairs with incremental ID (starting from 0) saved to: /content/gdrive/My Drive/BioGITOM-VLDB/pharm/Data/pharm_train.encoded.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cD5_nybTK-LR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6324ab2b-418f-4c1e-b12a-b41d870adf4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "source": [
        "# Load the encoded file (containing entity pairs) into a DataFrame\n",
        "df = pd.read_csv(encoded_train_path, sep=',')\n",
        "\n",
        "# Iterate over different numbers of negative examples to generate\n",
        "for nb_negs in [20, 50, 100, 200]:\n",
        "    # Create a copy of the original DataFrame to avoid modifying the original data\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Generate random negative examples\n",
        "    # 'extract_negatives' generates negative samples by pairing non-matching entities.\n",
        "    # It uses the embeddings from 'f1' (source) and 'f2' (target) and creates 'nb_negs' negative pairs for each entity.\n",
        "    df_negs = extract_negatives(src_emb, tgt_emb, df, n_negatives=nb_negs)\n",
        "\n",
        "    # Concatenate the original entity pairs with the newly generated negative examples\n",
        "    # 'df_final' will contain both positive and negative examples.\n",
        "    df_final = pd.concat([df, df_negs], axis=0).reset_index().drop(columns=[\"index\"])\n",
        "\n",
        "    # Save the resulting DataFrame with positive and negative examples to a new CSV file\n",
        "    # The file name reflects the number of negatives added (e.g., omim2ordo_rdm_20.csv).\n",
        "    df_final.to_csv(f\"{data_dir}/{task}_train_2_{nb_negs}.csv\", index=False)"
      ]
    }
  ]
}